{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0481315a-b5ad-4b80-a300-a2715bb6a40a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d26f864c63432c829e75b7595a829a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 4 GPUs per node.\n",
      "SW: Model with 6173M total params, 534M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  155.23GB |   1.99GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "  155.23GB |   1.99GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "  137.98GB |   4.87GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "  137.98GB |   4.87GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "   11.95GB |  27.86GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "  137.98GB |  27.86GB | offload_param=none, offload_optimizer=none, zero_init=0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live\n",
    "\n",
    "## specify the model you want to train on your device\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\",trust_remote_code=True) \n",
    "## estimate the memory cost (both CPU and GPU)\n",
    "estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=4, num_nodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83302e11-3fce-4e86-afc4-0681d124b36a",
   "metadata": {},
   "source": [
    "# 消耗时间汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551eea6-6bbd-4b92-b43c-cb6b072c7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "1、单GPU                                ---耗时：0.8616263747215271分钟\n",
    "2、DataParallel                         ---耗时：1.4458719611167907分钟\n",
    "3、Distributed分布式训练                 ---耗时：0.2986536264419556分钟\n",
    "4、distributed分布式训练-multiprocess启动 ---耗时：0.3739401698112488分钟\n",
    "5、AMP混合精度训练                        ---耗时：0.2881103197733561分钟\n",
    "6、deepspeed分布式训练                   ---耗时：1.2790814956029257分钟\n",
    "7、accelerate                           ---耗时：0.3006397406260172分钟\n",
    "8、transformers的Trainer分布式训练        ---43s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c056675e-faa5-4a4a-ba75-4f61ed8c6f56",
   "metadata": {},
   "source": [
    "# 训练方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e701b59-4695-4ef2-bf2c-0d87b7ecf192",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 单GPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10dd43fe-08aa-43bb-a4b8-3b3a2a559646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-21 02:37:21,958] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Downloading (…)solve/main/vocab.txt: 100%|███| 110k/110k [00:00<00:00, 1.62MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|█████| 2.00/2.00 [00:00<00:00, 816B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 112/112 [00:00<00:00, 150kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███| 19.0/19.0 [00:00<00:00, 25.6kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████| 647/647 [00:00<00:00, 902kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 412M/412M [00:01<00:00, 356MB/s]\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "【train】 epoch：1/1 step：1/288 loss：1.842221\n",
      "【train】 epoch：1/1 step：2/288 loss：1.889754\n",
      "【train】 epoch：1/1 step：3/288 loss：1.595842\n",
      "【train】 epoch：1/1 step：4/288 loss：1.683983\n",
      "【train】 epoch：1/1 step：5/288 loss：1.642007\n",
      "【train】 epoch：1/1 step：6/288 loss：1.639417\n",
      "【train】 epoch：1/1 step：7/288 loss：1.594382\n",
      "【train】 epoch：1/1 step：8/288 loss：1.742031\n",
      "【train】 epoch：1/1 step：9/288 loss：1.593593\n",
      "【train】 epoch：1/1 step：10/288 loss：1.642175\n",
      "【train】 epoch：1/1 step：11/288 loss：1.607156\n",
      "【train】 epoch：1/1 step：12/288 loss：1.675098\n",
      "【train】 epoch：1/1 step：13/288 loss：1.549546\n",
      "【train】 epoch：1/1 step：14/288 loss：1.531840\n",
      "【train】 epoch：1/1 step：15/288 loss：1.567795\n",
      "【train】 epoch：1/1 step：16/288 loss：1.799484\n",
      "【train】 epoch：1/1 step：17/288 loss：1.546693\n",
      "【train】 epoch：1/1 step：18/288 loss：2.009086\n",
      "【train】 epoch：1/1 step：19/288 loss：1.637240\n",
      "【train】 epoch：1/1 step：20/288 loss：1.536004\n",
      "【train】 epoch：1/1 step：21/288 loss：1.445081\n",
      "【train】 epoch：1/1 step：22/288 loss：1.388927\n",
      "【train】 epoch：1/1 step：23/288 loss：1.523077\n",
      "【train】 epoch：1/1 step：24/288 loss：1.417861\n",
      "【train】 epoch：1/1 step：25/288 loss：1.441376\n",
      "【train】 epoch：1/1 step：26/288 loss：1.533687\n",
      "【train】 epoch：1/1 step：27/288 loss：1.546214\n",
      "【train】 epoch：1/1 step：28/288 loss：1.378107\n",
      "【train】 epoch：1/1 step：29/288 loss：1.446327\n",
      "【train】 epoch：1/1 step：30/288 loss：1.592117\n",
      "【train】 epoch：1/1 step：31/288 loss：1.299654\n",
      "【train】 epoch：1/1 step：32/288 loss：1.456984\n",
      "【train】 epoch：1/1 step：33/288 loss：1.441024\n",
      "【train】 epoch：1/1 step：34/288 loss：1.320821\n",
      "【train】 epoch：1/1 step：35/288 loss：1.260438\n",
      "【train】 epoch：1/1 step：36/288 loss：1.341988\n",
      "【train】 epoch：1/1 step：37/288 loss：1.237723\n",
      "【train】 epoch：1/1 step：38/288 loss：1.331155\n",
      "【train】 epoch：1/1 step：39/288 loss：1.133899\n",
      "【train】 epoch：1/1 step：40/288 loss：1.385551\n",
      "【train】 epoch：1/1 step：41/288 loss：1.387642\n",
      "【train】 epoch：1/1 step：42/288 loss：1.446798\n",
      "【train】 epoch：1/1 step：43/288 loss：1.218416\n",
      "【train】 epoch：1/1 step：44/288 loss：1.288093\n",
      "【train】 epoch：1/1 step：45/288 loss：1.240415\n",
      "【train】 epoch：1/1 step：46/288 loss：1.443331\n",
      "【train】 epoch：1/1 step：47/288 loss：1.539987\n",
      "【train】 epoch：1/1 step：48/288 loss：1.355884\n",
      "【train】 epoch：1/1 step：49/288 loss：1.403552\n",
      "【train】 epoch：1/1 step：50/288 loss：1.167257\n",
      "【train】 epoch：1/1 step：51/288 loss：1.125566\n",
      "【train】 epoch：1/1 step：52/288 loss：1.250446\n",
      "【train】 epoch：1/1 step：53/288 loss：1.187134\n",
      "【train】 epoch：1/1 step：54/288 loss：1.378708\n",
      "【train】 epoch：1/1 step：55/288 loss：1.547734\n",
      "【train】 epoch：1/1 step：56/288 loss：1.763144\n",
      "【train】 epoch：1/1 step：57/288 loss：1.276694\n",
      "【train】 epoch：1/1 step：58/288 loss：1.206251\n",
      "【train】 epoch：1/1 step：59/288 loss：1.098378\n",
      "【train】 epoch：1/1 step：60/288 loss：1.259216\n",
      "【train】 epoch：1/1 step：61/288 loss：1.059310\n",
      "【train】 epoch：1/1 step：62/288 loss：1.261448\n",
      "【train】 epoch：1/1 step：63/288 loss：1.402100\n",
      "【train】 epoch：1/1 step：64/288 loss：1.390236\n",
      "【train】 epoch：1/1 step：65/288 loss：1.126804\n",
      "【train】 epoch：1/1 step：66/288 loss：1.396574\n",
      "【train】 epoch：1/1 step：67/288 loss：1.402313\n",
      "【train】 epoch：1/1 step：68/288 loss：1.196147\n",
      "【train】 epoch：1/1 step：69/288 loss：1.328372\n",
      "【train】 epoch：1/1 step：70/288 loss：1.011429\n",
      "【train】 epoch：1/1 step：71/288 loss：1.353058\n",
      "【train】 epoch：1/1 step：72/288 loss：1.438427\n",
      "【train】 epoch：1/1 step：73/288 loss：1.277313\n",
      "【train】 epoch：1/1 step：74/288 loss：1.350983\n",
      "【train】 epoch：1/1 step：75/288 loss：1.398123\n",
      "【train】 epoch：1/1 step：76/288 loss：0.947378\n",
      "【train】 epoch：1/1 step：77/288 loss：1.160832\n",
      "【train】 epoch：1/1 step：78/288 loss：1.225821\n",
      "【train】 epoch：1/1 step：79/288 loss：1.140773\n",
      "【train】 epoch：1/1 step：80/288 loss：1.041827\n",
      "【train】 epoch：1/1 step：81/288 loss：1.171800\n",
      "【train】 epoch：1/1 step：82/288 loss：1.296688\n",
      "【train】 epoch：1/1 step：83/288 loss：1.204181\n",
      "【train】 epoch：1/1 step：84/288 loss：1.056508\n",
      "【train】 epoch：1/1 step：85/288 loss：1.164000\n",
      "【train】 epoch：1/1 step：86/288 loss：1.857498\n",
      "【train】 epoch：1/1 step：87/288 loss：1.087365\n",
      "【train】 epoch：1/1 step：88/288 loss：1.418489\n",
      "【train】 epoch：1/1 step：89/288 loss：0.957268\n",
      "【train】 epoch：1/1 step：90/288 loss：1.185106\n",
      "【train】 epoch：1/1 step：91/288 loss：1.133587\n",
      "【train】 epoch：1/1 step：92/288 loss：1.112054\n",
      "【train】 epoch：1/1 step：93/288 loss：1.044793\n",
      "【train】 epoch：1/1 step：94/288 loss：1.024021\n",
      "【train】 epoch：1/1 step：95/288 loss：1.287642\n",
      "【train】 epoch：1/1 step：96/288 loss：1.633174\n",
      "【train】 epoch：1/1 step：97/288 loss：1.306284\n",
      "【train】 epoch：1/1 step：98/288 loss：1.158255\n",
      "【train】 epoch：1/1 step：99/288 loss：1.466884\n",
      "【train】 epoch：1/1 step：100/288 loss：1.332380\n",
      "【train】 epoch：1/1 step：101/288 loss：1.079401\n",
      "【train】 epoch：1/1 step：102/288 loss：1.456278\n",
      "【train】 epoch：1/1 step：103/288 loss：1.394180\n",
      "【train】 epoch：1/1 step：104/288 loss：1.096681\n",
      "【train】 epoch：1/1 step：105/288 loss：1.228453\n",
      "【train】 epoch：1/1 step：106/288 loss：1.050875\n",
      "【train】 epoch：1/1 step：107/288 loss：1.122408\n",
      "【train】 epoch：1/1 step：108/288 loss：0.997821\n",
      "【train】 epoch：1/1 step：109/288 loss：1.188410\n",
      "【train】 epoch：1/1 step：110/288 loss：1.204007\n",
      "【train】 epoch：1/1 step：111/288 loss：1.300774\n",
      "【train】 epoch：1/1 step：112/288 loss：1.253149\n",
      "【train】 epoch：1/1 step：113/288 loss：1.349950\n",
      "【train】 epoch：1/1 step：114/288 loss：1.009767\n",
      "【train】 epoch：1/1 step：115/288 loss：1.242776\n",
      "【train】 epoch：1/1 step：116/288 loss：1.242473\n",
      "【train】 epoch：1/1 step：117/288 loss：1.029673\n",
      "【train】 epoch：1/1 step：118/288 loss：1.255516\n",
      "【train】 epoch：1/1 step：119/288 loss：1.341253\n",
      "【train】 epoch：1/1 step：120/288 loss：1.298537\n",
      "【train】 epoch：1/1 step：121/288 loss：1.449651\n",
      "【train】 epoch：1/1 step：122/288 loss：1.359595\n",
      "【train】 epoch：1/1 step：123/288 loss：1.137203\n",
      "【train】 epoch：1/1 step：124/288 loss：1.500593\n",
      "【train】 epoch：1/1 step：125/288 loss：1.343245\n",
      "【train】 epoch：1/1 step：126/288 loss：1.304569\n",
      "【train】 epoch：1/1 step：127/288 loss：1.064867\n",
      "【train】 epoch：1/1 step：128/288 loss：1.156153\n",
      "【train】 epoch：1/1 step：129/288 loss：1.438203\n",
      "【train】 epoch：1/1 step：130/288 loss：1.340608\n",
      "【train】 epoch：1/1 step：131/288 loss：0.907088\n",
      "【train】 epoch：1/1 step：132/288 loss：0.927547\n",
      "【train】 epoch：1/1 step：133/288 loss：0.965219\n",
      "【train】 epoch：1/1 step：134/288 loss：1.290852\n",
      "【train】 epoch：1/1 step：135/288 loss：0.974874\n",
      "【train】 epoch：1/1 step：136/288 loss：1.390580\n",
      "【train】 epoch：1/1 step：137/288 loss：1.195488\n",
      "【train】 epoch：1/1 step：138/288 loss：0.999264\n",
      "【train】 epoch：1/1 step：139/288 loss：1.148827\n",
      "【train】 epoch：1/1 step：140/288 loss：1.255777\n",
      "【train】 epoch：1/1 step：141/288 loss：1.084284\n",
      "【train】 epoch：1/1 step：142/288 loss：0.848643\n",
      "【train】 epoch：1/1 step：143/288 loss：1.196710\n",
      "【train】 epoch：1/1 step：144/288 loss：1.359189\n",
      "【train】 epoch：1/1 step：145/288 loss：1.249083\n",
      "【train】 epoch：1/1 step：146/288 loss：0.999187\n",
      "【train】 epoch：1/1 step：147/288 loss：1.068898\n",
      "【train】 epoch：1/1 step：148/288 loss：1.239820\n",
      "【train】 epoch：1/1 step：149/288 loss：0.972217\n",
      "【train】 epoch：1/1 step：150/288 loss：0.967887\n",
      "【train】 epoch：1/1 step：151/288 loss：1.472968\n",
      "【train】 epoch：1/1 step：152/288 loss：1.373483\n",
      "【train】 epoch：1/1 step：153/288 loss：1.135004\n",
      "【train】 epoch：1/1 step：154/288 loss：1.118656\n",
      "【train】 epoch：1/1 step：155/288 loss：1.098940\n",
      "【train】 epoch：1/1 step：156/288 loss：0.749164\n",
      "【train】 epoch：1/1 step：157/288 loss：1.400386\n",
      "【train】 epoch：1/1 step：158/288 loss：0.975709\n",
      "【train】 epoch：1/1 step：159/288 loss：1.363654\n",
      "【train】 epoch：1/1 step：160/288 loss：0.916150\n",
      "【train】 epoch：1/1 step：161/288 loss：1.429937\n",
      "【train】 epoch：1/1 step：162/288 loss：1.332423\n",
      "【train】 epoch：1/1 step：163/288 loss：0.992864\n",
      "【train】 epoch：1/1 step：164/288 loss：1.164322\n",
      "【train】 epoch：1/1 step：165/288 loss：1.044234\n",
      "【train】 epoch：1/1 step：166/288 loss：1.325350\n",
      "【train】 epoch：1/1 step：167/288 loss：1.337299\n",
      "【train】 epoch：1/1 step：168/288 loss：1.231608\n",
      "【train】 epoch：1/1 step：169/288 loss：1.236987\n",
      "【train】 epoch：1/1 step：170/288 loss：1.350183\n",
      "【train】 epoch：1/1 step：171/288 loss：1.356925\n",
      "【train】 epoch：1/1 step：172/288 loss：1.249849\n",
      "【train】 epoch：1/1 step：173/288 loss：1.026532\n",
      "【train】 epoch：1/1 step：174/288 loss：1.321547\n",
      "【train】 epoch：1/1 step：175/288 loss：1.170776\n",
      "【train】 epoch：1/1 step：176/288 loss：1.168107\n",
      "【train】 epoch：1/1 step：177/288 loss：1.085701\n",
      "【train】 epoch：1/1 step：178/288 loss：1.211540\n",
      "【train】 epoch：1/1 step：179/288 loss：1.428354\n",
      "【train】 epoch：1/1 step：180/288 loss：1.144824\n",
      "【train】 epoch：1/1 step：181/288 loss：0.895862\n",
      "【train】 epoch：1/1 step：182/288 loss：1.331061\n",
      "【train】 epoch：1/1 step：183/288 loss：0.908161\n",
      "【train】 epoch：1/1 step：184/288 loss：1.210989\n",
      "【train】 epoch：1/1 step：185/288 loss：1.273117\n",
      "【train】 epoch：1/1 step：186/288 loss：0.987152\n",
      "【train】 epoch：1/1 step：187/288 loss：1.058678\n",
      "【train】 epoch：1/1 step：188/288 loss：1.195396\n",
      "【train】 epoch：1/1 step：189/288 loss：1.211129\n",
      "【train】 epoch：1/1 step：190/288 loss：1.278379\n",
      "【train】 epoch：1/1 step：191/288 loss：1.288210\n",
      "【train】 epoch：1/1 step：192/288 loss：1.304548\n",
      "【train】 epoch：1/1 step：193/288 loss：0.871206\n",
      "【train】 epoch：1/1 step：194/288 loss：1.129664\n",
      "【train】 epoch：1/1 step：195/288 loss：1.124249\n",
      "【train】 epoch：1/1 step：196/288 loss：1.368982\n",
      "【train】 epoch：1/1 step：197/288 loss：1.082366\n",
      "【train】 epoch：1/1 step：198/288 loss：1.291488\n",
      "【train】 epoch：1/1 step：199/288 loss：1.116984\n",
      "【train】 epoch：1/1 step：200/288 loss：1.072732\n",
      "【train】 epoch：1/1 step：201/288 loss：1.053859\n",
      "【train】 epoch：1/1 step：202/288 loss：1.056992\n",
      "【train】 epoch：1/1 step：203/288 loss：1.092140\n",
      "【train】 epoch：1/1 step：204/288 loss：1.035022\n",
      "【train】 epoch：1/1 step：205/288 loss：1.215563\n",
      "【train】 epoch：1/1 step：206/288 loss：1.444990\n",
      "【train】 epoch：1/1 step：207/288 loss：1.173223\n",
      "【train】 epoch：1/1 step：208/288 loss：1.000989\n",
      "【train】 epoch：1/1 step：209/288 loss：1.132510\n",
      "【train】 epoch：1/1 step：210/288 loss：1.313105\n",
      "【train】 epoch：1/1 step：211/288 loss：1.130279\n",
      "【train】 epoch：1/1 step：212/288 loss：1.535411\n",
      "【train】 epoch：1/1 step：213/288 loss：1.273536\n",
      "【train】 epoch：1/1 step：214/288 loss：1.038376\n",
      "【train】 epoch：1/1 step：215/288 loss：1.144987\n",
      "【train】 epoch：1/1 step：216/288 loss：1.043368\n",
      "【train】 epoch：1/1 step：217/288 loss：1.226847\n",
      "【train】 epoch：1/1 step：218/288 loss：1.035742\n",
      "【train】 epoch：1/1 step：219/288 loss：1.173071\n",
      "【train】 epoch：1/1 step：220/288 loss：0.952754\n",
      "【train】 epoch：1/1 step：221/288 loss：0.970488\n",
      "【train】 epoch：1/1 step：222/288 loss：1.024092\n",
      "【train】 epoch：1/1 step：223/288 loss：1.522485\n",
      "【train】 epoch：1/1 step：224/288 loss：1.325018\n",
      "【train】 epoch：1/1 step：225/288 loss：1.200199\n",
      "【train】 epoch：1/1 step：226/288 loss：0.920105\n",
      "【train】 epoch：1/1 step：227/288 loss：1.003339\n",
      "【train】 epoch：1/1 step：228/288 loss：1.144334\n",
      "【train】 epoch：1/1 step：229/288 loss：1.207768\n",
      "【train】 epoch：1/1 step：230/288 loss：1.466726\n",
      "【train】 epoch：1/1 step：231/288 loss：1.146720\n",
      "【train】 epoch：1/1 step：232/288 loss：1.169653\n",
      "【train】 epoch：1/1 step：233/288 loss：1.004494\n",
      "【train】 epoch：1/1 step：234/288 loss：1.083710\n",
      "【train】 epoch：1/1 step：235/288 loss：1.314272\n",
      "【train】 epoch：1/1 step：236/288 loss：1.197859\n",
      "【train】 epoch：1/1 step：237/288 loss：1.433766\n",
      "【train】 epoch：1/1 step：238/288 loss：1.037548\n",
      "【train】 epoch：1/1 step：239/288 loss：1.346975\n",
      "【train】 epoch：1/1 step：240/288 loss：1.284676\n",
      "【train】 epoch：1/1 step：241/288 loss：1.196754\n",
      "【train】 epoch：1/1 step：242/288 loss：1.114348\n",
      "【train】 epoch：1/1 step：243/288 loss：1.191631\n",
      "【train】 epoch：1/1 step：244/288 loss：1.349935\n",
      "【train】 epoch：1/1 step：245/288 loss：1.248855\n",
      "【train】 epoch：1/1 step：246/288 loss：1.193445\n",
      "【train】 epoch：1/1 step：247/288 loss：0.948432\n",
      "【train】 epoch：1/1 step：248/288 loss：1.305067\n",
      "【train】 epoch：1/1 step：249/288 loss：1.175983\n",
      "【train】 epoch：1/1 step：250/288 loss：1.294248\n",
      "【train】 epoch：1/1 step：251/288 loss：1.476595\n",
      "【train】 epoch：1/1 step：252/288 loss：1.239568\n",
      "【train】 epoch：1/1 step：253/288 loss：0.911946\n",
      "【train】 epoch：1/1 step：254/288 loss：1.131777\n",
      "【train】 epoch：1/1 step：255/288 loss：1.366970\n",
      "【train】 epoch：1/1 step：256/288 loss：1.296846\n",
      "【train】 epoch：1/1 step：257/288 loss：0.953452\n",
      "【train】 epoch：1/1 step：258/288 loss：1.164911\n",
      "【train】 epoch：1/1 step：259/288 loss：1.029307\n",
      "【train】 epoch：1/1 step：260/288 loss：1.139385\n",
      "【train】 epoch：1/1 step：261/288 loss：1.346931\n",
      "【train】 epoch：1/1 step：262/288 loss：1.508587\n",
      "【train】 epoch：1/1 step：263/288 loss：1.004747\n",
      "【train】 epoch：1/1 step：264/288 loss：1.215659\n",
      "【train】 epoch：1/1 step：265/288 loss：1.368509\n",
      "【train】 epoch：1/1 step：266/288 loss：1.483073\n",
      "【train】 epoch：1/1 step：267/288 loss：1.304574\n",
      "【train】 epoch：1/1 step：268/288 loss：1.350207\n",
      "【train】 epoch：1/1 step：269/288 loss：1.177527\n",
      "【train】 epoch：1/1 step：270/288 loss：1.189805\n",
      "【train】 epoch：1/1 step：271/288 loss：1.126414\n",
      "【train】 epoch：1/1 step：272/288 loss：1.240553\n",
      "【train】 epoch：1/1 step：273/288 loss：1.237737\n",
      "【train】 epoch：1/1 step：274/288 loss：1.159212\n",
      "【train】 epoch：1/1 step：275/288 loss：1.008645\n",
      "【train】 epoch：1/1 step：276/288 loss：1.176263\n",
      "【train】 epoch：1/1 step：277/288 loss：1.129448\n",
      "【train】 epoch：1/1 step：278/288 loss：1.027557\n",
      "【train】 epoch：1/1 step：279/288 loss：1.329855\n",
      "【train】 epoch：1/1 step：280/288 loss：1.128487\n",
      "【train】 epoch：1/1 step：281/288 loss：1.234525\n",
      "【train】 epoch：1/1 step：282/288 loss：1.089155\n",
      "【train】 epoch：1/1 step：283/288 loss：1.335584\n",
      "【train】 epoch：1/1 step：284/288 loss：1.010698\n",
      "【train】 epoch：1/1 step：285/288 loss：1.102064\n",
      "【train】 epoch：1/1 step：286/288 loss：1.057685\n",
      "【train】 epoch：1/1 step：287/288 loss：1.261547\n",
      "【train】 epoch：1/1 step：288/288 loss：1.173614\n",
      "耗时：0.8511070847511292分钟\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "(800,) (800,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          其他       0.62      0.74      0.67       273\n",
      "          喜好       0.53      0.62      0.57       112\n",
      "          悲伤       0.58      0.52      0.55       114\n",
      "          厌恶       0.44      0.43      0.44       120\n",
      "          愤怒       0.62      0.26      0.36        62\n",
      "          高兴       0.72      0.56      0.63       119\n",
      "\n",
      "    accuracy                           0.58       800\n",
      "   macro avg       0.58      0.52      0.54       800\n",
      "weighted avg       0.59      0.58      0.57       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!~/anaconda3/envs/python39_p13/bin/python single-gpu-cls.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c8663-c155-4253-b368-424f2638f2da",
   "metadata": {},
   "source": [
    "## DataParallel分布式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad578fd6-f718-49f5-8afd-784abcfd1831",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "【train】 epoch：1/1 step：1/288 loss：1.890254\n",
      "【train】 epoch：1/1 step：2/288 loss：1.903190\n",
      "【train】 epoch：1/1 step：3/288 loss：1.596405\n",
      "【train】 epoch：1/1 step：4/288 loss：1.684801\n",
      "【train】 epoch：1/1 step：5/288 loss：1.571018\n",
      "【train】 epoch：1/1 step：6/288 loss：1.563324\n",
      "【train】 epoch：1/1 step：7/288 loss：1.644753\n",
      "【train】 epoch：1/1 step：8/288 loss：1.723611\n",
      "【train】 epoch：1/1 step：9/288 loss：1.647003\n",
      "【train】 epoch：1/1 step：10/288 loss：1.547561\n",
      "【train】 epoch：1/1 step：11/288 loss：1.605038\n",
      "【train】 epoch：1/1 step：12/288 loss：1.678797\n",
      "【train】 epoch：1/1 step：13/288 loss：1.578031\n",
      "【train】 epoch：1/1 step：14/288 loss：1.575957\n",
      "【train】 epoch：1/1 step：15/288 loss：1.541853\n",
      "【train】 epoch：1/1 step：16/288 loss：1.750878\n",
      "【train】 epoch：1/1 step：17/288 loss：1.625691\n",
      "【train】 epoch：1/1 step：18/288 loss：1.953422\n",
      "【train】 epoch：1/1 step：19/288 loss：1.616647\n",
      "【train】 epoch：1/1 step：20/288 loss：1.572353\n",
      "【train】 epoch：1/1 step：21/288 loss：1.447700\n",
      "【train】 epoch：1/1 step：22/288 loss：1.418161\n",
      "【train】 epoch：1/1 step：23/288 loss：1.586742\n",
      "【train】 epoch：1/1 step：24/288 loss：1.453836\n",
      "【train】 epoch：1/1 step：25/288 loss：1.552701\n",
      "【train】 epoch：1/1 step：26/288 loss：1.565987\n",
      "【train】 epoch：1/1 step：27/288 loss：1.503088\n",
      "【train】 epoch：1/1 step：28/288 loss：1.350055\n",
      "【train】 epoch：1/1 step：29/288 loss：1.478591\n",
      "【train】 epoch：1/1 step：30/288 loss：1.544381\n",
      "【train】 epoch：1/1 step：31/288 loss：1.288843\n",
      "【train】 epoch：1/1 step：32/288 loss：1.409695\n",
      "【train】 epoch：1/1 step：33/288 loss：1.555618\n",
      "【train】 epoch：1/1 step：34/288 loss：1.225311\n",
      "【train】 epoch：1/1 step：35/288 loss：1.291361\n",
      "【train】 epoch：1/1 step：36/288 loss：1.385503\n",
      "【train】 epoch：1/1 step：37/288 loss：1.330403\n",
      "【train】 epoch：1/1 step：38/288 loss：1.402305\n",
      "【train】 epoch：1/1 step：39/288 loss：1.166812\n",
      "【train】 epoch：1/1 step：40/288 loss：1.304627\n",
      "【train】 epoch：1/1 step：41/288 loss：1.444454\n",
      "【train】 epoch：1/1 step：42/288 loss：1.443681\n",
      "【train】 epoch：1/1 step：43/288 loss：1.264161\n",
      "【train】 epoch：1/1 step：44/288 loss：1.360168\n",
      "【train】 epoch：1/1 step：45/288 loss：1.295295\n",
      "【train】 epoch：1/1 step：46/288 loss：1.372973\n",
      "【train】 epoch：1/1 step：47/288 loss：1.529051\n",
      "【train】 epoch：1/1 step：48/288 loss：1.144567\n",
      "【train】 epoch：1/1 step：49/288 loss：1.503499\n",
      "【train】 epoch：1/1 step：50/288 loss：1.153067\n",
      "【train】 epoch：1/1 step：51/288 loss：1.178736\n",
      "【train】 epoch：1/1 step：52/288 loss：1.113689\n",
      "【train】 epoch：1/1 step：53/288 loss：1.212726\n",
      "【train】 epoch：1/1 step：54/288 loss：1.377870\n",
      "【train】 epoch：1/1 step：55/288 loss：1.549609\n",
      "【train】 epoch：1/1 step：56/288 loss：1.621553\n",
      "【train】 epoch：1/1 step：57/288 loss：1.224477\n",
      "【train】 epoch：1/1 step：58/288 loss：1.187292\n",
      "【train】 epoch：1/1 step：59/288 loss：1.121613\n",
      "【train】 epoch：1/1 step：60/288 loss：1.188336\n",
      "【train】 epoch：1/1 step：61/288 loss：1.040259\n",
      "【train】 epoch：1/1 step：62/288 loss：1.245046\n",
      "【train】 epoch：1/1 step：63/288 loss：1.338905\n",
      "【train】 epoch：1/1 step：64/288 loss：1.365711\n",
      "【train】 epoch：1/1 step：65/288 loss：1.186905\n",
      "【train】 epoch：1/1 step：66/288 loss：1.460309\n",
      "【train】 epoch：1/1 step：67/288 loss：1.372131\n",
      "【train】 epoch：1/1 step：68/288 loss：1.210435\n",
      "【train】 epoch：1/1 step：69/288 loss：1.488447\n",
      "【train】 epoch：1/1 step：70/288 loss：1.093002\n",
      "【train】 epoch：1/1 step：71/288 loss：1.292703\n",
      "【train】 epoch：1/1 step：72/288 loss：1.326361\n",
      "【train】 epoch：1/1 step：73/288 loss：1.257250\n",
      "【train】 epoch：1/1 step：74/288 loss：1.477139\n",
      "【train】 epoch：1/1 step：75/288 loss：1.345778\n",
      "【train】 epoch：1/1 step：76/288 loss：0.976986\n",
      "【train】 epoch：1/1 step：77/288 loss：1.134403\n",
      "【train】 epoch：1/1 step：78/288 loss：1.333933\n",
      "【train】 epoch：1/1 step：79/288 loss：1.178218\n",
      "【train】 epoch：1/1 step：80/288 loss：1.115987\n",
      "【train】 epoch：1/1 step：81/288 loss：1.243826\n",
      "【train】 epoch：1/1 step：82/288 loss：1.257628\n",
      "【train】 epoch：1/1 step：83/288 loss：1.300196\n",
      "【train】 epoch：1/1 step：84/288 loss：1.150123\n",
      "【train】 epoch：1/1 step：85/288 loss：1.228148\n",
      "【train】 epoch：1/1 step：86/288 loss：1.694537\n",
      "【train】 epoch：1/1 step：87/288 loss：1.161840\n",
      "【train】 epoch：1/1 step：88/288 loss：1.467468\n",
      "【train】 epoch：1/1 step：89/288 loss：1.019744\n",
      "【train】 epoch：1/1 step：90/288 loss：1.219194\n",
      "【train】 epoch：1/1 step：91/288 loss：1.206174\n",
      "【train】 epoch：1/1 step：92/288 loss：1.262954\n",
      "【train】 epoch：1/1 step：93/288 loss：1.040884\n",
      "【train】 epoch：1/1 step：94/288 loss：1.109663\n",
      "【train】 epoch：1/1 step：95/288 loss：1.381250\n",
      "【train】 epoch：1/1 step：96/288 loss：1.596961\n",
      "【train】 epoch：1/1 step：97/288 loss：1.370488\n",
      "【train】 epoch：1/1 step：98/288 loss：1.207409\n",
      "【train】 epoch：1/1 step：99/288 loss：1.541173\n",
      "【train】 epoch：1/1 step：100/288 loss：1.304475\n",
      "【train】 epoch：1/1 step：101/288 loss：1.124711\n",
      "【train】 epoch：1/1 step：102/288 loss：1.453841\n",
      "【train】 epoch：1/1 step：103/288 loss：1.476226\n",
      "【train】 epoch：1/1 step：104/288 loss：1.134921\n",
      "【train】 epoch：1/1 step：105/288 loss：1.216847\n",
      "【train】 epoch：1/1 step：106/288 loss：1.111611\n",
      "【train】 epoch：1/1 step：107/288 loss：1.170798\n",
      "【train】 epoch：1/1 step：108/288 loss：1.099830\n",
      "【train】 epoch：1/1 step：109/288 loss：1.175350\n",
      "【train】 epoch：1/1 step：110/288 loss：1.275634\n",
      "【train】 epoch：1/1 step：111/288 loss：1.269557\n",
      "【train】 epoch：1/1 step：112/288 loss：1.226974\n",
      "【train】 epoch：1/1 step：113/288 loss：1.417455\n",
      "【train】 epoch：1/1 step：114/288 loss：0.993397\n",
      "【train】 epoch：1/1 step：115/288 loss：1.146699\n",
      "【train】 epoch：1/1 step：116/288 loss：1.276247\n",
      "【train】 epoch：1/1 step：117/288 loss：0.998058\n",
      "【train】 epoch：1/1 step：118/288 loss：1.327625\n",
      "【train】 epoch：1/1 step：119/288 loss：1.212288\n",
      "【train】 epoch：1/1 step：120/288 loss：1.300117\n",
      "【train】 epoch：1/1 step：121/288 loss：1.397870\n",
      "【train】 epoch：1/1 step：122/288 loss：1.438488\n",
      "【train】 epoch：1/1 step：123/288 loss：1.150949\n",
      "【train】 epoch：1/1 step：124/288 loss：1.362716\n",
      "【train】 epoch：1/1 step：125/288 loss：1.279940\n",
      "【train】 epoch：1/1 step：126/288 loss：1.256246\n",
      "【train】 epoch：1/1 step：127/288 loss：1.115882\n",
      "【train】 epoch：1/1 step：128/288 loss：1.080442\n",
      "【train】 epoch：1/1 step：129/288 loss：1.437163\n",
      "【train】 epoch：1/1 step：130/288 loss：1.383085\n",
      "【train】 epoch：1/1 step：131/288 loss：0.857048\n",
      "【train】 epoch：1/1 step：132/288 loss：0.888681\n",
      "【train】 epoch：1/1 step：133/288 loss：0.914897\n",
      "【train】 epoch：1/1 step：134/288 loss：1.331268\n",
      "【train】 epoch：1/1 step：135/288 loss：0.967801\n",
      "【train】 epoch：1/1 step：136/288 loss：1.264810\n",
      "【train】 epoch：1/1 step：137/288 loss：1.237838\n",
      "【train】 epoch：1/1 step：138/288 loss：1.009820\n",
      "【train】 epoch：1/1 step：139/288 loss：1.183830\n",
      "【train】 epoch：1/1 step：140/288 loss：1.064335\n",
      "【train】 epoch：1/1 step：141/288 loss：1.062840\n",
      "【train】 epoch：1/1 step：142/288 loss：0.937545\n",
      "【train】 epoch：1/1 step：143/288 loss：1.247384\n",
      "【train】 epoch：1/1 step：144/288 loss：1.218081\n",
      "【train】 epoch：1/1 step：145/288 loss：1.163406\n",
      "【train】 epoch：1/1 step：146/288 loss：0.944029\n",
      "【train】 epoch：1/1 step：147/288 loss：1.200018\n",
      "【train】 epoch：1/1 step：148/288 loss：1.228468\n",
      "【train】 epoch：1/1 step：149/288 loss：1.052908\n",
      "【train】 epoch：1/1 step：150/288 loss：0.963078\n",
      "【train】 epoch：1/1 step：151/288 loss：1.436611\n",
      "【train】 epoch：1/1 step：152/288 loss：1.430395\n",
      "【train】 epoch：1/1 step：153/288 loss：1.127057\n",
      "【train】 epoch：1/1 step：154/288 loss：0.953389\n",
      "【train】 epoch：1/1 step：155/288 loss：1.093063\n",
      "【train】 epoch：1/1 step：156/288 loss：0.775465\n",
      "【train】 epoch：1/1 step：157/288 loss：1.286086\n",
      "【train】 epoch：1/1 step：158/288 loss：0.880623\n",
      "【train】 epoch：1/1 step：159/288 loss：1.387407\n",
      "【train】 epoch：1/1 step：160/288 loss：0.989073\n",
      "【train】 epoch：1/1 step：161/288 loss：1.490471\n",
      "【train】 epoch：1/1 step：162/288 loss：1.382175\n",
      "【train】 epoch：1/1 step：163/288 loss：1.081529\n",
      "【train】 epoch：1/1 step：164/288 loss：1.234124\n",
      "【train】 epoch：1/1 step：165/288 loss：1.068005\n",
      "【train】 epoch：1/1 step：166/288 loss：1.399379\n",
      "【train】 epoch：1/1 step：167/288 loss：1.307397\n",
      "【train】 epoch：1/1 step：168/288 loss：1.217694\n",
      "【train】 epoch：1/1 step：169/288 loss：1.169973\n",
      "【train】 epoch：1/1 step：170/288 loss：1.248103\n",
      "【train】 epoch：1/1 step：171/288 loss：1.364963\n",
      "【train】 epoch：1/1 step：172/288 loss：1.196567\n",
      "【train】 epoch：1/1 step：173/288 loss：1.014305\n",
      "【train】 epoch：1/1 step：174/288 loss：1.382160\n",
      "【train】 epoch：1/1 step：175/288 loss：1.257878\n",
      "【train】 epoch：1/1 step：176/288 loss：1.100921\n",
      "【train】 epoch：1/1 step：177/288 loss：1.151421\n",
      "【train】 epoch：1/1 step：178/288 loss：1.128843\n",
      "【train】 epoch：1/1 step：179/288 loss：1.443850\n",
      "【train】 epoch：1/1 step：180/288 loss：1.199147\n",
      "【train】 epoch：1/1 step：181/288 loss：0.985395\n",
      "【train】 epoch：1/1 step：182/288 loss：1.360602\n",
      "【train】 epoch：1/1 step：183/288 loss：0.926609\n",
      "【train】 epoch：1/1 step：184/288 loss：1.175715\n",
      "【train】 epoch：1/1 step：185/288 loss：1.288782\n",
      "【train】 epoch：1/1 step：186/288 loss：1.101621\n",
      "【train】 epoch：1/1 step：187/288 loss：1.028020\n",
      "【train】 epoch：1/1 step：188/288 loss：1.185909\n",
      "【train】 epoch：1/1 step：189/288 loss：1.228693\n",
      "【train】 epoch：1/1 step：190/288 loss：1.303815\n",
      "【train】 epoch：1/1 step：191/288 loss：1.350091\n",
      "【train】 epoch：1/1 step：192/288 loss：1.182594\n",
      "【train】 epoch：1/1 step：193/288 loss：0.881745\n",
      "【train】 epoch：1/1 step：194/288 loss：1.080671\n",
      "【train】 epoch：1/1 step：195/288 loss：1.134372\n",
      "【train】 epoch：1/1 step：196/288 loss：1.279666\n",
      "【train】 epoch：1/1 step：197/288 loss：0.999262\n",
      "【train】 epoch：1/1 step：198/288 loss：1.233588\n",
      "【train】 epoch：1/1 step：199/288 loss：1.127193\n",
      "【train】 epoch：1/1 step：200/288 loss：1.081009\n",
      "【train】 epoch：1/1 step：201/288 loss：1.144990\n",
      "【train】 epoch：1/1 step：202/288 loss：1.082940\n",
      "【train】 epoch：1/1 step：203/288 loss：1.130525\n",
      "【train】 epoch：1/1 step：204/288 loss：0.997139\n",
      "【train】 epoch：1/1 step：205/288 loss：1.134829\n",
      "【train】 epoch：1/1 step：206/288 loss：1.475625\n",
      "【train】 epoch：1/1 step：207/288 loss：1.203632\n",
      "【train】 epoch：1/1 step：208/288 loss：0.870100\n",
      "【train】 epoch：1/1 step：209/288 loss：1.219508\n",
      "【train】 epoch：1/1 step：210/288 loss：1.364440\n",
      "【train】 epoch：1/1 step：211/288 loss：1.039248\n",
      "【train】 epoch：1/1 step：212/288 loss：1.518759\n",
      "【train】 epoch：1/1 step：213/288 loss：1.494666\n",
      "【train】 epoch：1/1 step：214/288 loss：0.989104\n",
      "【train】 epoch：1/1 step：215/288 loss：1.029006\n",
      "【train】 epoch：1/1 step：216/288 loss：1.057196\n",
      "【train】 epoch：1/1 step：217/288 loss：1.100752\n",
      "【train】 epoch：1/1 step：218/288 loss：1.041777\n",
      "【train】 epoch：1/1 step：219/288 loss：1.237018\n",
      "【train】 epoch：1/1 step：220/288 loss：0.954781\n",
      "【train】 epoch：1/1 step：221/288 loss：1.186973\n",
      "【train】 epoch：1/1 step：222/288 loss：1.064086\n",
      "【train】 epoch：1/1 step：223/288 loss：1.291728\n",
      "【train】 epoch：1/1 step：224/288 loss：1.317878\n",
      "【train】 epoch：1/1 step：225/288 loss：1.134282\n",
      "【train】 epoch：1/1 step：226/288 loss：0.875858\n",
      "【train】 epoch：1/1 step：227/288 loss：1.069505\n",
      "【train】 epoch：1/1 step：228/288 loss：1.134413\n",
      "【train】 epoch：1/1 step：229/288 loss：1.100052\n",
      "【train】 epoch：1/1 step：230/288 loss：1.429658\n",
      "【train】 epoch：1/1 step：231/288 loss：1.279451\n",
      "【train】 epoch：1/1 step：232/288 loss：1.137680\n",
      "【train】 epoch：1/1 step：233/288 loss：0.963616\n",
      "【train】 epoch：1/1 step：234/288 loss：1.007169\n",
      "【train】 epoch：1/1 step：235/288 loss：1.314614\n",
      "【train】 epoch：1/1 step：236/288 loss：1.224364\n",
      "【train】 epoch：1/1 step：237/288 loss：1.303066\n",
      "【train】 epoch：1/1 step：238/288 loss：1.037948\n",
      "【train】 epoch：1/1 step：239/288 loss：1.246919\n",
      "【train】 epoch：1/1 step：240/288 loss：1.279688\n",
      "【train】 epoch：1/1 step：241/288 loss：1.227261\n",
      "【train】 epoch：1/1 step：242/288 loss：1.089295\n",
      "【train】 epoch：1/1 step：243/288 loss：1.183564\n",
      "【train】 epoch：1/1 step：244/288 loss：1.298460\n",
      "【train】 epoch：1/1 step：245/288 loss：1.199295\n",
      "【train】 epoch：1/1 step：246/288 loss：1.241092\n",
      "【train】 epoch：1/1 step：247/288 loss：0.952701\n",
      "【train】 epoch：1/1 step：248/288 loss：1.287577\n",
      "【train】 epoch：1/1 step：249/288 loss：1.115751\n",
      "【train】 epoch：1/1 step：250/288 loss：1.325338\n",
      "【train】 epoch：1/1 step：251/288 loss：1.512999\n",
      "【train】 epoch：1/1 step：252/288 loss：1.278094\n",
      "【train】 epoch：1/1 step：253/288 loss：0.872439\n",
      "【train】 epoch：1/1 step：254/288 loss：1.207770\n",
      "【train】 epoch：1/1 step：255/288 loss：1.315393\n",
      "【train】 epoch：1/1 step：256/288 loss：1.238823\n",
      "【train】 epoch：1/1 step：257/288 loss：0.997686\n",
      "【train】 epoch：1/1 step：258/288 loss：1.132895\n",
      "【train】 epoch：1/1 step：259/288 loss：1.148213\n",
      "【train】 epoch：1/1 step：260/288 loss：1.149038\n",
      "【train】 epoch：1/1 step：261/288 loss：1.368665\n",
      "【train】 epoch：1/1 step：262/288 loss：1.480891\n",
      "【train】 epoch：1/1 step：263/288 loss：0.966723\n",
      "【train】 epoch：1/1 step：264/288 loss：1.080926\n",
      "【train】 epoch：1/1 step：265/288 loss：1.342254\n",
      "【train】 epoch：1/1 step：266/288 loss：1.312531\n",
      "【train】 epoch：1/1 step：267/288 loss：1.394116\n",
      "【train】 epoch：1/1 step：268/288 loss：1.302548\n",
      "【train】 epoch：1/1 step：269/288 loss：1.091738\n",
      "【train】 epoch：1/1 step：270/288 loss：1.136767\n",
      "【train】 epoch：1/1 step：271/288 loss：1.106968\n",
      "【train】 epoch：1/1 step：272/288 loss：1.219203\n",
      "【train】 epoch：1/1 step：273/288 loss：1.189411\n",
      "【train】 epoch：1/1 step：274/288 loss：1.125547\n",
      "【train】 epoch：1/1 step：275/288 loss：0.950517\n",
      "【train】 epoch：1/1 step：276/288 loss：1.115714\n",
      "【train】 epoch：1/1 step：277/288 loss：1.081522\n",
      "【train】 epoch：1/1 step：278/288 loss：1.164217\n",
      "【train】 epoch：1/1 step：279/288 loss：1.317440\n",
      "【train】 epoch：1/1 step：280/288 loss：1.084627\n",
      "【train】 epoch：1/1 step：281/288 loss：1.269013\n",
      "【train】 epoch：1/1 step：282/288 loss：1.029673\n",
      "【train】 epoch：1/1 step：283/288 loss：1.348992\n",
      "【train】 epoch：1/1 step：284/288 loss：0.893437\n",
      "【train】 epoch：1/1 step：285/288 loss：1.082008\n",
      "【train】 epoch：1/1 step：286/288 loss：0.996188\n",
      "【train】 epoch：1/1 step：287/288 loss：1.411925\n",
      "【train】 epoch：1/1 step：288/288 loss：0.896896\n",
      "耗时：1.427138376235962分钟\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[3, 0, 3, 1, 0, 5, 5, 5, 4, 1, 0, 0, 2, 0, 3, 2, 2, 0, 3, 0, 1, 0, 5, 0, 5, 0, 0, 2, 1, 0, 2, 5, 4, 0, 3, 2, 3, 3, 1, 2, 0, 1, 5, 4, 0, 0, 0, 3, 2, 5, 2, 0, 1, 0, 1, 5, 0, 5, 0, 0, 3, 0, 3, 1, 1, 0, 1, 2, 1, 1, 2, 2, 5, 1, 2, 3, 0, 2, 3, 0, 5, 1, 2, 1, 2, 0, 0, 4, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 4, 2, 5, 3, 5, 0, 5, 0, 0, 0, 5, 3, 3, 3, 2, 5, 0, 0, 0, 0, 2, 2, 0, 1, 2, 5, 0, 0, 1, 2, 2, 4, 1, 0, 4, 1, 5, 0, 0, 4, 5, 0, 0, 3, 5, 5, 4, 3, 2, 1, 0, 2, 5, 0, 3, 0, 0, 2, 5, 3, 0, 0, 0, 5, 3, 2, 3, 0, 3, 0, 0, 0, 3, 2, 0, 4, 5, 5, 2, 2, 5, 4, 0, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 2, 3, 3, 0, 5, 0, 5, 4, 1, 5, 3, 1, 0, 5, 5, 5, 3, 0, 5, 3, 0, 3, 2, 0, 3, 4, 4, 0, 3, 1, 1, 4, 0, 5, 0, 2, 2, 3, 3, 0, 0, 1, 3, 3, 5, 5, 3, 0, 4, 0, 3, 0, 0, 0, 0, 1, 2, 0, 3, 4, 2, 1, 1, 3, 0, 2, 0, 3, 0, 5, 2, 1, 1, 0, 0, 3, 1, 1, 5, 4, 2, 1, 0, 1, 5, 0, 5, 2, 5, 4, 3, 1, 3, 0, 4, 0, 4, 5, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 3, 0, 2, 2, 2, 0, 1, 2, 3, 5, 0, 1, 3, 0, 5, 3, 0, 0, 1, 0, 0, 2, 4, 1, 0, 3, 2, 0, 0, 2, 0, 0, 3, 1, 4, 1, 0, 4, 1, 3, 0, 0, 5, 0, 2, 5, 1, 3, 2, 2, 5, 2, 3, 0, 0, 0, 4, 5, 2, 5, 4, 4, 2, 0, 4, 3, 4, 3, 0, 0, 5, 3, 5, 3, 0, 1, 3, 0, 4, 0, 0, 3, 2, 2, 0, 0, 3, 0, 2, 5, 3, 0, 5, 1, 2, 0, 0, 4, 5, 0, 1, 0, 5, 1, 5, 3, 5, 0, 5, 5, 3, 1, 0, 3, 5, 3, 5, 5, 5, 3, 1, 1, 1, 3, 0, 1, 0, 0, 1, 2, 0, 2, 3, 0, 5, 0, 0, 5, 1, 5, 1, 0, 3, 0, 2, 0, 2, 0, 4, 5, 0, 4, 1, 2, 4, 3, 2, 1, 0, 0, 3, 4, 5, 0, 1, 5, 3, 2, 0, 0, 2, 3, 3, 3, 3, 2, 0, 5, 2, 0, 0, 3, 0, 3, 0, 0, 5, 2, 2, 1, 4, 0, 0, 3, 0, 1, 2, 0, 0, 5, 5, 0, 5, 5, 3, 0, 3, 5, 3, 3, 0, 0, 3, 5, 3, 5, 0, 5, 0, 2, 4, 4, 0, 2, 5, 1, 3, 0, 0, 0, 2, 0, 1, 0, 3, 0, 5, 0, 0, 5, 2, 5, 0, 0, 3, 5, 4, 1, 2, 0, 0, 3, 5, 0, 5, 2, 0, 2, 4, 1, 1, 3, 2, 0, 1, 0, 4, 5, 0, 0, 1, 0, 5, 5, 0, 5, 2, 2, 0, 2, 0, 1, 2, 5, 2, 0, 3, 1, 3, 0, 2, 2, 4, 0, 0, 3, 0, 5, 5, 0, 2, 5, 0, 3, 1, 0, 4, 0, 2, 0, 0, 0, 5, 0, 5, 3, 0, 0, 0, 0, 2, 3, 0, 4, 0, 4, 0, 3, 1, 5, 1, 0, 0, 3, 1, 4, 2, 2, 1, 1, 3, 4, 4, 0, 2, 1, 1, 4, 0, 0, 0, 0, 4, 4, 0, 4, 2, 0, 0, 1, 3, 2, 0, 5, 1, 2, 5, 0, 3, 3, 0, 0, 4, 0, 3, 5, 0, 1, 1, 1, 0, 2, 2, 5, 0, 4, 5, 4, 1, 2, 0, 0, 0, 1, 0, 0, 5, 4, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 0, 0, 5, 0, 2, 2, 1, 5, 3, 3, 5, 0, 5, 1, 0, 0, 0, 3, 2, 2, 1, 2, 0, 0, 3, 3, 3, 2, 5, 2, 0, 3, 0, 5, 2, 5, 1, 4, 0, 2, 2, 0, 0, 5, 4, 0, 3, 5, 0, 1, 1, 0, 1, 3, 3, 1, 2, 4, 5, 0, 1, 4, 5, 1, 0, 5, 1, 0, 5, 1, 0, 0, 2, 1, 0, 3, 1, 3, 4, 3, 1, 0, 3, 2, 3, 3, 0] [3, 0, 3, 1, 3, 5, 0, 1, 4, 1, 0, 0, 0, 0, 0, 3, 2, 5, 3, 3, 1, 4, 0, 0, 5, 0, 0, 1, 4, 2, 2, 5, 4, 2, 3, 2, 3, 3, 1, 2, 0, 1, 4, 3, 5, 0, 0, 1, 2, 1, 2, 0, 1, 1, 1, 5, 0, 5, 0, 0, 0, 0, 2, 1, 1, 4, 1, 2, 1, 1, 0, 2, 5, 1, 2, 0, 3, 1, 3, 0, 5, 0, 4, 0, 2, 1, 0, 3, 1, 1, 3, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 1, 0, 3, 2, 5, 0, 0, 0, 5, 1, 0, 1, 4, 3, 5, 0, 2, 5, 0, 2, 1, 0, 3, 2, 2, 1, 3, 5, 1, 0, 1, 2, 2, 4, 5, 0, 3, 0, 5, 0, 0, 4, 1, 3, 0, 3, 5, 5, 0, 1, 2, 1, 1, 4, 5, 4, 2, 3, 0, 0, 0, 0, 1, 0, 0, 5, 3, 0, 1, 0, 3, 3, 0, 1, 5, 3, 1, 3, 1, 1, 2, 2, 5, 4, 0, 0, 0, 0, 1, 2, 0, 0, 4, 2, 1, 2, 0, 4, 0, 5, 0, 5, 2, 1, 5, 0, 1, 0, 5, 5, 1, 3, 0, 5, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 1, 0, 3, 3, 5, 1, 2, 1, 3, 0, 0, 4, 1, 4, 5, 5, 5, 3, 0, 4, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 4, 0, 5, 2, 1, 1, 0, 0, 4, 1, 5, 5, 0, 2, 1, 3, 1, 0, 0, 1, 1, 5, 0, 3, 0, 1, 0, 0, 0, 3, 1, 0, 0, 0, 1, 1, 1, 0, 4, 0, 1, 2, 4, 3, 2, 2, 4, 0, 1, 2, 0, 5, 3, 5, 0, 0, 5, 3, 0, 0, 1, 0, 0, 2, 4, 1, 0, 3, 2, 0, 5, 4, 0, 0, 2, 1, 4, 1, 0, 4, 1, 3, 0, 0, 1, 0, 0, 0, 1, 0, 2, 2, 0, 2, 3, 0, 3, 3, 4, 1, 2, 5, 3, 0, 0, 2, 3, 4, 4, 1, 0, 0, 1, 0, 5, 2, 0, 1, 0, 0, 0, 0, 0, 3, 2, 2, 3, 0, 0, 0, 2, 1, 2, 0, 5, 1, 0, 0, 0, 4, 0, 0, 1, 0, 5, 5, 0, 1, 0, 0, 5, 5, 0, 1, 0, 1, 2, 3, 5, 5, 5, 4, 5, 1, 5, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2, 3, 5, 0, 5, 1, 1, 1, 1, 0, 3, 0, 2, 3, 4, 3, 0, 5, 0, 4, 1, 0, 4, 3, 1, 0, 3, 0, 0, 4, 5, 0, 1, 1, 0, 2, 0, 0, 0, 3, 0, 0, 3, 2, 3, 1, 2, 3, 5, 3, 0, 0, 0, 3, 1, 2, 5, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 5, 5, 2, 5, 1, 1, 0, 0, 1, 3, 0, 0, 0, 0, 1, 2, 1, 0, 1, 1, 2, 3, 3, 0, 3, 3, 1, 4, 0, 3, 0, 2, 0, 5, 0, 3, 1, 1, 0, 0, 2, 2, 5, 1, 4, 0, 5, 4, 3, 1, 0, 2, 2, 1, 0, 0, 5, 0, 0, 4, 1, 0, 3, 2, 0, 0, 0, 3, 5, 2, 0, 1, 0, 5, 5, 0, 5, 2, 1, 2, 2, 0, 0, 2, 2, 3, 0, 0, 5, 3, 0, 2, 2, 3, 0, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 1, 0, 4, 0, 4, 0, 0, 3, 0, 1, 5, 0, 0, 0, 0, 0, 2, 1, 0, 4, 0, 4, 4, 3, 1, 5, 5, 0, 0, 2, 3, 0, 1, 0, 1, 0, 0, 4, 3, 1, 5, 0, 1, 3, 0, 3, 0, 0, 4, 2, 2, 0, 0, 0, 0, 1, 3, 2, 0, 5, 1, 2, 5, 0, 1, 0, 0, 0, 4, 0, 2, 0, 0, 1, 1, 1, 0, 0, 3, 5, 3, 1, 5, 4, 0, 2, 0, 3, 0, 1, 1, 1, 0, 2, 0, 0, 1, 3, 0, 0, 0, 5, 0, 4, 0, 0, 1, 0, 1, 3, 1, 1, 0, 3, 5, 3, 0, 0, 0, 0, 3, 0, 1, 5, 1, 3, 3, 0, 3, 4, 3, 4, 5, 3, 0, 5, 0, 5, 3, 5, 1, 4, 0, 1, 2, 0, 1, 1, 4, 0, 0, 0, 2, 1, 1, 5, 1, 0, 2, 5, 2, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 2, 3, 5, 0, 0, 2, 1, 0, 3, 0, 1, 2, 4, 5, 0, 3, 0, 1, 3, 3] ['其他', '喜好', '悲伤', '厌恶', '愤怒', '高兴']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          其他       0.63      0.70      0.66       273\n",
      "          喜好       0.48      0.67      0.56       112\n",
      "          悲伤       0.64      0.52      0.57       114\n",
      "          厌恶       0.39      0.33      0.36       120\n",
      "          愤怒       0.51      0.47      0.49        62\n",
      "          高兴       0.71      0.55      0.62       119\n",
      "\n",
      "    accuracy                           0.57       800\n",
      "   macro avg       0.56      0.54      0.54       800\n",
      "weighted avg       0.58      0.57      0.57       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!~/anaconda3/envs/python39_p13/bin/python multi-gpu-dataparallel-cls.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53473e1e-d5e1-4778-aced-dea1492a6249",
   "metadata": {},
   "source": [
    "## Distributed分布式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e63bd3-82f7-4f42-a683-23349249efbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[24386] Initializing process group with: {'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'RANK': '2', 'WORLD_SIZE': '4', 'LOCAL_RANK': '2'}\n",
      "[24385] Initializing process group with: {'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'RANK': '1', 'WORLD_SIZE': '4', 'LOCAL_RANK': '1'}\n",
      "MASTER_ADDR 127.0.0.1\n",
      "MASTER_PORT 29500\n",
      "RANK 0\n",
      "WORLD_SIZE 4\n",
      "LOCAL_RANK 0\n",
      "[24384] Initializing process group with: {'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'RANK': '0', 'WORLD_SIZE': '4', 'LOCAL_RANK': '0'}\n",
      "[24387] Initializing process group with: {'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'RANK': '3', 'WORLD_SIZE': '4', 'LOCAL_RANK': '3'}\n",
      "[24384] rank = 0, world_size = 4, n = 1, device_ids = [0] \n",
      "[24385] rank = 1, world_size = 4, n = 1, device_ids = [1] \n",
      "[24386] rank = 2, world_size = 4, n = 1, device_ids = [2] \n",
      "[24387] rank = 3, world_size = 4, n = 1, device_ids = [3] \n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "【train】 epoch：1/1 step：1/72 loss：1.783924\n",
      "【train】 epoch：1/1 step：2/72 loss：1.727741\n",
      "【train】 epoch：1/1 step：3/72 loss：1.672189\n",
      "【train】 epoch：1/1 step：4/72 loss：1.701559\n",
      "【train】 epoch：1/1 step：5/72 loss：1.760728\n",
      "【train】 epoch：1/1 step：6/72 loss：1.682931\n",
      "【train】 epoch：1/1 step：7/72 loss：1.635502\n",
      "【train】 epoch：1/1 step：8/72 loss：1.639267\n",
      "【train】 epoch：1/1 step：9/72 loss：1.651427\n",
      "【train】 epoch：1/1 step：10/72 loss：1.603941\n",
      "【train】 epoch：1/1 step：11/72 loss：1.635462\n",
      "【train】 epoch：1/1 step：12/72 loss：1.600205\n",
      "【train】 epoch：1/1 step：13/72 loss：1.592772\n",
      "【train】 epoch：1/1 step：14/72 loss：1.564878\n",
      "【train】 epoch：1/1 step：15/72 loss：1.515855\n",
      "【train】 epoch：1/1 step：16/72 loss：1.587446\n",
      "【train】 epoch：1/1 step：17/72 loss：1.511317\n",
      "【train】 epoch：1/1 step：18/72 loss：1.405862\n",
      "【train】 epoch：1/1 step：19/72 loss：1.482137\n",
      "【train】 epoch：1/1 step：20/72 loss：1.456917\n",
      "【train】 epoch：1/1 step：21/72 loss：1.381254\n",
      "【train】 epoch：1/1 step：22/72 loss：1.368268\n",
      "【train】 epoch：1/1 step：23/72 loss：1.414969\n",
      "【train】 epoch：1/1 step：24/72 loss：1.429376\n",
      "【train】 epoch：1/1 step：25/72 loss：1.433588\n",
      "【train】 epoch：1/1 step：26/72 loss：1.337312\n",
      "【train】 epoch：1/1 step：27/72 loss：1.417993\n",
      "【train】 epoch：1/1 step：28/72 loss：1.366267\n",
      "【train】 epoch：1/1 step：29/72 loss：1.287375\n",
      "【train】 epoch：1/1 step：30/72 loss：1.399702\n",
      "【train】 epoch：1/1 step：31/72 loss：1.275151\n",
      "【train】 epoch：1/1 step：32/72 loss：1.339190\n",
      "【train】 epoch：1/1 step：33/72 loss：1.275933\n",
      "【train】 epoch：1/1 step：34/72 loss：1.442395\n",
      "【train】 epoch：1/1 step：35/72 loss：1.339748\n",
      "【train】 epoch：1/1 step：36/72 loss：1.354957\n",
      "【train】 epoch：1/1 step：37/72 loss：1.278903\n",
      "【train】 epoch：1/1 step：38/72 loss：1.190139\n",
      "【train】 epoch：1/1 step：39/72 loss：1.276584\n",
      "【train】 epoch：1/1 step：40/72 loss：1.206730\n",
      "【train】 epoch：1/1 step：41/72 loss：1.160640\n",
      "【train】 epoch：1/1 step：42/72 loss：1.248005\n",
      "【train】 epoch：1/1 step：43/72 loss：1.202195\n",
      "【train】 epoch：1/1 step：44/72 loss：1.284843\n",
      "【train】 epoch：1/1 step：45/72 loss：1.102668\n",
      "【train】 epoch：1/1 step：46/72 loss：1.228355\n",
      "【train】 epoch：1/1 step：47/72 loss：1.148646\n",
      "【train】 epoch：1/1 step：48/72 loss：1.171604\n",
      "【train】 epoch：1/1 step：49/72 loss：1.204826\n",
      "【train】 epoch：1/1 step：50/72 loss：1.246058\n",
      "【train】 epoch：1/1 step：51/72 loss：1.240028\n",
      "【train】 epoch：1/1 step：52/72 loss：1.116595\n",
      "【train】 epoch：1/1 step：53/72 loss：1.068061\n",
      "【train】 epoch：1/1 step：54/72 loss：1.219132\n",
      "【train】 epoch：1/1 step：55/72 loss：1.186736\n",
      "【train】 epoch：1/1 step：56/72 loss：1.188521\n",
      "【train】 epoch：1/1 step：57/72 loss：1.169731\n",
      "【train】 epoch：1/1 step：58/72 loss：1.298004\n",
      "【train】 epoch：1/1 step：59/72 loss：1.243661\n",
      "【train】 epoch：1/1 step：60/72 loss：1.175477\n",
      "【train】 epoch：1/1 step：61/72 loss：1.081908\n",
      "【train】 epoch：1/1 step：62/72 loss：1.182464\n",
      "【train】 epoch：1/1 step：63/72 loss：1.267214\n",
      "【train】 epoch：1/1 step：64/72 loss：1.183699\n",
      "【train】 epoch：1/1 step：65/72 loss：1.137232\n",
      "【train】 epoch：1/1 step：66/72 loss：1.108094\n",
      "【train】 epoch：1/1 step：67/72 loss：1.200392\n",
      "【train】 epoch：1/1 step：68/72 loss：1.142727\n",
      "【train】 epoch：1/1 step：69/72 loss：1.100268\n",
      "【train】 epoch：1/1 step：70/72 loss：1.190680\n",
      "【train】 epoch：1/1 step：71/72 loss：1.179150\n",
      "【train】 epoch：1/1 step：72/72 loss：1.120926\n",
      "耗时：0.2986536264419556分钟\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          其他       0.64      0.73      0.68       273\n",
      "          喜好       0.45      0.62      0.52       112\n",
      "          悲伤       0.62      0.37      0.46       114\n",
      "          厌恶       0.41      0.19      0.26       120\n",
      "          愤怒       0.34      0.63      0.44        62\n",
      "          高兴       0.72      0.60      0.65       119\n",
      "\n",
      "    accuracy                           0.55       800\n",
      "   macro avg       0.53      0.52      0.50       800\n",
      "weighted avg       0.57      0.55      0.54       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!~/anaconda3/envs/python39_p13/bin/python -m torch.distributed.launch --nnode=1 --node_rank=0 --nproc_per_node=4 multi-gpu-distributed-cls.py --local_world_size=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513e304-5d90-4dec-9f80-3e9ca382e2a4",
   "metadata": {},
   "source": [
    "## distributed分布式训练-multiprocess启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e203415-95f9-4512-9c74-6ae7a1acb40e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33473] rank = 0, world_size = 4, n = 1, device_ids = [0] \n",
      "[33474] rank = 1, world_size = 4, n = 1, device_ids = [1] \n",
      "[33475] rank = 2, world_size = 4, n = 1, device_ids = [2] \n",
      "[33476] rank = 3, world_size = 4, n = 1, device_ids = [3] \n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "【train】 epoch：1/1 step：1/72 loss：1.783924\n",
      "【train】 epoch：1/1 step：2/72 loss：1.727741\n",
      "【train】 epoch：1/1 step：3/72 loss：1.672189\n",
      "【train】 epoch：1/1 step：4/72 loss：1.701559\n",
      "【train】 epoch：1/1 step：5/72 loss：1.760728\n",
      "【train】 epoch：1/1 step：6/72 loss：1.682931\n",
      "【train】 epoch：1/1 step：7/72 loss：1.635502\n",
      "【train】 epoch：1/1 step：8/72 loss：1.639267\n",
      "【train】 epoch：1/1 step：9/72 loss：1.651427\n",
      "【train】 epoch：1/1 step：10/72 loss：1.603941\n",
      "【train】 epoch：1/1 step：11/72 loss：1.635462\n",
      "【train】 epoch：1/1 step：12/72 loss：1.600205\n",
      "【train】 epoch：1/1 step：13/72 loss：1.592772\n",
      "【train】 epoch：1/1 step：14/72 loss：1.564878\n",
      "【train】 epoch：1/1 step：15/72 loss：1.515855\n",
      "【train】 epoch：1/1 step：16/72 loss：1.587446\n",
      "【train】 epoch：1/1 step：17/72 loss：1.511317\n",
      "【train】 epoch：1/1 step：18/72 loss：1.405862\n",
      "【train】 epoch：1/1 step：19/72 loss：1.482137\n",
      "【train】 epoch：1/1 step：20/72 loss：1.456917\n",
      "【train】 epoch：1/1 step：21/72 loss：1.381254\n",
      "【train】 epoch：1/1 step：22/72 loss：1.368268\n",
      "【train】 epoch：1/1 step：23/72 loss：1.414969\n",
      "【train】 epoch：1/1 step：24/72 loss：1.429376\n",
      "【train】 epoch：1/1 step：25/72 loss：1.433588\n",
      "【train】 epoch：1/1 step：26/72 loss：1.337312\n",
      "【train】 epoch：1/1 step：27/72 loss：1.417993\n",
      "【train】 epoch：1/1 step：28/72 loss：1.366267\n",
      "【train】 epoch：1/1 step：29/72 loss：1.287375\n",
      "【train】 epoch：1/1 step：30/72 loss：1.399702\n",
      "【train】 epoch：1/1 step：31/72 loss：1.275151\n",
      "【train】 epoch：1/1 step：32/72 loss：1.339190\n",
      "【train】 epoch：1/1 step：33/72 loss：1.275933\n",
      "【train】 epoch：1/1 step：34/72 loss：1.442395\n",
      "【train】 epoch：1/1 step：35/72 loss：1.339748\n",
      "【train】 epoch：1/1 step：36/72 loss：1.354957\n",
      "【train】 epoch：1/1 step：37/72 loss：1.278903\n",
      "【train】 epoch：1/1 step：38/72 loss：1.190139\n",
      "【train】 epoch：1/1 step：39/72 loss：1.276584\n",
      "【train】 epoch：1/1 step：40/72 loss：1.206730\n",
      "【train】 epoch：1/1 step：41/72 loss：1.160640\n",
      "【train】 epoch：1/1 step：42/72 loss：1.248005\n",
      "【train】 epoch：1/1 step：43/72 loss：1.202195\n",
      "【train】 epoch：1/1 step：44/72 loss：1.284843\n",
      "【train】 epoch：1/1 step：45/72 loss：1.102668\n",
      "【train】 epoch：1/1 step：46/72 loss：1.228355\n",
      "【train】 epoch：1/1 step：47/72 loss：1.148646\n",
      "【train】 epoch：1/1 step：48/72 loss：1.171604\n",
      "【train】 epoch：1/1 step：49/72 loss：1.204826\n",
      "【train】 epoch：1/1 step：50/72 loss：1.246058\n",
      "【train】 epoch：1/1 step：51/72 loss：1.240028\n",
      "【train】 epoch：1/1 step：52/72 loss：1.116595\n",
      "【train】 epoch：1/1 step：53/72 loss：1.068061\n",
      "【train】 epoch：1/1 step：54/72 loss：1.219132\n",
      "【train】 epoch：1/1 step：55/72 loss：1.186736\n",
      "【train】 epoch：1/1 step：56/72 loss：1.188521\n",
      "【train】 epoch：1/1 step：57/72 loss：1.169731\n",
      "【train】 epoch：1/1 step：58/72 loss：1.298004\n",
      "【train】 epoch：1/1 step：59/72 loss：1.243661\n",
      "【train】 epoch：1/1 step：60/72 loss：1.175477\n",
      "【train】 epoch：1/1 step：61/72 loss：1.081908\n",
      "【train】 epoch：1/1 step：62/72 loss：1.182464\n",
      "【train】 epoch：1/1 step：63/72 loss：1.267214\n",
      "【train】 epoch：1/1 step：64/72 loss：1.183699\n",
      "【train】 epoch：1/1 step：65/72 loss：1.137232\n",
      "【train】 epoch：1/1 step：66/72 loss：1.108094\n",
      "【train】 epoch：1/1 step：67/72 loss：1.200392\n",
      "【train】 epoch：1/1 step：68/72 loss：1.142727\n",
      "【train】 epoch：1/1 step：69/72 loss：1.100268\n",
      "【train】 epoch：1/1 step：70/72 loss：1.190680\n",
      "【train】 epoch：1/1 step：71/72 loss：1.179150\n",
      "【train】 epoch：1/1 step：72/72 loss：1.120926\n",
      "耗时：0.3739401698112488分钟\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          其他       0.64      0.73      0.68       273\n",
      "          喜好       0.45      0.62      0.52       112\n",
      "          悲伤       0.62      0.37      0.46       114\n",
      "          厌恶       0.41      0.19      0.26       120\n",
      "          愤怒       0.34      0.63      0.44        62\n",
      "          高兴       0.72      0.60      0.65       119\n",
      "\n",
      "    accuracy                           0.55       800\n",
      "   macro avg       0.53      0.52      0.50       800\n",
      "weighted avg       0.57      0.55      0.54       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!~/anaconda3/envs/python39_p13/bin/python multi-gpu-distributed-mp-cls.py --local_world_size=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979ca6d-0a8f-4dee-8f35-69c1ece86ead",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AMP混合精度训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835f958-142c-484d-bd4f-dca300467b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d890f6fb-732d-42ec-ba34-0197dd1875b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44505] rank = 0, world_size = 4, n = 1, device_ids = [0] \n",
      "[44507] rank = 2, world_size = 4, n = 1, device_ids = [2] \n",
      "[44508] rank = 3, world_size = 4, n = 1, device_ids = [3] \n",
      "[44506] rank = 1, world_size = 4, n = 1, device_ids = [1] \n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "【train】 epoch：1/1 step：1/72 loss：1.758331\n",
      "【train】 epoch：1/1 step：2/72 loss：1.810211\n",
      "【train】 epoch：1/1 step：3/72 loss：1.793640\n",
      "【train】 epoch：1/1 step：4/72 loss：1.807617\n",
      "【train】 epoch：1/1 step：5/72 loss：1.916992\n",
      "【train】 epoch：1/1 step：6/72 loss：1.731201\n",
      "【train】 epoch：1/1 step：7/72 loss：1.768982\n",
      "【train】 epoch：1/1 step：8/72 loss：1.791107\n",
      "【train】 epoch：1/1 step：9/72 loss：1.745056\n",
      "【train】 epoch：1/1 step：10/72 loss：1.797241\n",
      "【train】 epoch：1/1 step：11/72 loss：1.694946\n",
      "【train】 epoch：1/1 step：12/72 loss：1.758453\n",
      "【train】 epoch：1/1 step：13/72 loss：1.898865\n",
      "【train】 epoch：1/1 step：14/72 loss：1.764923\n",
      "【train】 epoch：1/1 step：15/72 loss：1.721802\n",
      "【train】 epoch：1/1 step：16/72 loss：1.797974\n",
      "【train】 epoch：1/1 step：17/72 loss：1.736664\n",
      "【train】 epoch：1/1 step：18/72 loss：1.638367\n",
      "【train】 epoch：1/1 step：19/72 loss：1.778290\n",
      "【train】 epoch：1/1 step：20/72 loss：1.875946\n",
      "【train】 epoch：1/1 step：21/72 loss：1.790161\n",
      "【train】 epoch：1/1 step：22/72 loss：1.594299\n",
      "【train】 epoch：1/1 step：23/72 loss：1.808533\n",
      "【train】 epoch：1/1 step：24/72 loss：1.814423\n",
      "【train】 epoch：1/1 step：25/72 loss：1.821747\n",
      "【train】 epoch：1/1 step：26/72 loss：1.789001\n",
      "【train】 epoch：1/1 step：27/72 loss：1.716461\n",
      "【train】 epoch：1/1 step：28/72 loss：1.802002\n",
      "【train】 epoch：1/1 step：29/72 loss：1.708435\n",
      "【train】 epoch：1/1 step：30/72 loss：1.901764\n",
      "【train】 epoch：1/1 step：31/72 loss：1.683136\n",
      "【train】 epoch：1/1 step：32/72 loss：1.785614\n",
      "【train】 epoch：1/1 step：33/72 loss：1.821136\n",
      "【train】 epoch：1/1 step：34/72 loss：1.810760\n",
      "【train】 epoch：1/1 step：35/72 loss：1.676025\n",
      "【train】 epoch：1/1 step：36/72 loss：1.780426\n",
      "【train】 epoch：1/1 step：37/72 loss：1.819977\n",
      "【train】 epoch：1/1 step：38/72 loss：1.821533\n",
      "【train】 epoch：1/1 step：39/72 loss：1.792175\n",
      "【train】 epoch：1/1 step：40/72 loss：1.785156\n",
      "【train】 epoch：1/1 step：41/72 loss：1.735809\n",
      "【train】 epoch：1/1 step：42/72 loss：1.848022\n",
      "【train】 epoch：1/1 step：43/72 loss：1.773376\n",
      "【train】 epoch：1/1 step：44/72 loss：1.703705\n",
      "【train】 epoch：1/1 step：45/72 loss：1.724426\n",
      "【train】 epoch：1/1 step：46/72 loss：1.833099\n",
      "【train】 epoch：1/1 step：47/72 loss：1.864899\n",
      "【train】 epoch：1/1 step：48/72 loss：1.868835\n",
      "【train】 epoch：1/1 step：49/72 loss：1.659241\n",
      "【train】 epoch：1/1 step：50/72 loss：1.785675\n",
      "【train】 epoch：1/1 step：51/72 loss：1.827026\n",
      "【train】 epoch：1/1 step：52/72 loss：1.652496\n",
      "【train】 epoch：1/1 step：53/72 loss：1.690643\n",
      "【train】 epoch：1/1 step：54/72 loss：1.771637\n",
      "【train】 epoch：1/1 step：55/72 loss：1.774017\n",
      "【train】 epoch：1/1 step：56/72 loss：1.868439\n",
      "【train】 epoch：1/1 step：57/72 loss：1.779236\n",
      "【train】 epoch：1/1 step：58/72 loss：1.909912\n",
      "【train】 epoch：1/1 step：59/72 loss：1.792297\n",
      "【train】 epoch：1/1 step：60/72 loss：1.744476\n",
      "【train】 epoch：1/1 step：61/72 loss：1.791962\n",
      "【train】 epoch：1/1 step：62/72 loss：1.755737\n",
      "【train】 epoch：1/1 step：63/72 loss：1.722015\n",
      "【train】 epoch：1/1 step：64/72 loss：1.797150\n",
      "【train】 epoch：1/1 step：65/72 loss：1.820892\n",
      "【train】 epoch：1/1 step：66/72 loss：1.802216\n",
      "【train】 epoch：1/1 step：67/72 loss：1.749512\n",
      "【train】 epoch：1/1 step：68/72 loss：1.870728\n",
      "【train】 epoch：1/1 step：69/72 loss：1.874634\n",
      "【train】 epoch：1/1 step：70/72 loss：1.819885\n",
      "【train】 epoch：1/1 step：71/72 loss：1.792480\n",
      "【train】 epoch：1/1 step：72/72 loss：1.819057\n",
      "耗时：0.2881103197733561分钟\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          其他       0.35      0.88      0.50       273\n",
      "          喜好       0.00      0.00      0.00       112\n",
      "          悲伤       0.00      0.00      0.00       114\n",
      "          厌恶       1.00      0.01      0.02       120\n",
      "          愤怒       0.17      0.29      0.21        62\n",
      "          高兴       0.00      0.00      0.00       119\n",
      "\n",
      "    accuracy                           0.32       800\n",
      "   macro avg       0.25      0.20      0.12       800\n",
      "weighted avg       0.28      0.32      0.19       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!~/anaconda3/envs/python39_p13/bin/python multi-gpu-distributed-mp-amp-cls.py --local_world_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f16e2c-9407-4152-8e60-f739fc90b99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7979822-baac-4e5c-b729-487ee9be86c9",
   "metadata": {},
   "source": [
    "## deepspeed分布式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a82cdd6f-94a8-4130-bb2e-df8e89fa0573",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-20 09:27:40,262] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-06-20 09:27:40,637] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-06-20 09:27:40,664] [INFO] [runner.py:555:main] cmd = /home/ec2-user/anaconda3/envs/python39_p13/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=11222 --enable_each_rank_log=None multi-gpu-deepspeed-cls.py\n",
      "[2023-06-20 09:27:41,802] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-06-20 09:27:42,172] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2023-06-20 09:27:42,173] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2023-06-20 09:27:42,173] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2023-06-20 09:27:42,173] [INFO] [launch.py:163:main] dist_world_size=4\n",
      "[2023-06-20 09:27:42,173] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2023-06-20 09:27:43,367] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-06-20 09:27:43,383] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-06-20 09:27:43,395] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-06-20 09:27:43,397] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-06-20 09:27:46,153] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.4, git-hash=unknown, git-branch=unknown\n",
      "[2023-06-20 09:27:46,153] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-06-20 09:27:46,153] [INFO] [comm.py:594:init_distributed] cdb=None\n",
      "[2023-06-20 09:27:46,153] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2023-06-20 09:27:46,174] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.4, git-hash=unknown, git-branch=unknown\n",
      "[2023-06-20 09:27:46,174] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-06-20 09:27:46,174] [INFO] [comm.py:594:init_distributed] cdb=None\n",
      "[2023-06-20 09:27:46,176] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.4, git-hash=unknown, git-branch=unknown\n",
      "[2023-06-20 09:27:46,176] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-06-20 09:27:46,177] [INFO] [comm.py:594:init_distributed] cdb=None\n",
      "[2023-06-20 09:27:46,183] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.4, git-hash=unknown, git-branch=unknown\n",
      "[2023-06-20 09:27:46,183] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-06-20 09:27:46,183] [INFO] [comm.py:594:init_distributed] cdb=None\n",
      "[2023-06-20 09:27:46,934] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /home/ec2-user/.cache/torch_extensions/py39_cu117/fused_adam...\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/TH -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda-11.8/include -isystem /home/ec2-user/anaconda3/envs/python39_p13/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[2/3] /usr/local/cuda-11.8/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/TH -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda-11.8/include -isystem /home/ec2-user/anaconda3/envs/python39_p13/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -std=c++17 -c /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-11.8/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 23.843460083007812 seconds\n",
      "Loading extension module fused_adam...\n",
      "[2023-06-20 09:28:11,121] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "Time to load fused_adam op: 23.83094310760498 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 23.832178831100464 seconds\n",
      "[2023-06-20 09:28:11,128] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2023-06-20 09:28:11,128] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2023-06-20 09:28:11,128] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2023-06-20 09:28:11,128] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /home/ec2-user/.cache/torch_extensions/py39_cu117/utils...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "[2023-06-20 09:28:11,206] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning\n",
      "[2023-06-20 09:28:11,207] [INFO] [utils.py:786:see_memory_usage] MA 0.21 GB         Max_MA 0.41 GB         CA 0.46 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:11,207] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.35 GB, percent = 5.0%\n",
      "[2023-06-20 09:28:11,208] [INFO] [stage3.py:113:__init__] Reduce bucket size 200000000\n",
      "[2023-06-20 09:28:11,208] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50,000,000\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 23.933227062225342 seconds\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py39_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/TH -isystem /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/include/THC -isystem /home/ec2-user/anaconda3/envs/python39_p13/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
      "[2/2] c++ flatten_unflatten.o -shared -L/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 13.809337139129639 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 13.71888518333435 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 13.818177461624146 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 13.818086624145508 seconds\n",
      "[2023-06-20 09:28:25,099] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-06-20 09:28:25,100] [INFO] [utils.py:786:see_memory_usage] MA 0.21 GB         Max_MA 0.21 GB         CA 0.46 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:25,100] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.35 GB, percent = 5.0%\n",
      "Parameter Offload: Total persistent parameters: 128262 in 126 params\n",
      "[2023-06-20 09:28:25,214] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-06-20 09:28:25,215] [INFO] [utils.py:786:see_memory_usage] MA 0.05 GB         Max_MA 0.21 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:25,215] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.35 GB, percent = 5.0%\n",
      "[2023-06-20 09:28:25,291] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions\n",
      "[2023-06-20 09:28:25,291] [INFO] [utils.py:786:see_memory_usage] MA 0.05 GB         Max_MA 0.05 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:25,291] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.35 GB, percent = 5.0%\n",
      "[2023-06-20 09:28:25,591] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2023-06-20 09:28:25,591] [INFO] [utils.py:786:see_memory_usage] MA 0.05 GB         Max_MA 0.05 GB         CA 0.05 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:25,592] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.64 GB, percent = 5.2%\n",
      "[2023-06-20 09:28:25,667] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions\n",
      "[2023-06-20 09:28:25,667] [INFO] [utils.py:786:see_memory_usage] MA 0.05 GB         Max_MA 0.05 GB         CA 0.05 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:25,668] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.64 GB, percent = 5.2%\n",
      "[2023-06-20 09:28:25,744] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions\n",
      "[2023-06-20 09:28:25,745] [INFO] [utils.py:786:see_memory_usage] MA 0.14 GB         Max_MA 0.19 GB         CA 0.2 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:25,745] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.64 GB, percent = 5.2%\n",
      "[2023-06-20 09:28:25,820] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n",
      "[2023-06-20 09:28:25,821] [INFO] [utils.py:786:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.2 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:25,821] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.64 GB, percent = 5.2%\n",
      "[2023-06-20 09:28:25,824] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 2.36\n",
      "[2023-06-20 09:28:25,899] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n",
      "[2023-06-20 09:28:25,900] [INFO] [utils.py:786:see_memory_usage] MA 0.33 GB         Max_MA 0.43 GB         CA 0.48 GB         Max_CA 0 GB \n",
      "[2023-06-20 09:28:25,900] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.64 GB, percent = 5.2%\n",
      "[2023-06-20 09:28:25,900] [INFO] [stage3.py:388:_setup_for_real_optimizer] optimizer state initialized\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00031638145446777344 secondsNo modifications detected for re-loaded extension module utils, skipping build step...\n",
      "\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002911090850830078 seconds\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00034046173095703125 seconds\n",
      "[2023-06-20 09:28:26,098] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-06-20 09:28:26,099] [INFO] [utils.py:786:see_memory_usage] MA 0.76 GB         Max_MA 0.82 GB         CA 0.86 GB         Max_CA 1 GB \n",
      "[2023-06-20 09:28:26,099] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 9.71 GB, percent = 5.2%\n",
      "[2023-06-20 09:28:26,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
      "[2023-06-20 09:28:26,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-06-20 09:28:26,100] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2023-06-20 09:28:26,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:26,100] [INFO] [config.py:960:print] DeepSpeedEngine configuration:\n",
      "[2023-06-20 09:28:26,100] [INFO] [config.py:964:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": true, \n",
      "    \"contiguous_memory_optimization\": true, \n",
      "    \"cpu_checkpointing\": true, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-06-20 09:28:26,100] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-06-20 09:28:26,100] [INFO] [config.py:964:print]   amp_enabled .................. False\n",
      "[2023-06-20 09:28:26,100] [INFO] [config.py:964:print]   amp_params ................... False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   bfloat16_enabled ............. False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc4daabfa30>\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   communication_data_type ...... None\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   dataloader_drop_last ......... False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   disable_allgather ............ False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   dump_state ................... False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-06-20 09:28:26,101] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   elasticity_enabled ........... False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   fp16_auto_cast ............... False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   fp16_enabled ................. True\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   global_rank .................. 0\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   grad_accum_dtype ............. None\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   load_universal_checkpoint .... False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   loss_scale ................... 0\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   memory_breakdown ............. False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   mics_shard_size .............. -1\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   optimizer_name ............... adamw\n",
      "[2023-06-20 09:28:26,102] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 3e-05}\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   pld_enabled .................. False\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   pld_params ................... False\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   prescale_gradients ........... False\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   scheduler_name ............... None\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   scheduler_params ............. None\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   sparse_attention ............. None\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   steps_per_print .............. 10\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   train_batch_size ............. 128\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  32\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   use_node_local_storage ....... False\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   wall_clock_breakdown ......... True\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   world_size ................... 4\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   zero_enabled ................. True\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3\n",
      "[2023-06-20 09:28:26,103] [INFO] [config.py:950:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05\n",
      "        }\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": true, \n",
      "        \"cpu_checkpointing\": true, \n",
      "        \"contiguous_memory_optimization\": true\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"log_dist\": false\n",
      "}\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026702880859375 seconds\n",
      "[2023-06-20 09:28:26,908] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "[2023-06-20 09:28:26,908] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 496.29 | backward_microstep: 229.00 | backward_inner_microstep: 142.71 | backward_allreduce_microstep: 86.19 | step_microstep: 3.58\n",
      "[2023-06-20 09:28:26,908] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 496.29 | backward: 229.00 | backward_inner: 142.71 | backward_allreduce: 86.20 | step: 3.59\n",
      "【train】 epoch：1/1 step：1/288 loss：1.842773\n",
      "[2023-06-20 09:28:27,602] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2023-06-20 09:28:27,603] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 409.41 | backward_microstep: 269.98 | backward_inner_microstep: 185.55 | backward_allreduce_microstep: 84.29 | step_microstep: 10.22\n",
      "[2023-06-20 09:28:27,603] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 409.42 | backward: 269.96 | backward_inner: 185.58 | backward_allreduce: 84.28 | step: 10.22\n",
      "【train】 epoch：1/1 step：2/288 loss：1.875000\n",
      "[2023-06-20 09:28:27,863] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "[2023-06-20 09:28:27,864] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 84.71 | backward_microstep: 160.11 | backward_inner_microstep: 75.45 | backward_allreduce_microstep: 84.54 | step_microstep: 10.28\n",
      "[2023-06-20 09:28:27,864] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 84.72 | backward: 160.11 | backward_inner: 75.46 | backward_allreduce: 84.55 | step: 10.29\n",
      "【train】 epoch：1/1 step：3/288 loss：1.639648\n",
      "[2023-06-20 09:28:28,124] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "[2023-06-20 09:28:28,125] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 87.46 | backward_microstep: 160.44 | backward_inner_microstep: 76.22 | backward_allreduce_microstep: 84.11 | step_microstep: 10.21\n",
      "[2023-06-20 09:28:28,125] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 87.47 | backward: 160.42 | backward_inner: 76.22 | backward_allreduce: 84.11 | step: 10.22\n",
      "【train】 epoch：1/1 step：4/288 loss：1.786133\n",
      "[2023-06-20 09:28:28,387] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "[2023-06-20 09:28:28,387] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 86.43 | backward_microstep: 161.32 | backward_inner_microstep: 76.76 | backward_allreduce_microstep: 84.41 | step_microstep: 10.12\n",
      "[2023-06-20 09:28:28,387] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 86.43 | backward: 161.31 | backward_inner: 76.78 | backward_allreduce: 84.38 | step: 10.13\n",
      "【train】 epoch：1/1 step：5/288 loss：1.742188\n",
      "[2023-06-20 09:28:28,648] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "[2023-06-20 09:28:28,648] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 85.58 | backward_microstep: 161.20 | backward_inner_microstep: 76.87 | backward_allreduce_microstep: 84.19 | step_microstep: 9.98\n",
      "[2023-06-20 09:28:28,648] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 85.59 | backward: 161.19 | backward_inner: 76.88 | backward_allreduce: 84.19 | step: 9.98\n",
      "【train】 epoch：1/1 step：6/288 loss：1.821289\n",
      "[2023-06-20 09:28:28,912] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "[2023-06-20 09:28:28,912] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 87.41 | backward_microstep: 161.73 | backward_inner_microstep: 77.89 | backward_allreduce_microstep: 83.70 | step_microstep: 10.03\n",
      "[2023-06-20 09:28:28,913] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 87.43 | backward: 161.72 | backward_inner: 77.89 | backward_allreduce: 83.69 | step: 10.04\n",
      "【train】 epoch：1/1 step：7/288 loss：1.786133\n",
      "[2023-06-20 09:28:29,174] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "[2023-06-20 09:28:29,175] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 85.55 | backward_microstep: 161.65 | backward_inner_microstep: 76.24 | backward_allreduce_microstep: 85.28 | step_microstep: 10.17\n",
      "[2023-06-20 09:28:29,175] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 85.54 | backward: 161.65 | backward_inner: 76.25 | backward_allreduce: 85.29 | step: 10.17\n",
      "【train】 epoch：1/1 step：8/288 loss：1.831055\n",
      "[2023-06-20 09:28:29,437] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "[2023-06-20 09:28:29,438] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 87.17 | backward_microstep: 161.58 | backward_inner_microstep: 77.32 | backward_allreduce_microstep: 84.12 | step_microstep: 9.99\n",
      "[2023-06-20 09:28:29,438] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 87.17 | backward: 161.58 | backward_inner: 77.34 | backward_allreduce: 84.11 | step: 10.00\n",
      "【train】 epoch：1/1 step：9/288 loss：1.787109\n",
      "[2023-06-20 09:28:29,699] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "[2023-06-20 09:28:29,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:29,700] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=495.1976754241034, CurrSamplesPerSec=496.40634850187007, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:29,700] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 85.78 | backward_microstep: 160.94 | backward_inner_microstep: 76.72 | backward_allreduce_microstep: 84.09 | step_microstep: 10.58\n",
      "[2023-06-20 09:28:29,700] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 85.79 | backward: 160.94 | backward_inner: 76.73 | backward_allreduce: 84.09 | step: 10.59\n",
      "【train】 epoch：1/1 step：10/288 loss：1.790039\n",
      "[2023-06-20 09:28:29,959] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n",
      "[2023-06-20 09:28:29,960] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 84.67 | backward_microstep: 160.68 | backward_inner_microstep: 77.12 | backward_allreduce_microstep: 83.45 | step_microstep: 9.98\n",
      "[2023-06-20 09:28:29,960] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 84.68 | backward: 160.68 | backward_inner: 77.12 | backward_allreduce: 83.44 | step: 9.99\n",
      "【train】 epoch：1/1 step：11/288 loss：1.774414\n",
      "[2023-06-20 09:28:30,221] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "[2023-06-20 09:28:30,221] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 85.58 | backward_microstep: 160.80 | backward_inner_microstep: 76.81 | backward_allreduce_microstep: 83.86 | step_microstep: 10.19\n",
      "[2023-06-20 09:28:30,221] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 85.59 | backward: 160.80 | backward_inner: 76.82 | backward_allreduce: 83.86 | step: 10.19\n",
      "【train】 epoch：1/1 step：12/288 loss：1.843750\n",
      "[2023-06-20 09:28:30,483] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "[2023-06-20 09:28:30,484] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 86.68 | backward_microstep: 161.80 | backward_inner_microstep: 77.77 | backward_allreduce_microstep: 83.91 | step_microstep: 9.85\n",
      "[2023-06-20 09:28:30,484] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 86.68 | backward: 161.80 | backward_inner: 77.77 | backward_allreduce: 83.92 | step: 9.85\n",
      "【train】 epoch：1/1 step：13/288 loss：1.846680\n",
      "[2023-06-20 09:28:30,742] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "[2023-06-20 09:28:30,743] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 84.46 | backward_microstep: 159.92 | backward_inner_microstep: 76.20 | backward_allreduce_microstep: 83.60 | step_microstep: 10.05\n",
      "[2023-06-20 09:28:30,743] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 84.46 | backward: 159.89 | backward_inner: 76.21 | backward_allreduce: 83.60 | step: 10.05\n",
      "【train】 epoch：1/1 step：14/288 loss：1.793945\n",
      "[2023-06-20 09:28:31,002] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "[2023-06-20 09:28:31,003] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 85.31 | backward_microstep: 160.19 | backward_inner_microstep: 75.77 | backward_allreduce_microstep: 84.29 | step_microstep: 10.15\n",
      "[2023-06-20 09:28:31,003] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 85.31 | backward: 160.19 | backward_inner: 75.78 | backward_allreduce: 84.27 | step: 10.16\n",
      "【train】 epoch：1/1 step：15/288 loss：1.688477\n",
      "[2023-06-20 09:28:31,263] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n",
      "[2023-06-20 09:28:31,263] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 85.56 | backward_microstep: 160.01 | backward_inner_microstep: 75.64 | backward_allreduce_microstep: 84.26 | step_microstep: 10.07\n",
      "[2023-06-20 09:28:31,263] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 85.56 | backward: 160.02 | backward_inner: 75.64 | backward_allreduce: 84.26 | step: 10.07\n",
      "【train】 epoch：1/1 step：16/288 loss：1.813477\n",
      "[2023-06-20 09:28:31,523] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "[2023-06-20 09:28:31,524] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 85.40 | backward_microstep: 160.30 | backward_inner_microstep: 75.00 | backward_allreduce_microstep: 85.14 | step_microstep: 9.98\n",
      "[2023-06-20 09:28:31,524] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 85.41 | backward: 160.28 | backward_inner: 75.02 | backward_allreduce: 85.14 | step: 10.00\n",
      "【train】 epoch：1/1 step：17/288 loss：1.648438\n",
      "[2023-06-20 09:28:31,786] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2023-06-20 09:28:31,786] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 87.24 | backward_microstep: 161.08 | backward_inner_microstep: 76.31 | backward_allreduce_microstep: 84.62 | step_microstep: 10.05\n",
      "[2023-06-20 09:28:31,787] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 87.24 | backward: 161.08 | backward_inner: 76.33 | backward_allreduce: 84.61 | step: 10.05\n",
      "【train】 epoch：1/1 step：18/288 loss：1.790039\n",
      "[2023-06-20 09:28:32,064] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 5.67\n",
      "[2023-06-20 09:28:32,064] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 86.06 | backward_microstep: 159.57 | backward_inner_microstep: 75.56 | backward_allreduce_microstep: 83.89 | step_microstep: 27.59\n",
      "[2023-06-20 09:28:32,065] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 86.06 | backward: 159.55 | backward_inner: 75.56 | backward_allreduce: 83.88 | step: 27.60\n",
      "【train】 epoch：1/1 step：19/288 loss：1.764648\n",
      "[2023-06-20 09:28:32,327] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.74\n",
      "[2023-06-20 09:28:32,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:32,328] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=494.4622070658478, CurrSamplesPerSec=493.1828062878014, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:32,328] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.70 | backward_microstep: 160.40 | backward_inner_microstep: 76.07 | backward_allreduce_microstep: 84.21 | step_microstep: 26.72\n",
      "[2023-06-20 09:28:32,328] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.70 | backward: 160.40 | backward_inner: 76.10 | backward_allreduce: 84.18 | step: 26.73\n",
      "【train】 epoch：1/1 step：20/288 loss：1.724609\n",
      "[2023-06-20 09:28:32,588] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:28:32,589] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.13 | backward_microstep: 160.03 | backward_inner_microstep: 75.93 | backward_allreduce_microstep: 84.00 | step_microstep: 25.92\n",
      "[2023-06-20 09:28:32,589] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.13 | backward: 160.03 | backward_inner: 75.93 | backward_allreduce: 84.01 | step: 25.93\n",
      "【train】 epoch：1/1 step：21/288 loss：1.728516\n",
      "[2023-06-20 09:28:32,849] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:32,850] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.47 | backward_microstep: 160.11 | backward_inner_microstep: 75.84 | backward_allreduce_microstep: 84.18 | step_microstep: 26.13\n",
      "[2023-06-20 09:28:32,850] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.47 | backward: 160.11 | backward_inner: 75.84 | backward_allreduce: 84.17 | step: 26.13\n",
      "【train】 epoch：1/1 step：22/288 loss：1.549805\n",
      "[2023-06-20 09:28:33,110] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.71\n",
      "[2023-06-20 09:28:33,111] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.99 | backward_microstep: 159.29 | backward_inner_microstep: 75.22 | backward_allreduce_microstep: 83.95 | step_microstep: 26.23\n",
      "[2023-06-20 09:28:33,111] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.99 | backward: 159.27 | backward_inner: 75.23 | backward_allreduce: 83.95 | step: 26.24\n",
      "【train】 epoch：1/1 step：23/288 loss：1.688477\n",
      "[2023-06-20 09:28:33,372] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:28:33,373] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.87 | backward_microstep: 160.26 | backward_inner_microstep: 75.70 | backward_allreduce_microstep: 84.44 | step_microstep: 26.10\n",
      "[2023-06-20 09:28:33,373] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.87 | backward: 160.24 | backward_inner: 75.71 | backward_allreduce: 84.44 | step: 26.10\n",
      "【train】 epoch：1/1 step：24/288 loss：1.598633\n",
      "[2023-06-20 09:28:33,633] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.71\n",
      "[2023-06-20 09:28:33,633] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.90 | backward_microstep: 158.74 | backward_inner_microstep: 75.40 | backward_allreduce_microstep: 83.25 | step_microstep: 25.72\n",
      "[2023-06-20 09:28:33,633] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.90 | backward: 158.74 | backward_inner: 75.40 | backward_allreduce: 83.26 | step: 25.74\n",
      "【train】 epoch：1/1 step：25/288 loss：1.553711\n",
      "[2023-06-20 09:28:33,893] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:28:33,894] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.72 | backward_microstep: 159.96 | backward_inner_microstep: 75.27 | backward_allreduce_microstep: 84.60 | step_microstep: 25.81\n",
      "[2023-06-20 09:28:33,894] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.72 | backward: 159.96 | backward_inner: 75.28 | backward_allreduce: 84.60 | step: 25.82\n",
      "【train】 epoch：1/1 step：26/288 loss：1.730469\n",
      "[2023-06-20 09:28:34,156] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:34,156] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.40 | backward_microstep: 161.02 | backward_inner_microstep: 75.42 | backward_allreduce_microstep: 85.50 | step_microstep: 25.94\n",
      "[2023-06-20 09:28:34,157] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.40 | backward: 161.02 | backward_inner: 75.43 | backward_allreduce: 85.49 | step: 25.95\n",
      "【train】 epoch：1/1 step：27/288 loss：1.664062\n",
      "[2023-06-20 09:28:34,416] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:34,417] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.62 | backward_microstep: 159.43 | backward_inner_microstep: 75.46 | backward_allreduce_microstep: 83.87 | step_microstep: 25.98\n",
      "[2023-06-20 09:28:34,417] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.62 | backward: 159.43 | backward_inner: 75.47 | backward_allreduce: 83.87 | step: 25.98\n",
      "【train】 epoch：1/1 step：28/288 loss：1.478516\n",
      "[2023-06-20 09:28:34,678] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:34,679] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.65 | backward_microstep: 160.84 | backward_inner_microstep: 75.14 | backward_allreduce_microstep: 85.60 | step_microstep: 25.87\n",
      "[2023-06-20 09:28:34,679] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.65 | backward: 160.84 | backward_inner: 75.15 | backward_allreduce: 85.60 | step: 25.87\n",
      "【train】 epoch：1/1 step：29/288 loss：1.609375\n",
      "[2023-06-20 09:28:34,939] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:28:34,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:34,940] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=495.20155790789295, CurrSamplesPerSec=496.92279184110197, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:34,940] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.60 | backward_microstep: 160.05 | backward_inner_microstep: 75.09 | backward_allreduce_microstep: 84.85 | step_microstep: 26.52\n",
      "[2023-06-20 09:28:34,940] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.60 | backward: 160.05 | backward_inner: 75.10 | backward_allreduce: 84.84 | step: 26.52\n",
      "【train】 epoch：1/1 step：30/288 loss：1.693359\n",
      "[2023-06-20 09:28:35,201] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:28:35,201] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.69 | backward_microstep: 159.53 | backward_inner_microstep: 75.60 | backward_allreduce_microstep: 83.82 | step_microstep: 26.33\n",
      "[2023-06-20 09:28:35,201] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.69 | backward: 159.52 | backward_inner: 75.60 | backward_allreduce: 83.83 | step: 26.34\n",
      "【train】 epoch：1/1 step：31/288 loss：1.540039\n",
      "[2023-06-20 09:28:35,559] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:35,560] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.05 | backward_microstep: 257.32 | backward_inner_microstep: 76.12 | backward_allreduce_microstep: 181.06 | step_microstep: 26.12\n",
      "[2023-06-20 09:28:35,560] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.05 | backward: 257.32 | backward_inner: 76.15 | backward_allreduce: 181.05 | step: 26.12\n",
      "【train】 epoch：1/1 step：32/288 loss：1.581055\n",
      "[2023-06-20 09:28:35,821] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:28:35,822] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.23 | backward_microstep: 160.44 | backward_inner_microstep: 75.87 | backward_allreduce_microstep: 84.46 | step_microstep: 25.92\n",
      "[2023-06-20 09:28:35,822] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.23 | backward: 160.42 | backward_inner: 75.87 | backward_allreduce: 84.46 | step: 25.92\n",
      "【train】 epoch：1/1 step：33/288 loss：1.644531\n",
      "[2023-06-20 09:28:36,080] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:36,081] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.65 | backward_microstep: 158.78 | backward_inner_microstep: 75.48 | backward_allreduce_microstep: 83.20 | step_microstep: 25.77\n",
      "[2023-06-20 09:28:36,081] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.65 | backward: 158.78 | backward_inner: 75.49 | backward_allreduce: 83.21 | step: 25.78\n",
      "【train】 epoch：1/1 step：34/288 loss：1.484375\n",
      "[2023-06-20 09:28:36,343] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:28:36,343] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.49 | backward_microstep: 160.73 | backward_inner_microstep: 75.65 | backward_allreduce_microstep: 84.98 | step_microstep: 25.98\n",
      "[2023-06-20 09:28:36,343] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.49 | backward: 160.73 | backward_inner: 75.66 | backward_allreduce: 84.97 | step: 25.99\n",
      "【train】 epoch：1/1 step：35/288 loss：1.351562\n",
      "[2023-06-20 09:28:36,602] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.77\n",
      "[2023-06-20 09:28:36,603] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.43 | backward_microstep: 158.83 | backward_inner_microstep: 75.12 | backward_allreduce_microstep: 83.58 | step_microstep: 25.84\n",
      "[2023-06-20 09:28:36,603] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.43 | backward: 158.82 | backward_inner: 75.15 | backward_allreduce: 83.49 | step: 25.85\n",
      "【train】 epoch：1/1 step：36/288 loss：1.443359\n",
      "[2023-06-20 09:28:36,862] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:36,863] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.07 | backward_microstep: 159.69 | backward_inner_microstep: 75.38 | backward_allreduce_microstep: 84.21 | step_microstep: 25.35\n",
      "[2023-06-20 09:28:36,863] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.07 | backward: 159.69 | backward_inner: 75.39 | backward_allreduce: 84.21 | step: 25.36\n",
      "【train】 epoch：1/1 step：37/288 loss：1.567383\n",
      "[2023-06-20 09:28:37,123] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:28:37,123] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.61 | backward_microstep: 160.96 | backward_inner_microstep: 75.27 | backward_allreduce_microstep: 85.57 | step_microstep: 25.77\n",
      "[2023-06-20 09:28:37,123] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.62 | backward: 160.96 | backward_inner: 75.29 | backward_allreduce: 85.54 | step: 25.77\n",
      "【train】 epoch：1/1 step：38/288 loss：1.557617\n",
      "[2023-06-20 09:28:37,384] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:37,384] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.23 | backward_microstep: 160.06 | backward_inner_microstep: 75.36 | backward_allreduce_microstep: 84.60 | step_microstep: 25.88\n",
      "[2023-06-20 09:28:37,385] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.23 | backward: 160.06 | backward_inner: 75.37 | backward_allreduce: 84.60 | step: 25.88\n",
      "【train】 epoch：1/1 step：39/288 loss：1.334961\n",
      "[2023-06-20 09:28:37,644] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.70\n",
      "[2023-06-20 09:28:37,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:37,645] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=490.93000510275107, CurrSamplesPerSec=497.76361576389394, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:37,645] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.36 | backward_microstep: 160.42 | backward_inner_microstep: 75.73 | backward_allreduce_microstep: 84.59 | step_microstep: 25.98\n",
      "[2023-06-20 09:28:37,645] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.36 | backward: 160.42 | backward_inner: 75.74 | backward_allreduce: 84.60 | step: 25.98\n",
      "【train】 epoch：1/1 step：40/288 loss：1.562500\n",
      "[2023-06-20 09:28:37,905] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.70\n",
      "[2023-06-20 09:28:37,905] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.53 | backward_microstep: 159.48 | backward_inner_microstep: 75.60 | backward_allreduce_microstep: 83.71 | step_microstep: 25.85\n",
      "[2023-06-20 09:28:37,905] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.54 | backward: 159.47 | backward_inner: 75.66 | backward_allreduce: 83.69 | step: 25.86\n",
      "【train】 epoch：1/1 step：41/288 loss：1.531250\n",
      "[2023-06-20 09:28:38,166] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:38,167] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.66 | backward_microstep: 160.14 | backward_inner_microstep: 76.25 | backward_allreduce_microstep: 83.80 | step_microstep: 25.78\n",
      "[2023-06-20 09:28:38,167] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.66 | backward: 160.14 | backward_inner: 76.25 | backward_allreduce: 83.80 | step: 25.79\n",
      "【train】 epoch：1/1 step：42/288 loss：1.434570\n",
      "[2023-06-20 09:28:38,426] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:38,427] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.72 | backward_microstep: 159.49 | backward_inner_microstep: 75.70 | backward_allreduce_microstep: 83.67 | step_microstep: 25.54\n",
      "[2023-06-20 09:28:38,427] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.73 | backward: 159.48 | backward_inner: 75.71 | backward_allreduce: 83.67 | step: 25.54\n",
      "【train】 epoch：1/1 step：43/288 loss：1.325195\n",
      "[2023-06-20 09:28:38,687] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:38,688] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.66 | backward_microstep: 159.60 | backward_inner_microstep: 75.82 | backward_allreduce_microstep: 83.69 | step_microstep: 25.61\n",
      "[2023-06-20 09:28:38,688] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.66 | backward: 159.60 | backward_inner: 75.83 | backward_allreduce: 83.68 | step: 25.62\n",
      "【train】 epoch：1/1 step：44/288 loss：1.370117\n",
      "[2023-06-20 09:28:38,948] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:28:38,949] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.97 | backward_microstep: 160.31 | backward_inner_microstep: 76.16 | backward_allreduce_microstep: 84.05 | step_microstep: 25.80\n",
      "[2023-06-20 09:28:38,949] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.97 | backward: 160.30 | backward_inner: 76.17 | backward_allreduce: 84.05 | step: 25.80\n",
      "【train】 epoch：1/1 step：45/288 loss：1.350586\n",
      "[2023-06-20 09:28:39,209] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.70\n",
      "[2023-06-20 09:28:39,209] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.22 | backward_microstep: 159.47 | backward_inner_microstep: 75.70 | backward_allreduce_microstep: 83.68 | step_microstep: 25.80\n",
      "[2023-06-20 09:28:39,210] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.22 | backward: 159.47 | backward_inner: 75.70 | backward_allreduce: 83.68 | step: 25.80\n",
      "【train】 epoch：1/1 step：46/288 loss：1.514648\n",
      "[2023-06-20 09:28:39,468] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.70\n",
      "[2023-06-20 09:28:39,469] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.34 | backward_microstep: 159.64 | backward_inner_microstep: 75.87 | backward_allreduce_microstep: 83.66 | step_microstep: 25.58\n",
      "[2023-06-20 09:28:39,469] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.34 | backward: 159.63 | backward_inner: 75.88 | backward_allreduce: 83.67 | step: 25.58\n",
      "【train】 epoch：1/1 step：47/288 loss：1.595703\n",
      "[2023-06-20 09:28:39,729] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:28:39,730] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.34 | backward_microstep: 160.48 | backward_inner_microstep: 75.58 | backward_allreduce_microstep: 84.80 | step_microstep: 25.80\n",
      "[2023-06-20 09:28:39,730] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.34 | backward: 160.48 | backward_inner: 75.58 | backward_allreduce: 84.81 | step: 25.80\n",
      "【train】 epoch：1/1 step：48/288 loss：1.383789\n",
      "[2023-06-20 09:28:39,990] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.77\n",
      "[2023-06-20 09:28:39,991] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 72.19 | backward_microstep: 158.74 | backward_inner_microstep: 74.48 | backward_allreduce_microstep: 84.16 | step_microstep: 26.04\n",
      "[2023-06-20 09:28:39,991] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 72.19 | backward: 158.73 | backward_inner: 74.48 | backward_allreduce: 84.16 | step: 26.04\n",
      "【train】 epoch：1/1 step：49/288 loss：1.590820\n",
      "[2023-06-20 09:28:40,250] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:40,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:40,251] [INFO] [timer.py:215:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=492.3182160559814, CurrSamplesPerSec=498.41518962898596, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:40,251] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.84 | backward_microstep: 159.26 | backward_inner_microstep: 75.17 | backward_allreduce_microstep: 83.99 | step_microstep: 26.11\n",
      "[2023-06-20 09:28:40,251] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.84 | backward: 159.26 | backward_inner: 75.18 | backward_allreduce: 83.98 | step: 26.12\n",
      "【train】 epoch：1/1 step：50/288 loss：1.299805\n",
      "[2023-06-20 09:28:40,510] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:28:40,511] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.18 | backward_microstep: 159.67 | backward_inner_microstep: 75.27 | backward_allreduce_microstep: 84.30 | step_microstep: 25.72\n",
      "[2023-06-20 09:28:40,511] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.18 | backward: 159.67 | backward_inner: 75.28 | backward_allreduce: 84.30 | step: 25.72\n",
      "【train】 epoch：1/1 step：51/288 loss：1.327148\n",
      "[2023-06-20 09:28:40,770] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:40,771] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.11 | backward_microstep: 159.76 | backward_inner_microstep: 75.61 | backward_allreduce_microstep: 84.04 | step_microstep: 25.71\n",
      "[2023-06-20 09:28:40,771] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.12 | backward: 159.75 | backward_inner: 75.62 | backward_allreduce: 84.04 | step: 25.71\n",
      "【train】 epoch：1/1 step：52/288 loss：1.285156\n",
      "[2023-06-20 09:28:41,031] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:41,032] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.39 | backward_microstep: 160.19 | backward_inner_microstep: 75.27 | backward_allreduce_microstep: 84.83 | step_microstep: 25.90\n",
      "[2023-06-20 09:28:41,032] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.40 | backward: 160.19 | backward_inner: 75.28 | backward_allreduce: 84.83 | step: 25.91\n",
      "【train】 epoch：1/1 step：53/288 loss：1.286133\n",
      "[2023-06-20 09:28:41,292] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:41,293] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.49 | backward_microstep: 160.68 | backward_inner_microstep: 75.63 | backward_allreduce_microstep: 84.91 | step_microstep: 25.89\n",
      "[2023-06-20 09:28:41,293] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.50 | backward: 160.66 | backward_inner: 75.65 | backward_allreduce: 84.91 | step: 25.90\n",
      "【train】 epoch：1/1 step：54/288 loss：1.486328\n",
      "[2023-06-20 09:28:41,554] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:28:41,554] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.76 | backward_microstep: 160.70 | backward_inner_microstep: 75.57 | backward_allreduce_microstep: 85.03 | step_microstep: 25.81\n",
      "[2023-06-20 09:28:41,554] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.76 | backward: 160.70 | backward_inner: 75.58 | backward_allreduce: 85.04 | step: 25.81\n",
      "【train】 epoch：1/1 step：55/288 loss：1.460938\n",
      "[2023-06-20 09:28:41,814] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:28:41,815] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.26 | backward_microstep: 159.52 | backward_inner_microstep: 75.30 | backward_allreduce_microstep: 84.11 | step_microstep: 25.63\n",
      "[2023-06-20 09:28:41,815] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.26 | backward: 159.52 | backward_inner: 75.31 | backward_allreduce: 84.11 | step: 25.63\n",
      "【train】 epoch：1/1 step：56/288 loss：1.740234\n",
      "[2023-06-20 09:28:42,075] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:42,075] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.99 | backward_microstep: 160.38 | backward_inner_microstep: 75.56 | backward_allreduce_microstep: 84.71 | step_microstep: 25.46\n",
      "[2023-06-20 09:28:42,075] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.00 | backward: 160.38 | backward_inner: 75.57 | backward_allreduce: 84.71 | step: 25.47\n",
      "【train】 epoch：1/1 step：57/288 loss：1.516602\n",
      "[2023-06-20 09:28:42,336] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:42,336] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.26 | backward_microstep: 160.63 | backward_inner_microstep: 75.69 | backward_allreduce_microstep: 84.84 | step_microstep: 25.95\n",
      "[2023-06-20 09:28:42,336] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.26 | backward: 160.62 | backward_inner: 75.70 | backward_allreduce: 84.83 | step: 25.95\n",
      "【train】 epoch：1/1 step：58/288 loss：1.366211\n",
      "[2023-06-20 09:28:42,597] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:28:42,597] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.42 | backward_microstep: 159.37 | backward_inner_microstep: 75.32 | backward_allreduce_microstep: 83.95 | step_microstep: 26.07\n",
      "[2023-06-20 09:28:42,597] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.42 | backward: 159.37 | backward_inner: 75.33 | backward_allreduce: 83.96 | step: 26.07\n",
      "【train】 epoch：1/1 step：59/288 loss：1.173828\n",
      "[2023-06-20 09:28:42,858] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.71\n",
      "[2023-06-20 09:28:42,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:42,858] [INFO] [timer.py:215:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=493.2169826740312, CurrSamplesPerSec=496.6258435187695, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:42,859] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.00 | backward_microstep: 160.79 | backward_inner_microstep: 75.79 | backward_allreduce_microstep: 84.90 | step_microstep: 26.52\n",
      "[2023-06-20 09:28:42,859] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.00 | backward: 160.79 | backward_inner: 75.80 | backward_allreduce: 84.90 | step: 26.52\n",
      "【train】 epoch：1/1 step：60/288 loss：1.250000\n",
      "[2023-06-20 09:28:43,117] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:43,118] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.20 | backward_microstep: 159.42 | backward_inner_microstep: 75.33 | backward_allreduce_microstep: 83.99 | step_microstep: 25.56\n",
      "[2023-06-20 09:28:43,118] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.20 | backward: 159.42 | backward_inner: 75.34 | backward_allreduce: 84.00 | step: 25.56\n",
      "【train】 epoch：1/1 step：61/288 loss：1.145508\n",
      "[2023-06-20 09:28:43,377] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:28:43,377] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.68 | backward_microstep: 159.14 | backward_inner_microstep: 75.68 | backward_allreduce_microstep: 83.37 | step_microstep: 25.75\n",
      "[2023-06-20 09:28:43,378] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.69 | backward: 159.14 | backward_inner: 75.69 | backward_allreduce: 83.37 | step: 25.75\n",
      "【train】 epoch：1/1 step：62/288 loss：1.240234\n",
      "[2023-06-20 09:28:43,639] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.70\n",
      "[2023-06-20 09:28:43,639] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.82 | backward_microstep: 160.75 | backward_inner_microstep: 75.68 | backward_allreduce_microstep: 84.98 | step_microstep: 26.12\n",
      "[2023-06-20 09:28:43,639] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.83 | backward: 160.75 | backward_inner: 75.69 | backward_allreduce: 84.98 | step: 26.12\n",
      "【train】 epoch：1/1 step：63/288 loss：1.410156\n",
      "[2023-06-20 09:28:43,899] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:43,900] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.54 | backward_microstep: 160.66 | backward_inner_microstep: 75.53 | backward_allreduce_microstep: 85.03 | step_microstep: 25.78\n",
      "[2023-06-20 09:28:43,900] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.54 | backward: 160.65 | backward_inner: 75.53 | backward_allreduce: 85.03 | step: 25.78\n",
      "【train】 epoch：1/1 step：64/288 loss：1.259766\n",
      "[2023-06-20 09:28:44,161] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:28:44,161] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.73 | backward_microstep: 160.01 | backward_inner_microstep: 75.80 | backward_allreduce_microstep: 84.09 | step_microstep: 25.78\n",
      "[2023-06-20 09:28:44,162] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.73 | backward: 160.01 | backward_inner: 75.81 | backward_allreduce: 84.09 | step: 25.78\n",
      "【train】 epoch：1/1 step：65/288 loss：1.492188\n",
      "[2023-06-20 09:28:44,421] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:28:44,422] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.57 | backward_microstep: 160.21 | backward_inner_microstep: 76.17 | backward_allreduce_microstep: 83.92 | step_microstep: 25.81\n",
      "[2023-06-20 09:28:44,422] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.57 | backward: 160.21 | backward_inner: 76.19 | backward_allreduce: 83.91 | step: 25.82\n",
      "【train】 epoch：1/1 step：66/288 loss：1.500977\n",
      "[2023-06-20 09:28:44,681] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:28:44,681] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.49 | backward_microstep: 159.97 | backward_inner_microstep: 75.31 | backward_allreduce_microstep: 84.56 | step_microstep: 25.90\n",
      "[2023-06-20 09:28:44,682] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.47 | backward: 159.97 | backward_inner: 75.32 | backward_allreduce: 84.57 | step: 25.91\n",
      "【train】 epoch：1/1 step：67/288 loss：1.490234\n",
      "[2023-06-20 09:28:45,020] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.74\n",
      "[2023-06-20 09:28:45,021] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.98 | backward_microstep: 239.75 | backward_inner_microstep: 76.03 | backward_allreduce_microstep: 163.61 | step_microstep: 25.88\n",
      "[2023-06-20 09:28:45,021] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.98 | backward: 239.74 | backward_inner: 76.04 | backward_allreduce: 163.61 | step: 25.88\n",
      "【train】 epoch：1/1 step：68/288 loss：1.330078\n",
      "[2023-06-20 09:28:45,280] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:28:45,280] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.40 | backward_microstep: 159.43 | backward_inner_microstep: 75.19 | backward_allreduce_microstep: 84.14 | step_microstep: 25.39\n",
      "[2023-06-20 09:28:45,280] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.41 | backward: 159.43 | backward_inner: 75.20 | backward_allreduce: 84.14 | step: 25.39\n",
      "【train】 epoch：1/1 step：69/288 loss：1.468750\n",
      "[2023-06-20 09:28:45,539] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:45,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:45,540] [INFO] [timer.py:215:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=491.76742816763175, CurrSamplesPerSec=499.63603604945075, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:45,540] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.57 | backward_microstep: 159.05 | backward_inner_microstep: 75.60 | backward_allreduce_microstep: 83.34 | step_microstep: 26.15\n",
      "[2023-06-20 09:28:45,540] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.57 | backward: 159.05 | backward_inner: 75.61 | backward_allreduce: 83.35 | step: 26.16\n",
      "【train】 epoch：1/1 step：70/288 loss：1.177734\n",
      "[2023-06-20 09:28:45,800] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:28:45,800] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.31 | backward_microstep: 158.77 | backward_inner_microstep: 74.46 | backward_allreduce_microstep: 84.22 | step_microstep: 25.67\n",
      "[2023-06-20 09:28:45,800] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.31 | backward: 158.77 | backward_inner: 74.46 | backward_allreduce: 84.22 | step: 25.67\n",
      "【train】 epoch：1/1 step：71/288 loss：1.374023\n",
      "[2023-06-20 09:28:46,060] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.75\n",
      "[2023-06-20 09:28:46,061] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.50 | backward_microstep: 160.25 | backward_inner_microstep: 74.75 | backward_allreduce_microstep: 85.40 | step_microstep: 25.77\n",
      "[2023-06-20 09:28:46,061] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.50 | backward: 160.25 | backward_inner: 74.76 | backward_allreduce: 85.40 | step: 25.77\n",
      "【train】 epoch：1/1 step：72/288 loss：1.433594\n",
      "[2023-06-20 09:28:46,321] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:46,321] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.48 | backward_microstep: 160.08 | backward_inner_microstep: 74.62 | backward_allreduce_microstep: 85.37 | step_microstep: 25.57\n",
      "[2023-06-20 09:28:46,321] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.49 | backward: 160.08 | backward_inner: 74.62 | backward_allreduce: 85.38 | step: 25.58\n",
      "【train】 epoch：1/1 step：73/288 loss：1.310547\n",
      "[2023-06-20 09:28:46,579] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:28:46,579] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.67 | backward_microstep: 159.09 | backward_inner_microstep: 74.76 | backward_allreduce_microstep: 84.24 | step_microstep: 25.51\n",
      "[2023-06-20 09:28:46,580] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.68 | backward: 159.09 | backward_inner: 74.77 | backward_allreduce: 84.24 | step: 25.51\n",
      "【train】 epoch：1/1 step：74/288 loss：1.484375\n",
      "[2023-06-20 09:28:46,840] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.56\n",
      "[2023-06-20 09:28:46,840] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.89 | backward_microstep: 160.12 | backward_inner_microstep: 74.35 | backward_allreduce_microstep: 85.67 | step_microstep: 25.34\n",
      "[2023-06-20 09:28:46,840] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.89 | backward: 160.11 | backward_inner: 74.36 | backward_allreduce: 85.67 | step: 25.34\n",
      "【train】 epoch：1/1 step：75/288 loss：1.476562\n",
      "[2023-06-20 09:28:47,098] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:28:47,099] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.11 | backward_microstep: 159.58 | backward_inner_microstep: 74.54 | backward_allreduce_microstep: 84.93 | step_microstep: 25.70\n",
      "[2023-06-20 09:28:47,099] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.11 | backward: 159.58 | backward_inner: 74.55 | backward_allreduce: 84.93 | step: 25.70\n",
      "【train】 epoch：1/1 step：76/288 loss：1.083008\n",
      "[2023-06-20 09:28:47,356] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:47,357] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.62 | backward_microstep: 157.99 | backward_inner_microstep: 74.81 | backward_allreduce_microstep: 83.09 | step_microstep: 25.27\n",
      "[2023-06-20 09:28:47,357] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.62 | backward: 157.99 | backward_inner: 74.81 | backward_allreduce: 83.09 | step: 25.27\n",
      "【train】 epoch：1/1 step：77/288 loss：1.148438\n",
      "[2023-06-20 09:28:47,614] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:28:47,615] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.55 | backward_microstep: 158.14 | backward_inner_microstep: 74.59 | backward_allreduce_microstep: 83.43 | step_microstep: 25.15\n",
      "[2023-06-20 09:28:47,615] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.55 | backward: 158.11 | backward_inner: 74.60 | backward_allreduce: 83.43 | step: 25.15\n",
      "【train】 epoch：1/1 step：78/288 loss：1.152344\n",
      "[2023-06-20 09:28:47,873] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.77\n",
      "[2023-06-20 09:28:47,873] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.48 | backward_microstep: 157.55 | backward_inner_microstep: 74.21 | backward_allreduce_microstep: 83.24 | step_microstep: 25.60\n",
      "[2023-06-20 09:28:47,874] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.48 | backward: 157.54 | backward_inner: 74.22 | backward_allreduce: 83.24 | step: 25.60\n",
      "【train】 epoch：1/1 step：79/288 loss：1.146484\n",
      "[2023-06-20 09:28:48,132] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:28:48,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:48,133] [INFO] [timer.py:215:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=492.8492910736851, CurrSamplesPerSec=499.8556047565667, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:48,133] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.95 | backward_microstep: 160.07 | backward_inner_microstep: 75.27 | backward_allreduce_microstep: 84.70 | step_microstep: 25.73\n",
      "[2023-06-20 09:28:48,133] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.95 | backward: 160.07 | backward_inner: 75.28 | backward_allreduce: 84.71 | step: 25.74\n",
      "【train】 epoch：1/1 step：80/288 loss：1.123047\n",
      "[2023-06-20 09:28:48,392] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:28:48,393] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.76 | backward_microstep: 160.18 | backward_inner_microstep: 74.78 | backward_allreduce_microstep: 85.30 | step_microstep: 25.59\n",
      "[2023-06-20 09:28:48,393] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.76 | backward: 160.18 | backward_inner: 74.79 | backward_allreduce: 85.31 | step: 25.59\n",
      "【train】 epoch：1/1 step：81/288 loss：1.287109\n",
      "[2023-06-20 09:28:48,652] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:28:48,652] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.00 | backward_microstep: 159.68 | backward_inner_microstep: 74.92 | backward_allreduce_microstep: 84.64 | step_microstep: 25.63\n",
      "[2023-06-20 09:28:48,652] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.00 | backward: 159.68 | backward_inner: 74.93 | backward_allreduce: 84.64 | step: 25.63\n",
      "【train】 epoch：1/1 step：82/288 loss：1.233398\n",
      "[2023-06-20 09:28:48,914] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:48,914] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.24 | backward_microstep: 160.78 | backward_inner_microstep: 74.48 | backward_allreduce_microstep: 86.20 | step_microstep: 25.94\n",
      "[2023-06-20 09:28:48,914] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.24 | backward: 160.77 | backward_inner: 74.49 | backward_allreduce: 86.20 | step: 25.94\n",
      "【train】 epoch：1/1 step：83/288 loss：1.212891\n",
      "[2023-06-20 09:28:49,173] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:49,174] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.92 | backward_microstep: 159.85 | backward_inner_microstep: 75.59 | backward_allreduce_microstep: 84.16 | step_microstep: 25.72\n",
      "[2023-06-20 09:28:49,174] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.92 | backward: 159.85 | backward_inner: 75.60 | backward_allreduce: 84.17 | step: 25.72\n",
      "【train】 epoch：1/1 step：84/288 loss：1.126953\n",
      "[2023-06-20 09:28:49,434] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:49,434] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.69 | backward_microstep: 159.40 | backward_inner_microstep: 75.19 | backward_allreduce_microstep: 84.12 | step_microstep: 25.48\n",
      "[2023-06-20 09:28:49,435] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.70 | backward: 159.40 | backward_inner: 75.19 | backward_allreduce: 84.13 | step: 25.49\n",
      "【train】 epoch：1/1 step：85/288 loss：1.248047\n",
      "[2023-06-20 09:28:49,693] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:49,693] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.90 | backward_microstep: 158.96 | backward_inner_microstep: 75.27 | backward_allreduce_microstep: 83.60 | step_microstep: 25.71\n",
      "[2023-06-20 09:28:49,693] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.91 | backward: 158.96 | backward_inner: 75.28 | backward_allreduce: 83.60 | step: 25.72\n",
      "【train】 epoch：1/1 step：86/288 loss：1.668945\n",
      "[2023-06-20 09:28:49,954] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:28:49,954] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.46 | backward_microstep: 160.74 | backward_inner_microstep: 75.51 | backward_allreduce_microstep: 85.13 | step_microstep: 26.03\n",
      "[2023-06-20 09:28:49,954] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.46 | backward: 160.74 | backward_inner: 75.52 | backward_allreduce: 85.14 | step: 26.04\n",
      "【train】 epoch：1/1 step：87/288 loss：1.113281\n",
      "[2023-06-20 09:28:50,213] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:50,214] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.02 | backward_microstep: 159.78 | backward_inner_microstep: 75.88 | backward_allreduce_microstep: 83.81 | step_microstep: 25.69\n",
      "[2023-06-20 09:28:50,214] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.02 | backward: 159.78 | backward_inner: 75.89 | backward_allreduce: 83.80 | step: 25.69\n",
      "【train】 epoch：1/1 step：88/288 loss：1.471680\n",
      "[2023-06-20 09:28:50,473] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:50,474] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.25 | backward_microstep: 159.03 | backward_inner_microstep: 74.96 | backward_allreduce_microstep: 83.97 | step_microstep: 25.75\n",
      "[2023-06-20 09:28:50,474] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.25 | backward: 159.03 | backward_inner: 74.97 | backward_allreduce: 83.97 | step: 25.76\n",
      "【train】 epoch：1/1 step：89/288 loss：0.880371\n",
      "[2023-06-20 09:28:50,735] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:50,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:50,736] [INFO] [timer.py:215:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=493.4516782733227, CurrSamplesPerSec=494.838352649266, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:50,736] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.33 | backward_microstep: 160.58 | backward_inner_microstep: 75.42 | backward_allreduce_microstep: 85.05 | step_microstep: 26.33\n",
      "[2023-06-20 09:28:50,736] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.33 | backward: 160.57 | backward_inner: 75.41 | backward_allreduce: 85.05 | step: 26.34\n",
      "【train】 epoch：1/1 step：90/288 loss：1.288086\n",
      "[2023-06-20 09:28:50,995] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:50,995] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.10 | backward_microstep: 159.23 | backward_inner_microstep: 75.18 | backward_allreduce_microstep: 83.96 | step_microstep: 25.41\n",
      "[2023-06-20 09:28:50,995] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.10 | backward: 159.23 | backward_inner: 75.19 | backward_allreduce: 83.96 | step: 25.41\n",
      "【train】 epoch：1/1 step：91/288 loss：1.045898\n",
      "[2023-06-20 09:28:51,254] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.73\n",
      "[2023-06-20 09:28:51,255] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.81 | backward_microstep: 158.52 | backward_inner_microstep: 75.40 | backward_allreduce_microstep: 83.01 | step_microstep: 25.82\n",
      "[2023-06-20 09:28:51,255] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.81 | backward: 158.52 | backward_inner: 75.41 | backward_allreduce: 83.00 | step: 25.83\n",
      "【train】 epoch：1/1 step：92/288 loss：1.317383\n",
      "[2023-06-20 09:28:51,514] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:28:51,515] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.92 | backward_microstep: 159.57 | backward_inner_microstep: 75.60 | backward_allreduce_microstep: 83.86 | step_microstep: 25.57\n",
      "[2023-06-20 09:28:51,515] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.92 | backward: 159.57 | backward_inner: 75.61 | backward_allreduce: 83.87 | step: 25.57\n",
      "【train】 epoch：1/1 step：93/288 loss：1.072266\n",
      "[2023-06-20 09:28:51,775] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:51,775] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.09 | backward_microstep: 159.39 | backward_inner_microstep: 75.32 | backward_allreduce_microstep: 83.94 | step_microstep: 25.74\n",
      "[2023-06-20 09:28:51,776] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.09 | backward: 159.38 | backward_inner: 75.34 | backward_allreduce: 83.93 | step: 25.75\n",
      "【train】 epoch：1/1 step：94/288 loss：1.134766\n",
      "[2023-06-20 09:28:52,037] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.74\n",
      "[2023-06-20 09:28:52,038] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.78 | backward_microstep: 160.09 | backward_inner_microstep: 75.92 | backward_allreduce_microstep: 84.04 | step_microstep: 26.12\n",
      "[2023-06-20 09:28:52,038] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.78 | backward: 160.08 | backward_inner: 75.94 | backward_allreduce: 84.03 | step: 26.12\n",
      "【train】 epoch：1/1 step：95/288 loss：1.355469\n",
      "[2023-06-20 09:28:52,297] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:52,297] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.55 | backward_microstep: 159.38 | backward_inner_microstep: 75.17 | backward_allreduce_microstep: 84.10 | step_microstep: 25.66\n",
      "[2023-06-20 09:28:52,298] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.56 | backward: 159.37 | backward_inner: 75.17 | backward_allreduce: 84.11 | step: 25.66\n",
      "【train】 epoch：1/1 step：96/288 loss：1.624023\n",
      "[2023-06-20 09:28:52,558] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:52,559] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.99 | backward_microstep: 160.89 | backward_inner_microstep: 75.50 | backward_allreduce_microstep: 85.28 | step_microstep: 25.69\n",
      "[2023-06-20 09:28:52,559] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.99 | backward: 160.88 | backward_inner: 75.51 | backward_allreduce: 85.28 | step: 25.70\n",
      "【train】 epoch：1/1 step：97/288 loss：1.404297\n",
      "[2023-06-20 09:28:52,818] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.74\n",
      "[2023-06-20 09:28:52,819] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.81 | backward_microstep: 159.02 | backward_inner_microstep: 75.71 | backward_allreduce_microstep: 83.20 | step_microstep: 25.89\n",
      "[2023-06-20 09:28:52,819] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.81 | backward: 159.01 | backward_inner: 75.71 | backward_allreduce: 83.20 | step: 25.90\n",
      "【train】 epoch：1/1 step：98/288 loss：1.348633\n",
      "[2023-06-20 09:28:53,079] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:28:53,080] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.34 | backward_microstep: 161.08 | backward_inner_microstep: 75.86 | backward_allreduce_microstep: 85.12 | step_microstep: 25.45\n",
      "[2023-06-20 09:28:53,080] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.35 | backward: 161.07 | backward_inner: 75.87 | backward_allreduce: 85.12 | step: 25.46\n",
      "【train】 epoch：1/1 step：99/288 loss：1.519531\n",
      "[2023-06-20 09:28:53,340] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:28:53,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:53,341] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=493.9176275949467, CurrSamplesPerSec=497.0217001673801, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:53,342] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.55 | backward_microstep: 160.07 | backward_inner_microstep: 75.69 | backward_allreduce_microstep: 84.28 | step_microstep: 26.60\n",
      "[2023-06-20 09:28:53,342] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.55 | backward: 160.07 | backward_inner: 75.70 | backward_allreduce: 84.27 | step: 26.60\n",
      "【train】 epoch：1/1 step：100/288 loss：1.333008\n",
      "[2023-06-20 09:28:53,602] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:53,602] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.29 | backward_microstep: 160.39 | backward_inner_microstep: 75.71 | backward_allreduce_microstep: 84.57 | step_microstep: 26.07\n",
      "[2023-06-20 09:28:53,602] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.29 | backward: 160.38 | backward_inner: 75.72 | backward_allreduce: 84.57 | step: 26.08\n",
      "【train】 epoch：1/1 step：101/288 loss：1.083984\n",
      "[2023-06-20 09:28:53,861] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:53,861] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.74 | backward_microstep: 158.82 | backward_inner_microstep: 75.34 | backward_allreduce_microstep: 83.37 | step_microstep: 25.50\n",
      "[2023-06-20 09:28:53,861] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.74 | backward: 158.82 | backward_inner: 75.36 | backward_allreduce: 83.37 | step: 25.50\n",
      "【train】 epoch：1/1 step：102/288 loss：1.497070\n",
      "[2023-06-20 09:28:54,122] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:28:54,122] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.61 | backward_microstep: 160.38 | backward_inner_microstep: 75.47 | backward_allreduce_microstep: 84.81 | step_microstep: 25.90\n",
      "[2023-06-20 09:28:54,122] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.62 | backward: 160.37 | backward_inner: 75.48 | backward_allreduce: 84.81 | step: 25.90\n",
      "【train】 epoch：1/1 step：103/288 loss：1.436523\n",
      "[2023-06-20 09:28:54,461] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:54,462] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.75 | backward_microstep: 239.26 | backward_inner_microstep: 75.78 | backward_allreduce_microstep: 163.36 | step_microstep: 25.69\n",
      "[2023-06-20 09:28:54,462] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.75 | backward: 239.26 | backward_inner: 75.78 | backward_allreduce: 163.36 | step: 25.70\n",
      "【train】 epoch：1/1 step：104/288 loss：1.142578\n",
      "[2023-06-20 09:28:54,722] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.71\n",
      "[2023-06-20 09:28:54,723] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.65 | backward_microstep: 160.54 | backward_inner_microstep: 75.61 | backward_allreduce_microstep: 84.83 | step_microstep: 25.62\n",
      "[2023-06-20 09:28:54,723] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.65 | backward: 160.54 | backward_inner: 75.62 | backward_allreduce: 84.82 | step: 25.63\n",
      "【train】 epoch：1/1 step：105/288 loss：1.238281\n",
      "[2023-06-20 09:28:54,982] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:28:54,983] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.26 | backward_microstep: 158.87 | backward_inner_microstep: 75.74 | backward_allreduce_microstep: 83.02 | step_microstep: 25.52\n",
      "[2023-06-20 09:28:54,983] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.27 | backward: 158.87 | backward_inner: 75.75 | backward_allreduce: 83.02 | step: 25.52\n",
      "【train】 epoch：1/1 step：106/288 loss：1.022461\n",
      "[2023-06-20 09:28:55,244] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:28:55,245] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.48 | backward_microstep: 161.25 | backward_inner_microstep: 75.54 | backward_allreduce_microstep: 85.61 | step_microstep: 25.63\n",
      "[2023-06-20 09:28:55,245] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.48 | backward: 161.25 | backward_inner: 75.55 | backward_allreduce: 85.62 | step: 25.63\n",
      "【train】 epoch：1/1 step：107/288 loss：1.230469\n",
      "[2023-06-20 09:28:55,504] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:28:55,504] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.12 | backward_microstep: 159.38 | backward_inner_microstep: 75.65 | backward_allreduce_microstep: 83.63 | step_microstep: 25.90\n",
      "[2023-06-20 09:28:55,505] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.12 | backward: 159.38 | backward_inner: 75.66 | backward_allreduce: 83.62 | step: 25.90\n",
      "【train】 epoch：1/1 step：108/288 loss：1.171875\n",
      "[2023-06-20 09:28:55,766] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:28:55,766] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.17 | backward_microstep: 160.72 | backward_inner_microstep: 75.54 | backward_allreduce_microstep: 85.08 | step_microstep: 25.80\n",
      "[2023-06-20 09:28:55,766] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.17 | backward: 160.72 | backward_inner: 75.54 | backward_allreduce: 85.09 | step: 25.81\n",
      "【train】 epoch：1/1 step：109/288 loss：1.298828\n",
      "[2023-06-20 09:28:56,026] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.72\n",
      "[2023-06-20 09:28:56,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:56,027] [INFO] [timer.py:215:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=492.86347461768844, CurrSamplesPerSec=498.073016320684, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:56,027] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.48 | backward_microstep: 160.03 | backward_inner_microstep: 75.43 | backward_allreduce_microstep: 84.50 | step_microstep: 26.05\n",
      "[2023-06-20 09:28:56,027] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.49 | backward: 160.02 | backward_inner: 75.44 | backward_allreduce: 84.50 | step: 26.06\n",
      "【train】 epoch：1/1 step：110/288 loss：1.478516\n",
      "[2023-06-20 09:28:56,287] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:28:56,287] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.46 | backward_microstep: 160.30 | backward_inner_microstep: 75.57 | backward_allreduce_microstep: 84.63 | step_microstep: 25.71\n",
      "[2023-06-20 09:28:56,288] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.46 | backward: 160.30 | backward_inner: 75.58 | backward_allreduce: 84.63 | step: 25.71\n",
      "【train】 epoch：1/1 step：111/288 loss：1.448242\n",
      "[2023-06-20 09:28:56,548] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:56,548] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.68 | backward_microstep: 159.95 | backward_inner_microstep: 75.58 | backward_allreduce_microstep: 84.28 | step_microstep: 26.00\n",
      "[2023-06-20 09:28:56,549] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.68 | backward: 159.95 | backward_inner: 75.58 | backward_allreduce: 84.28 | step: 26.00\n",
      "【train】 epoch：1/1 step：112/288 loss：1.336914\n",
      "[2023-06-20 09:28:56,810] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.77\n",
      "[2023-06-20 09:28:56,811] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.19 | backward_microstep: 161.05 | backward_inner_microstep: 75.61 | backward_allreduce_microstep: 85.34 | step_microstep: 26.15\n",
      "[2023-06-20 09:28:56,811] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.19 | backward: 161.05 | backward_inner: 75.62 | backward_allreduce: 85.34 | step: 26.15\n",
      "【train】 epoch：1/1 step：113/288 loss：1.397461\n",
      "[2023-06-20 09:28:57,072] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:28:57,072] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.53 | backward_microstep: 160.78 | backward_inner_microstep: 75.78 | backward_allreduce_microstep: 84.90 | step_microstep: 25.53\n",
      "[2023-06-20 09:28:57,072] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.53 | backward: 160.77 | backward_inner: 75.79 | backward_allreduce: 84.91 | step: 25.54\n",
      "【train】 epoch：1/1 step：114/288 loss：1.051758\n",
      "[2023-06-20 09:28:57,333] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:28:57,333] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.96 | backward_microstep: 159.99 | backward_inner_microstep: 75.02 | backward_allreduce_microstep: 84.88 | step_microstep: 26.06\n",
      "[2023-06-20 09:28:57,333] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.96 | backward: 159.99 | backward_inner: 75.03 | backward_allreduce: 84.88 | step: 26.07\n",
      "【train】 epoch：1/1 step：115/288 loss：1.339844\n",
      "[2023-06-20 09:28:57,593] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.79\n",
      "[2023-06-20 09:28:57,594] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.37 | backward_microstep: 159.21 | backward_inner_microstep: 75.02 | backward_allreduce_microstep: 84.10 | step_microstep: 25.70\n",
      "[2023-06-20 09:28:57,594] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.38 | backward: 159.21 | backward_inner: 75.02 | backward_allreduce: 84.10 | step: 25.70\n",
      "【train】 epoch：1/1 step：116/288 loss：1.245117\n",
      "[2023-06-20 09:28:57,852] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:28:57,853] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.48 | backward_microstep: 159.31 | backward_inner_microstep: 75.67 | backward_allreduce_microstep: 83.55 | step_microstep: 25.51\n",
      "[2023-06-20 09:28:57,853] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.48 | backward: 159.31 | backward_inner: 75.67 | backward_allreduce: 83.56 | step: 25.52\n",
      "【train】 epoch：1/1 step：117/288 loss：1.012695\n",
      "[2023-06-20 09:28:58,112] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:28:58,113] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.56 | backward_microstep: 159.50 | backward_inner_microstep: 75.43 | backward_allreduce_microstep: 83.97 | step_microstep: 25.85\n",
      "[2023-06-20 09:28:58,113] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.56 | backward: 159.50 | backward_inner: 75.44 | backward_allreduce: 83.97 | step: 25.85\n",
      "【train】 epoch：1/1 step：118/288 loss：1.359375\n",
      "[2023-06-20 09:28:58,374] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.73\n",
      "[2023-06-20 09:28:58,374] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.63 | backward_microstep: 160.81 | backward_inner_microstep: 75.67 | backward_allreduce_microstep: 85.02 | step_microstep: 25.87\n",
      "[2023-06-20 09:28:58,374] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.63 | backward: 160.80 | backward_inner: 75.68 | backward_allreduce: 85.02 | step: 25.87\n",
      "【train】 epoch：1/1 step：119/288 loss：1.300781\n",
      "[2023-06-20 09:28:58,634] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:28:58,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:28:58,635] [INFO] [timer.py:215:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=493.23731267106945, CurrSamplesPerSec=497.545880682608, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:28:58,635] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.28 | backward_microstep: 159.19 | backward_inner_microstep: 75.24 | backward_allreduce_microstep: 83.85 | step_microstep: 26.29\n",
      "[2023-06-20 09:28:58,635] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.28 | backward: 159.18 | backward_inner: 75.25 | backward_allreduce: 83.85 | step: 26.29\n",
      "【train】 epoch：1/1 step：120/288 loss：1.304688\n",
      "[2023-06-20 09:28:58,897] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:28:58,897] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.48 | backward_microstep: 161.53 | backward_inner_microstep: 75.15 | backward_allreduce_microstep: 86.28 | step_microstep: 25.95\n",
      "[2023-06-20 09:28:58,898] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.48 | backward: 161.53 | backward_inner: 75.16 | backward_allreduce: 86.28 | step: 25.96\n",
      "【train】 epoch：1/1 step：121/288 loss：1.396484\n",
      "[2023-06-20 09:28:59,158] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:28:59,158] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.51 | backward_microstep: 160.62 | backward_inner_microstep: 76.15 | backward_allreduce_microstep: 84.38 | step_microstep: 25.86\n",
      "[2023-06-20 09:28:59,159] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.51 | backward: 160.62 | backward_inner: 76.16 | backward_allreduce: 84.38 | step: 25.86\n",
      "【train】 epoch：1/1 step：122/288 loss：1.419922\n",
      "[2023-06-20 09:28:59,420] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:28:59,421] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.91 | backward_microstep: 161.76 | backward_inner_microstep: 75.17 | backward_allreduce_microstep: 86.49 | step_microstep: 25.73\n",
      "[2023-06-20 09:28:59,421] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.91 | backward: 161.76 | backward_inner: 75.17 | backward_allreduce: 86.49 | step: 25.73\n",
      "【train】 epoch：1/1 step：123/288 loss：1.177734\n",
      "[2023-06-20 09:28:59,681] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:28:59,682] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.19 | backward_microstep: 159.83 | backward_inner_microstep: 75.74 | backward_allreduce_microstep: 83.98 | step_microstep: 25.50\n",
      "[2023-06-20 09:28:59,682] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.20 | backward: 159.83 | backward_inner: 75.75 | backward_allreduce: 83.98 | step: 25.50\n",
      "【train】 epoch：1/1 step：124/288 loss：1.379883\n",
      "[2023-06-20 09:28:59,943] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:28:59,944] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.71 | backward_microstep: 160.70 | backward_inner_microstep: 74.96 | backward_allreduce_microstep: 85.64 | step_microstep: 25.70\n",
      "[2023-06-20 09:28:59,944] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.71 | backward: 160.70 | backward_inner: 74.97 | backward_allreduce: 85.64 | step: 25.70\n",
      "【train】 epoch：1/1 step：125/288 loss：1.416992\n",
      "[2023-06-20 09:29:00,205] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:00,206] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.18 | backward_microstep: 160.76 | backward_inner_microstep: 75.70 | backward_allreduce_microstep: 84.97 | step_microstep: 25.66\n",
      "[2023-06-20 09:29:00,206] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.18 | backward: 160.76 | backward_inner: 75.70 | backward_allreduce: 84.97 | step: 25.66\n",
      "【train】 epoch：1/1 step：126/288 loss：1.355469\n",
      "[2023-06-20 09:29:00,468] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.58\n",
      "[2023-06-20 09:29:00,469] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.64 | backward_microstep: 162.31 | backward_inner_microstep: 75.68 | backward_allreduce_microstep: 86.54 | step_microstep: 25.80\n",
      "[2023-06-20 09:29:00,469] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.64 | backward: 162.31 | backward_inner: 75.68 | backward_allreduce: 86.54 | step: 25.80\n",
      "【train】 epoch：1/1 step：127/288 loss：1.079102\n",
      "[2023-06-20 09:29:00,730] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:00,730] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.44 | backward_microstep: 161.29 | backward_inner_microstep: 75.38 | backward_allreduce_microstep: 85.82 | step_microstep: 25.87\n",
      "[2023-06-20 09:29:00,730] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.44 | backward: 161.29 | backward_inner: 75.39 | backward_allreduce: 85.82 | step: 25.87\n",
      "【train】 epoch：1/1 step：128/288 loss：1.058594\n",
      "[2023-06-20 09:29:00,990] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:00,990] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.77 | backward_microstep: 159.70 | backward_inner_microstep: 75.49 | backward_allreduce_microstep: 84.12 | step_microstep: 25.44\n",
      "[2023-06-20 09:29:00,990] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.77 | backward: 159.70 | backward_inner: 75.41 | backward_allreduce: 84.12 | step: 25.44\n",
      "【train】 epoch：1/1 step：129/288 loss：1.463867\n",
      "[2023-06-20 09:29:01,251] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:01,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:01,252] [INFO] [timer.py:215:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=493.4307597671618, CurrSamplesPerSec=496.4669427945511, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:01,252] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.99 | backward_microstep: 160.30 | backward_inner_microstep: 75.55 | backward_allreduce_microstep: 84.64 | step_microstep: 26.12\n",
      "[2023-06-20 09:29:01,252] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.99 | backward: 160.29 | backward_inner: 75.55 | backward_allreduce: 84.64 | step: 26.13\n",
      "【train】 epoch：1/1 step：130/288 loss：1.402344\n",
      "[2023-06-20 09:29:01,511] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:01,512] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.81 | backward_microstep: 160.30 | backward_inner_microstep: 75.20 | backward_allreduce_microstep: 85.01 | step_microstep: 25.59\n",
      "[2023-06-20 09:29:01,512] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.81 | backward: 160.30 | backward_inner: 75.20 | backward_allreduce: 85.00 | step: 25.59\n",
      "【train】 epoch：1/1 step：131/288 loss：0.885254\n",
      "[2023-06-20 09:29:01,771] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:01,771] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.38 | backward_microstep: 159.56 | backward_inner_microstep: 75.47 | backward_allreduce_microstep: 83.99 | step_microstep: 25.53\n",
      "[2023-06-20 09:29:01,772] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.38 | backward: 159.56 | backward_inner: 75.47 | backward_allreduce: 83.99 | step: 25.54\n",
      "【train】 epoch：1/1 step：132/288 loss：0.938477\n",
      "[2023-06-20 09:29:02,034] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:02,035] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.67 | backward_microstep: 161.34 | backward_inner_microstep: 74.87 | backward_allreduce_microstep: 86.35 | step_microstep: 25.90\n",
      "[2023-06-20 09:29:02,035] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.68 | backward: 161.31 | backward_inner: 74.88 | backward_allreduce: 86.34 | step: 25.90\n",
      "【train】 epoch：1/1 step：133/288 loss：0.974609\n",
      "[2023-06-20 09:29:02,295] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:29:02,295] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.47 | backward_microstep: 160.46 | backward_inner_microstep: 75.36 | backward_allreduce_microstep: 84.98 | step_microstep: 25.58\n",
      "[2023-06-20 09:29:02,296] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.47 | backward: 160.44 | backward_inner: 75.37 | backward_allreduce: 84.97 | step: 25.59\n",
      "【train】 epoch：1/1 step：134/288 loss：1.302734\n",
      "[2023-06-20 09:29:02,556] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:02,557] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.27 | backward_microstep: 159.50 | backward_inner_microstep: 75.69 | backward_allreduce_microstep: 83.71 | step_microstep: 26.07\n",
      "[2023-06-20 09:29:02,557] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.28 | backward: 159.49 | backward_inner: 75.70 | backward_allreduce: 83.72 | step: 26.07\n",
      "【train】 epoch：1/1 step：135/288 loss：0.937988\n",
      "[2023-06-20 09:29:02,816] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:02,817] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.65 | backward_microstep: 159.43 | backward_inner_microstep: 75.22 | backward_allreduce_microstep: 84.11 | step_microstep: 25.90\n",
      "[2023-06-20 09:29:02,817] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.65 | backward: 159.42 | backward_inner: 75.23 | backward_allreduce: 84.12 | step: 25.91\n",
      "【train】 epoch：1/1 step：136/288 loss：1.247070\n",
      "[2023-06-20 09:29:03,077] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.74\n",
      "[2023-06-20 09:29:03,078] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.48 | backward_microstep: 161.27 | backward_inner_microstep: 74.51 | backward_allreduce_microstep: 86.67 | step_microstep: 25.56\n",
      "[2023-06-20 09:29:03,078] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.48 | backward: 161.27 | backward_inner: 74.51 | backward_allreduce: 86.67 | step: 25.56\n",
      "【train】 epoch：1/1 step：137/288 loss：1.275391\n",
      "[2023-06-20 09:29:03,337] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:03,337] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.15 | backward_microstep: 159.18 | backward_inner_microstep: 75.38 | backward_allreduce_microstep: 83.71 | step_microstep: 25.92\n",
      "[2023-06-20 09:29:03,338] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.15 | backward: 159.18 | backward_inner: 75.39 | backward_allreduce: 83.71 | step: 25.93\n",
      "【train】 epoch：1/1 step：138/288 loss：0.990723\n",
      "[2023-06-20 09:29:03,597] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.75\n",
      "[2023-06-20 09:29:03,597] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.76 | backward_microstep: 159.19 | backward_inner_microstep: 75.21 | backward_allreduce_microstep: 83.86 | step_microstep: 25.67\n",
      "[2023-06-20 09:29:03,597] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.76 | backward: 159.18 | backward_inner: 75.23 | backward_allreduce: 83.86 | step: 25.67\n",
      "【train】 epoch：1/1 step：139/288 loss：1.240234\n",
      "[2023-06-20 09:29:03,935] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:29:03,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:03,936] [INFO] [timer.py:215:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=492.66992967687713, CurrSamplesPerSec=381.6063533898986, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:03,936] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.93 | backward_microstep: 238.21 | backward_inner_microstep: 75.24 | backward_allreduce_microstep: 162.84 | step_microstep: 26.02\n",
      "[2023-06-20 09:29:03,936] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.93 | backward: 238.19 | backward_inner: 75.24 | backward_allreduce: 162.84 | step: 26.03\n",
      "【train】 epoch：1/1 step：140/288 loss：1.132812\n",
      "[2023-06-20 09:29:04,197] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:04,197] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.47 | backward_microstep: 160.47 | backward_inner_microstep: 75.75 | backward_allreduce_microstep: 84.63 | step_microstep: 25.94\n",
      "[2023-06-20 09:29:04,198] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.47 | backward: 160.47 | backward_inner: 75.76 | backward_allreduce: 84.63 | step: 25.95\n",
      "【train】 epoch：1/1 step：141/288 loss：1.089844\n",
      "[2023-06-20 09:29:04,457] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:04,458] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.49 | backward_microstep: 159.95 | backward_inner_microstep: 75.49 | backward_allreduce_microstep: 84.37 | step_microstep: 25.74\n",
      "[2023-06-20 09:29:04,458] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.50 | backward: 159.95 | backward_inner: 75.49 | backward_allreduce: 84.38 | step: 25.75\n",
      "【train】 epoch：1/1 step：142/288 loss：0.891113\n",
      "[2023-06-20 09:29:04,718] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.76\n",
      "[2023-06-20 09:29:04,718] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.11 | backward_microstep: 160.68 | backward_inner_microstep: 75.79 | backward_allreduce_microstep: 84.79 | step_microstep: 25.84\n",
      "[2023-06-20 09:29:04,718] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.11 | backward: 160.67 | backward_inner: 75.79 | backward_allreduce: 84.78 | step: 25.84\n",
      "【train】 epoch：1/1 step：143/288 loss：1.225586\n",
      "[2023-06-20 09:29:04,978] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:04,979] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.96 | backward_microstep: 160.87 | backward_inner_microstep: 75.37 | backward_allreduce_microstep: 85.39 | step_microstep: 25.51\n",
      "[2023-06-20 09:29:04,979] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.96 | backward: 160.87 | backward_inner: 75.39 | backward_allreduce: 85.39 | step: 25.51\n",
      "【train】 epoch：1/1 step：144/288 loss：1.344727\n",
      "[2023-06-20 09:29:05,239] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:05,239] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.38 | backward_microstep: 160.73 | backward_inner_microstep: 75.45 | backward_allreduce_microstep: 85.18 | step_microstep: 25.59\n",
      "[2023-06-20 09:29:05,240] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.38 | backward: 160.72 | backward_inner: 75.46 | backward_allreduce: 85.18 | step: 25.60\n",
      "【train】 epoch：1/1 step：145/288 loss：1.291992\n",
      "[2023-06-20 09:29:05,499] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:05,499] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.98 | backward_microstep: 159.96 | backward_inner_microstep: 75.07 | backward_allreduce_microstep: 84.80 | step_microstep: 25.66\n",
      "[2023-06-20 09:29:05,499] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.98 | backward: 159.96 | backward_inner: 75.08 | backward_allreduce: 84.80 | step: 25.67\n",
      "【train】 epoch：1/1 step：146/288 loss：0.972656\n",
      "[2023-06-20 09:29:05,760] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:05,761] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.78 | backward_microstep: 159.86 | backward_inner_microstep: 75.69 | backward_allreduce_microstep: 84.05 | step_microstep: 25.79\n",
      "[2023-06-20 09:29:05,761] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.78 | backward: 159.84 | backward_inner: 75.70 | backward_allreduce: 84.04 | step: 25.79\n",
      "【train】 epoch：1/1 step：147/288 loss：1.177734\n",
      "[2023-06-20 09:29:06,020] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.72\n",
      "[2023-06-20 09:29:06,021] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.64 | backward_microstep: 160.89 | backward_inner_microstep: 75.35 | backward_allreduce_microstep: 85.45 | step_microstep: 25.58\n",
      "[2023-06-20 09:29:06,021] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.64 | backward: 160.89 | backward_inner: 75.35 | backward_allreduce: 85.45 | step: 25.58\n",
      "【train】 epoch：1/1 step：148/288 loss：1.272461\n",
      "[2023-06-20 09:29:06,281] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:06,282] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.47 | backward_microstep: 160.39 | backward_inner_microstep: 75.63 | backward_allreduce_microstep: 84.65 | step_microstep: 25.62\n",
      "[2023-06-20 09:29:06,282] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.47 | backward: 160.38 | backward_inner: 75.63 | backward_allreduce: 84.64 | step: 25.62\n",
      "【train】 epoch：1/1 step：149/288 loss：0.937012\n",
      "[2023-06-20 09:29:06,542] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:06,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:06,543] [INFO] [timer.py:215:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=492.9982086204496, CurrSamplesPerSec=496.4472021201636, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:06,543] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.68 | backward_microstep: 160.59 | backward_inner_microstep: 74.96 | backward_allreduce_microstep: 85.53 | step_microstep: 26.18\n",
      "[2023-06-20 09:29:06,543] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.68 | backward: 160.58 | backward_inner: 74.97 | backward_allreduce: 85.53 | step: 26.18\n",
      "【train】 epoch：1/1 step：150/288 loss：0.964844\n",
      "[2023-06-20 09:29:06,803] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:06,803] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.24 | backward_microstep: 160.09 | backward_inner_microstep: 75.37 | backward_allreduce_microstep: 84.62 | step_microstep: 25.76\n",
      "[2023-06-20 09:29:06,803] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.25 | backward: 160.08 | backward_inner: 75.38 | backward_allreduce: 84.62 | step: 25.77\n",
      "【train】 epoch：1/1 step：151/288 loss：1.411133\n",
      "[2023-06-20 09:29:07,064] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:07,064] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.58 | backward_microstep: 159.31 | backward_inner_microstep: 75.21 | backward_allreduce_microstep: 84.00 | step_microstep: 25.67\n",
      "[2023-06-20 09:29:07,064] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.58 | backward: 159.30 | backward_inner: 75.21 | backward_allreduce: 83.98 | step: 25.67\n",
      "【train】 epoch：1/1 step：152/288 loss：1.334961\n",
      "[2023-06-20 09:29:07,326] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:07,326] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.61 | backward_microstep: 160.70 | backward_inner_microstep: 75.51 | backward_allreduce_microstep: 85.06 | step_microstep: 25.68\n",
      "[2023-06-20 09:29:07,326] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.61 | backward: 160.70 | backward_inner: 75.54 | backward_allreduce: 85.06 | step: 25.68\n",
      "【train】 epoch：1/1 step：153/288 loss：1.114258\n",
      "[2023-06-20 09:29:07,586] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:07,586] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.18 | backward_microstep: 160.39 | backward_inner_microstep: 75.97 | backward_allreduce_microstep: 84.31 | step_microstep: 25.47\n",
      "[2023-06-20 09:29:07,587] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.18 | backward: 160.39 | backward_inner: 75.98 | backward_allreduce: 84.30 | step: 25.48\n",
      "【train】 epoch：1/1 step：154/288 loss：1.159180\n",
      "[2023-06-20 09:29:07,847] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:07,847] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.73 | backward_microstep: 161.34 | backward_inner_microstep: 75.99 | backward_allreduce_microstep: 85.25 | step_microstep: 25.84\n",
      "[2023-06-20 09:29:07,848] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.73 | backward: 161.33 | backward_inner: 76.00 | backward_allreduce: 85.25 | step: 25.84\n",
      "【train】 epoch：1/1 step：155/288 loss：1.016602\n",
      "[2023-06-20 09:29:08,107] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:08,107] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.25 | backward_microstep: 159.62 | backward_inner_microstep: 75.57 | backward_allreduce_microstep: 83.95 | step_microstep: 25.88\n",
      "[2023-06-20 09:29:08,108] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.25 | backward: 159.62 | backward_inner: 75.58 | backward_allreduce: 83.96 | step: 25.88\n",
      "【train】 epoch：1/1 step：156/288 loss：0.850098\n",
      "[2023-06-20 09:29:08,369] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.76\n",
      "[2023-06-20 09:29:08,370] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.15 | backward_microstep: 160.79 | backward_inner_microstep: 76.50 | backward_allreduce_microstep: 84.19 | step_microstep: 25.88\n",
      "[2023-06-20 09:29:08,370] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.15 | backward: 160.79 | backward_inner: 76.51 | backward_allreduce: 84.20 | step: 25.89\n",
      "【train】 epoch：1/1 step：157/288 loss：1.407227\n",
      "[2023-06-20 09:29:08,629] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:08,630] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.67 | backward_microstep: 160.77 | backward_inner_microstep: 75.60 | backward_allreduce_microstep: 85.08 | step_microstep: 25.77\n",
      "[2023-06-20 09:29:08,630] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.67 | backward: 160.77 | backward_inner: 75.60 | backward_allreduce: 85.08 | step: 25.78\n",
      "【train】 epoch：1/1 step：158/288 loss：0.862305\n",
      "[2023-06-20 09:29:08,890] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:08,890] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.24 | backward_microstep: 160.06 | backward_inner_microstep: 75.39 | backward_allreduce_microstep: 84.55 | step_microstep: 25.76\n",
      "[2023-06-20 09:29:08,890] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.25 | backward: 160.06 | backward_inner: 75.42 | backward_allreduce: 84.55 | step: 25.76\n",
      "【train】 epoch：1/1 step：159/288 loss：1.339844\n",
      "[2023-06-20 09:29:09,150] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:09,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:09,151] [INFO] [timer.py:215:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=493.2759973207846, CurrSamplesPerSec=497.09809353617095, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:09,151] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.44 | backward_microstep: 160.10 | backward_inner_microstep: 75.32 | backward_allreduce_microstep: 84.68 | step_microstep: 26.52\n",
      "[2023-06-20 09:29:09,151] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.44 | backward: 160.10 | backward_inner: 75.33 | backward_allreduce: 84.69 | step: 26.53\n",
      "【train】 epoch：1/1 step：160/288 loss：0.897949\n",
      "[2023-06-20 09:29:09,411] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:29:09,412] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.40 | backward_microstep: 160.10 | backward_inner_microstep: 74.98 | backward_allreduce_microstep: 85.02 | step_microstep: 25.81\n",
      "[2023-06-20 09:29:09,412] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.40 | backward: 160.10 | backward_inner: 74.99 | backward_allreduce: 85.01 | step: 25.81\n",
      "【train】 epoch：1/1 step：161/288 loss：1.413086\n",
      "[2023-06-20 09:29:09,672] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:09,673] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.46 | backward_microstep: 159.15 | backward_inner_microstep: 75.47 | backward_allreduce_microstep: 83.58 | step_microstep: 26.05\n",
      "[2023-06-20 09:29:09,673] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.47 | backward: 159.14 | backward_inner: 75.48 | backward_allreduce: 83.58 | step: 26.05\n",
      "【train】 epoch：1/1 step：162/288 loss：1.336914\n",
      "[2023-06-20 09:29:09,934] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:09,935] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.19 | backward_microstep: 160.44 | backward_inner_microstep: 75.04 | backward_allreduce_microstep: 85.30 | step_microstep: 26.11\n",
      "[2023-06-20 09:29:09,935] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.20 | backward: 160.43 | backward_inner: 75.04 | backward_allreduce: 85.28 | step: 26.11\n",
      "【train】 epoch：1/1 step：163/288 loss：1.046875\n",
      "[2023-06-20 09:29:10,195] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:10,195] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.15 | backward_microstep: 159.67 | backward_inner_microstep: 75.67 | backward_allreduce_microstep: 83.90 | step_microstep: 25.85\n",
      "[2023-06-20 09:29:10,195] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.16 | backward: 159.67 | backward_inner: 75.67 | backward_allreduce: 83.91 | step: 25.86\n",
      "【train】 epoch：1/1 step：164/288 loss：1.204102\n",
      "[2023-06-20 09:29:10,456] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:10,456] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.22 | backward_microstep: 160.66 | backward_inner_microstep: 75.81 | backward_allreduce_microstep: 84.76 | step_microstep: 25.58\n",
      "[2023-06-20 09:29:10,456] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.23 | backward: 160.66 | backward_inner: 75.82 | backward_allreduce: 84.76 | step: 25.58\n",
      "【train】 epoch：1/1 step：165/288 loss：1.125977\n",
      "[2023-06-20 09:29:10,716] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:29:10,716] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.77 | backward_microstep: 159.33 | backward_inner_microstep: 75.35 | backward_allreduce_microstep: 83.88 | step_microstep: 25.67\n",
      "[2023-06-20 09:29:10,716] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.77 | backward: 159.32 | backward_inner: 75.36 | backward_allreduce: 83.89 | step: 25.68\n",
      "【train】 epoch：1/1 step：166/288 loss：1.374023\n",
      "[2023-06-20 09:29:10,977] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:10,978] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.38 | backward_microstep: 160.35 | backward_inner_microstep: 75.89 | backward_allreduce_microstep: 84.35 | step_microstep: 25.68\n",
      "[2023-06-20 09:29:10,978] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.39 | backward: 160.35 | backward_inner: 75.90 | backward_allreduce: 84.36 | step: 25.68\n",
      "【train】 epoch：1/1 step：167/288 loss：1.313477\n",
      "[2023-06-20 09:29:11,238] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:11,238] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.04 | backward_microstep: 159.42 | backward_inner_microstep: 75.72 | backward_allreduce_microstep: 83.58 | step_microstep: 25.65\n",
      "[2023-06-20 09:29:11,238] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.05 | backward: 159.40 | backward_inner: 75.72 | backward_allreduce: 83.58 | step: 25.65\n",
      "【train】 epoch：1/1 step：168/288 loss：1.281250\n",
      "[2023-06-20 09:29:11,499] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:11,500] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.26 | backward_microstep: 160.71 | backward_inner_microstep: 75.78 | backward_allreduce_microstep: 84.81 | step_microstep: 25.85\n",
      "[2023-06-20 09:29:11,500] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.26 | backward: 160.69 | backward_inner: 75.78 | backward_allreduce: 84.81 | step: 25.85\n",
      "【train】 epoch：1/1 step：169/288 loss：1.084961\n",
      "[2023-06-20 09:29:11,761] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.77\n",
      "[2023-06-20 09:29:11,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:11,762] [INFO] [timer.py:215:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=493.4914831568726, CurrSamplesPerSec=495.22222744928746, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:11,762] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.42 | backward_microstep: 161.26 | backward_inner_microstep: 75.41 | backward_allreduce_microstep: 85.76 | step_microstep: 26.35\n",
      "[2023-06-20 09:29:11,762] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.42 | backward: 161.26 | backward_inner: 75.42 | backward_allreduce: 85.76 | step: 26.36\n",
      "【train】 epoch：1/1 step：170/288 loss：1.372070\n",
      "[2023-06-20 09:29:12,021] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:12,022] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.24 | backward_microstep: 159.85 | backward_inner_microstep: 75.35 | backward_allreduce_microstep: 84.40 | step_microstep: 25.57\n",
      "[2023-06-20 09:29:12,022] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.24 | backward: 159.85 | backward_inner: 75.36 | backward_allreduce: 84.40 | step: 25.57\n",
      "【train】 epoch：1/1 step：171/288 loss：1.394531\n",
      "[2023-06-20 09:29:12,281] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:12,281] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.54 | backward_microstep: 159.29 | backward_inner_microstep: 75.49 | backward_allreduce_microstep: 83.70 | step_microstep: 25.63\n",
      "[2023-06-20 09:29:12,282] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.54 | backward: 159.28 | backward_inner: 75.50 | backward_allreduce: 83.70 | step: 25.63\n",
      "【train】 epoch：1/1 step：172/288 loss：1.292969\n",
      "[2023-06-20 09:29:12,540] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:29:12,540] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.39 | backward_microstep: 158.56 | backward_inner_microstep: 75.41 | backward_allreduce_microstep: 83.05 | step_microstep: 25.65\n",
      "[2023-06-20 09:29:12,540] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.40 | backward: 158.56 | backward_inner: 75.41 | backward_allreduce: 83.06 | step: 25.65\n",
      "【train】 epoch：1/1 step：173/288 loss：1.083984\n",
      "[2023-06-20 09:29:12,800] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:12,800] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.36 | backward_microstep: 158.50 | backward_inner_microstep: 75.42 | backward_allreduce_microstep: 82.99 | step_microstep: 25.95\n",
      "[2023-06-20 09:29:12,800] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.36 | backward: 158.50 | backward_inner: 75.43 | backward_allreduce: 82.99 | step: 25.95\n",
      "【train】 epoch：1/1 step：174/288 loss：1.363281\n",
      "[2023-06-20 09:29:13,061] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:13,061] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.38 | backward_microstep: 159.97 | backward_inner_microstep: 75.57 | backward_allreduce_microstep: 84.30 | step_microstep: 25.65\n",
      "[2023-06-20 09:29:13,062] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.39 | backward: 159.96 | backward_inner: 75.58 | backward_allreduce: 84.30 | step: 25.66\n",
      "【train】 epoch：1/1 step：175/288 loss：1.194336\n",
      "[2023-06-20 09:29:13,400] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:29:13,401] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 149.06 | backward_microstep: 160.08 | backward_inner_microstep: 75.53 | backward_allreduce_microstep: 84.46 | step_microstep: 26.03\n",
      "[2023-06-20 09:29:13,401] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 149.06 | backward: 160.08 | backward_inner: 75.53 | backward_allreduce: 84.46 | step: 26.04\n",
      "【train】 epoch：1/1 step：176/288 loss：1.120117\n",
      "[2023-06-20 09:29:13,662] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:13,663] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.78 | backward_microstep: 161.32 | backward_inner_microstep: 75.42 | backward_allreduce_microstep: 85.79 | step_microstep: 25.72\n",
      "[2023-06-20 09:29:13,663] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.78 | backward: 161.32 | backward_inner: 75.44 | backward_allreduce: 85.79 | step: 25.73\n",
      "【train】 epoch：1/1 step：177/288 loss：1.115234\n",
      "[2023-06-20 09:29:13,923] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:13,923] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.08 | backward_microstep: 159.54 | backward_inner_microstep: 75.38 | backward_allreduce_microstep: 84.06 | step_microstep: 25.74\n",
      "[2023-06-20 09:29:13,924] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.08 | backward: 159.53 | backward_inner: 75.39 | backward_allreduce: 84.06 | step: 25.75\n",
      "【train】 epoch：1/1 step：178/288 loss：1.200195\n",
      "[2023-06-20 09:29:14,183] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:14,184] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.98 | backward_microstep: 159.38 | backward_inner_microstep: 75.63 | backward_allreduce_microstep: 83.65 | step_microstep: 25.79\n",
      "[2023-06-20 09:29:14,184] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.98 | backward: 159.38 | backward_inner: 75.64 | backward_allreduce: 83.65 | step: 25.80\n",
      "【train】 epoch：1/1 step：179/288 loss：1.288086\n",
      "[2023-06-20 09:29:14,444] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:29:14,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:14,445] [INFO] [timer.py:215:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=492.9111591789171, CurrSamplesPerSec=497.0750810140177, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:14,445] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.67 | backward_microstep: 160.12 | backward_inner_microstep: 75.56 | backward_allreduce_microstep: 84.46 | step_microstep: 26.30\n",
      "[2023-06-20 09:29:14,445] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.68 | backward: 160.12 | backward_inner: 75.56 | backward_allreduce: 84.47 | step: 26.30\n",
      "【train】 epoch：1/1 step：180/288 loss：1.292969\n",
      "[2023-06-20 09:29:14,706] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:14,706] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.83 | backward_microstep: 160.14 | backward_inner_microstep: 76.37 | backward_allreduce_microstep: 83.65 | step_microstep: 26.10\n",
      "[2023-06-20 09:29:14,707] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.83 | backward: 160.14 | backward_inner: 76.38 | backward_allreduce: 83.66 | step: 26.10\n",
      "【train】 epoch：1/1 step：181/288 loss：0.964844\n",
      "[2023-06-20 09:29:14,967] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:14,968] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.44 | backward_microstep: 159.71 | backward_inner_microstep: 76.20 | backward_allreduce_microstep: 83.42 | step_microstep: 25.92\n",
      "[2023-06-20 09:29:14,968] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.44 | backward: 159.71 | backward_inner: 76.20 | backward_allreduce: 83.42 | step: 25.93\n",
      "【train】 epoch：1/1 step：182/288 loss：1.367188\n",
      "[2023-06-20 09:29:15,226] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.72\n",
      "[2023-06-20 09:29:15,227] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.21 | backward_microstep: 158.64 | backward_inner_microstep: 75.18 | backward_allreduce_microstep: 83.36 | step_microstep: 26.09\n",
      "[2023-06-20 09:29:15,227] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.21 | backward: 158.63 | backward_inner: 75.19 | backward_allreduce: 83.37 | step: 26.09\n",
      "【train】 epoch：1/1 step：183/288 loss：0.992188\n",
      "[2023-06-20 09:29:15,487] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:15,488] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.53 | backward_microstep: 160.27 | backward_inner_microstep: 75.99 | backward_allreduce_microstep: 84.18 | step_microstep: 26.12\n",
      "[2023-06-20 09:29:15,488] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.53 | backward: 160.26 | backward_inner: 76.00 | backward_allreduce: 84.18 | step: 26.12\n",
      "【train】 epoch：1/1 step：184/288 loss：1.202148\n",
      "[2023-06-20 09:29:15,749] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.71\n",
      "[2023-06-20 09:29:15,749] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.79 | backward_microstep: 160.27 | backward_inner_microstep: 75.74 | backward_allreduce_microstep: 84.43 | step_microstep: 25.99\n",
      "[2023-06-20 09:29:15,749] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.79 | backward: 160.27 | backward_inner: 75.75 | backward_allreduce: 84.43 | step: 25.99\n",
      "【train】 epoch：1/1 step：185/288 loss：1.205078\n",
      "[2023-06-20 09:29:16,009] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:16,010] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.33 | backward_microstep: 160.30 | backward_inner_microstep: 76.09 | backward_allreduce_microstep: 84.10 | step_microstep: 25.85\n",
      "[2023-06-20 09:29:16,010] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.33 | backward: 160.29 | backward_inner: 76.09 | backward_allreduce: 84.08 | step: 25.85\n",
      "【train】 epoch：1/1 step：186/288 loss：1.039062\n",
      "[2023-06-20 09:29:16,270] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:16,271] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.87 | backward_microstep: 161.33 | backward_inner_microstep: 75.66 | backward_allreduce_microstep: 85.57 | step_microstep: 25.87\n",
      "[2023-06-20 09:29:16,271] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.88 | backward: 161.32 | backward_inner: 75.66 | backward_allreduce: 85.57 | step: 25.87\n",
      "【train】 epoch：1/1 step：187/288 loss：0.978516\n",
      "[2023-06-20 09:29:16,532] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:16,533] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.24 | backward_microstep: 160.71 | backward_inner_microstep: 75.93 | backward_allreduce_microstep: 84.69 | step_microstep: 25.76\n",
      "[2023-06-20 09:29:16,533] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.25 | backward: 160.71 | backward_inner: 75.93 | backward_allreduce: 84.69 | step: 25.77\n",
      "【train】 epoch：1/1 step：188/288 loss：1.256836\n",
      "[2023-06-20 09:29:16,795] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:16,795] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.84 | backward_microstep: 161.70 | backward_inner_microstep: 75.80 | backward_allreduce_microstep: 85.79 | step_microstep: 25.62\n",
      "[2023-06-20 09:29:16,795] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.84 | backward: 161.70 | backward_inner: 75.81 | backward_allreduce: 85.79 | step: 25.62\n",
      "【train】 epoch：1/1 step：189/288 loss：1.171875\n",
      "[2023-06-20 09:29:17,056] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:29:17,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:17,057] [INFO] [timer.py:215:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=493.11239360041674, CurrSamplesPerSec=496.09626396819044, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:17,057] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.82 | backward_microstep: 160.35 | backward_inner_microstep: 75.53 | backward_allreduce_microstep: 84.73 | step_microstep: 26.42\n",
      "[2023-06-20 09:29:17,057] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.82 | backward: 160.35 | backward_inner: 75.53 | backward_allreduce: 84.73 | step: 26.42\n",
      "【train】 epoch：1/1 step：190/288 loss：1.223633\n",
      "[2023-06-20 09:29:17,318] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:17,319] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.45 | backward_microstep: 160.91 | backward_inner_microstep: 75.30 | backward_allreduce_microstep: 85.52 | step_microstep: 26.01\n",
      "[2023-06-20 09:29:17,319] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.45 | backward: 160.91 | backward_inner: 75.31 | backward_allreduce: 85.52 | step: 26.02\n",
      "【train】 epoch：1/1 step：191/288 loss：1.296875\n",
      "[2023-06-20 09:29:17,578] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:17,579] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.37 | backward_microstep: 160.06 | backward_inner_microstep: 75.89 | backward_allreduce_microstep: 84.07 | step_microstep: 25.69\n",
      "[2023-06-20 09:29:17,579] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.38 | backward: 160.06 | backward_inner: 75.88 | backward_allreduce: 84.07 | step: 25.69\n",
      "【train】 epoch：1/1 step：192/288 loss：1.271484\n",
      "[2023-06-20 09:29:17,841] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.79\n",
      "[2023-06-20 09:29:17,842] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.53 | backward_microstep: 161.35 | backward_inner_microstep: 75.36 | backward_allreduce_microstep: 85.89 | step_microstep: 26.13\n",
      "[2023-06-20 09:29:17,842] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.53 | backward: 161.35 | backward_inner: 75.37 | backward_allreduce: 85.90 | step: 26.14\n",
      "【train】 epoch：1/1 step：193/288 loss：0.846680\n",
      "[2023-06-20 09:29:18,101] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:18,102] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.81 | backward_microstep: 160.71 | backward_inner_microstep: 75.73 | backward_allreduce_microstep: 84.88 | step_microstep: 25.37\n",
      "[2023-06-20 09:29:18,102] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.80 | backward: 160.71 | backward_inner: 75.74 | backward_allreduce: 84.88 | step: 25.37\n",
      "【train】 epoch：1/1 step：194/288 loss：1.104492\n",
      "[2023-06-20 09:29:18,363] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.57\n",
      "[2023-06-20 09:29:18,364] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.57 | backward_microstep: 161.38 | backward_inner_microstep: 75.53 | backward_allreduce_microstep: 85.72 | step_microstep: 25.95\n",
      "[2023-06-20 09:29:18,364] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.58 | backward: 161.37 | backward_inner: 75.55 | backward_allreduce: 85.71 | step: 25.95\n",
      "【train】 epoch：1/1 step：195/288 loss：1.138672\n",
      "[2023-06-20 09:29:18,624] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:18,624] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.75 | backward_microstep: 159.77 | backward_inner_microstep: 76.07 | backward_allreduce_microstep: 83.60 | step_microstep: 25.70\n",
      "[2023-06-20 09:29:18,624] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.75 | backward: 159.77 | backward_inner: 76.08 | backward_allreduce: 83.60 | step: 25.70\n",
      "【train】 epoch：1/1 step：196/288 loss：1.363281\n",
      "[2023-06-20 09:29:18,886] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:18,886] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.33 | backward_microstep: 161.45 | backward_inner_microstep: 75.10 | backward_allreduce_microstep: 86.22 | step_microstep: 26.23\n",
      "[2023-06-20 09:29:18,886] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.33 | backward: 161.43 | backward_inner: 75.11 | backward_allreduce: 86.21 | step: 26.23\n",
      "【train】 epoch：1/1 step：197/288 loss：1.069336\n",
      "[2023-06-20 09:29:19,147] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:19,147] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.75 | backward_microstep: 160.39 | backward_inner_microstep: 75.32 | backward_allreduce_microstep: 84.97 | step_microstep: 25.74\n",
      "[2023-06-20 09:29:19,147] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.75 | backward: 160.39 | backward_inner: 75.33 | backward_allreduce: 84.97 | step: 25.74\n",
      "【train】 epoch：1/1 step：198/288 loss：1.207031\n",
      "[2023-06-20 09:29:19,408] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:19,409] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.96 | backward_microstep: 160.53 | backward_inner_microstep: 75.46 | backward_allreduce_microstep: 84.97 | step_microstep: 25.79\n",
      "[2023-06-20 09:29:19,409] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.96 | backward: 160.52 | backward_inner: 75.47 | backward_allreduce: 84.97 | step: 25.79\n",
      "【train】 epoch：1/1 step：199/288 loss：1.130859\n",
      "[2023-06-20 09:29:19,670] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.71\n",
      "[2023-06-20 09:29:19,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:19,671] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=493.27079602495365, CurrSamplesPerSec=495.5133830011888, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:19,671] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.74 | backward_microstep: 160.87 | backward_inner_microstep: 75.74 | backward_allreduce_microstep: 85.03 | step_microstep: 26.36\n",
      "[2023-06-20 09:29:19,671] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.74 | backward: 160.87 | backward_inner: 75.75 | backward_allreduce: 85.03 | step: 26.36\n",
      "【train】 epoch：1/1 step：200/288 loss：1.195312\n",
      "[2023-06-20 09:29:19,931] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:29:19,931] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.40 | backward_microstep: 160.02 | backward_inner_microstep: 75.41 | backward_allreduce_microstep: 84.51 | step_microstep: 25.59\n",
      "[2023-06-20 09:29:19,931] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.40 | backward: 160.02 | backward_inner: 75.41 | backward_allreduce: 84.51 | step: 25.60\n",
      "【train】 epoch：1/1 step：201/288 loss：1.126953\n",
      "[2023-06-20 09:29:20,191] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:20,192] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.17 | backward_microstep: 160.06 | backward_inner_microstep: 75.73 | backward_allreduce_microstep: 84.22 | step_microstep: 25.62\n",
      "[2023-06-20 09:29:20,192] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.17 | backward: 160.04 | backward_inner: 75.73 | backward_allreduce: 84.22 | step: 25.62\n",
      "【train】 epoch：1/1 step：202/288 loss：1.058594\n",
      "[2023-06-20 09:29:20,454] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:29:20,454] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.76 | backward_microstep: 160.71 | backward_inner_microstep: 75.52 | backward_allreduce_microstep: 85.09 | step_microstep: 25.80\n",
      "[2023-06-20 09:29:20,455] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.76 | backward: 160.71 | backward_inner: 75.53 | backward_allreduce: 85.08 | step: 25.80\n",
      "【train】 epoch：1/1 step：203/288 loss：1.132812\n",
      "[2023-06-20 09:29:20,715] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:20,715] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.24 | backward_microstep: 159.60 | backward_inner_microstep: 75.91 | backward_allreduce_microstep: 83.57 | step_microstep: 25.73\n",
      "[2023-06-20 09:29:20,715] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.24 | backward: 159.60 | backward_inner: 75.93 | backward_allreduce: 83.57 | step: 25.74\n",
      "【train】 epoch：1/1 step：204/288 loss：1.048828\n",
      "[2023-06-20 09:29:20,975] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:20,976] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.85 | backward_microstep: 159.93 | backward_inner_microstep: 75.92 | backward_allreduce_microstep: 83.91 | step_microstep: 25.73\n",
      "[2023-06-20 09:29:20,976] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.85 | backward: 159.93 | backward_inner: 75.93 | backward_allreduce: 83.91 | step: 25.73\n",
      "【train】 epoch：1/1 step：205/288 loss：1.295898\n",
      "[2023-06-20 09:29:21,235] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:29:21,236] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.56 | backward_microstep: 159.33 | backward_inner_microstep: 75.60 | backward_allreduce_microstep: 83.63 | step_microstep: 25.65\n",
      "[2023-06-20 09:29:21,236] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.56 | backward: 159.33 | backward_inner: 75.61 | backward_allreduce: 83.63 | step: 25.65\n",
      "【train】 epoch：1/1 step：206/288 loss：1.389648\n",
      "[2023-06-20 09:29:21,496] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:21,496] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.47 | backward_microstep: 159.11 | backward_inner_microstep: 75.90 | backward_allreduce_microstep: 83.11 | step_microstep: 25.58\n",
      "[2023-06-20 09:29:21,496] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.47 | backward: 159.11 | backward_inner: 75.91 | backward_allreduce: 83.11 | step: 25.59\n",
      "【train】 epoch：1/1 step：207/288 loss：1.180664\n",
      "[2023-06-20 09:29:21,756] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:21,757] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.79 | backward_microstep: 159.34 | backward_inner_microstep: 75.86 | backward_allreduce_microstep: 83.38 | step_microstep: 25.91\n",
      "[2023-06-20 09:29:21,757] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.80 | backward: 159.34 | backward_inner: 75.87 | backward_allreduce: 83.37 | step: 25.91\n",
      "【train】 epoch：1/1 step：208/288 loss：0.863770\n",
      "[2023-06-20 09:29:22,017] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:22,018] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.73 | backward_microstep: 160.50 | backward_inner_microstep: 76.49 | backward_allreduce_microstep: 83.88 | step_microstep: 25.82\n",
      "[2023-06-20 09:29:22,018] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.73 | backward: 160.49 | backward_inner: 76.51 | backward_allreduce: 83.88 | step: 25.83\n",
      "【train】 epoch：1/1 step：209/288 loss：1.222656\n",
      "[2023-06-20 09:29:22,278] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:22,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:22,279] [INFO] [timer.py:215:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=493.4673593403822, CurrSamplesPerSec=496.9531502094277, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:22,279] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 72.13 | backward_microstep: 158.89 | backward_inner_microstep: 75.78 | backward_allreduce_microstep: 83.01 | step_microstep: 26.14\n",
      "[2023-06-20 09:29:22,279] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 72.13 | backward: 158.89 | backward_inner: 75.79 | backward_allreduce: 83.02 | step: 26.14\n",
      "【train】 epoch：1/1 step：210/288 loss：1.355469\n",
      "[2023-06-20 09:29:22,539] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:22,539] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.43 | backward_microstep: 159.68 | backward_inner_microstep: 75.68 | backward_allreduce_microstep: 83.89 | step_microstep: 25.86\n",
      "[2023-06-20 09:29:22,539] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.44 | backward: 159.67 | backward_inner: 75.70 | backward_allreduce: 83.88 | step: 25.85\n",
      "【train】 epoch：1/1 step：211/288 loss：1.050781\n",
      "[2023-06-20 09:29:22,879] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:22,880] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.29 | backward_microstep: 239.92 | backward_inner_microstep: 75.21 | backward_allreduce_microstep: 164.62 | step_microstep: 25.92\n",
      "[2023-06-20 09:29:22,880] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.29 | backward: 239.92 | backward_inner: 75.22 | backward_allreduce: 164.62 | step: 25.93\n",
      "【train】 epoch：1/1 step：212/288 loss：1.573242\n",
      "[2023-06-20 09:29:23,142] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:23,142] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 72.08 | backward_microstep: 160.32 | backward_inner_microstep: 76.19 | backward_allreduce_microstep: 84.03 | step_microstep: 25.78\n",
      "[2023-06-20 09:29:23,142] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 72.09 | backward: 160.32 | backward_inner: 76.19 | backward_allreduce: 84.03 | step: 25.78\n",
      "【train】 epoch：1/1 step：213/288 loss：1.417969\n",
      "[2023-06-20 09:29:23,403] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:23,403] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.18 | backward_microstep: 159.66 | backward_inner_microstep: 75.31 | backward_allreduce_microstep: 84.23 | step_microstep: 25.84\n",
      "[2023-06-20 09:29:23,403] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.18 | backward: 159.64 | backward_inner: 75.32 | backward_allreduce: 84.23 | step: 25.84\n",
      "【train】 epoch：1/1 step：214/288 loss：1.030273\n",
      "[2023-06-20 09:29:23,663] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:23,663] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.80 | backward_microstep: 159.45 | backward_inner_microstep: 75.74 | backward_allreduce_microstep: 83.60 | step_microstep: 25.59\n",
      "[2023-06-20 09:29:23,664] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.81 | backward: 159.45 | backward_inner: 75.76 | backward_allreduce: 83.60 | step: 25.59\n",
      "【train】 epoch：1/1 step：215/288 loss：1.092773\n",
      "[2023-06-20 09:29:23,925] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:23,926] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 72.04 | backward_microstep: 159.89 | backward_inner_microstep: 75.86 | backward_allreduce_microstep: 83.92 | step_microstep: 26.01\n",
      "[2023-06-20 09:29:23,926] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 72.05 | backward: 159.88 | backward_inner: 75.87 | backward_allreduce: 83.92 | step: 26.01\n",
      "【train】 epoch：1/1 step：216/288 loss：1.117188\n",
      "[2023-06-20 09:29:24,185] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:24,186] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.64 | backward_microstep: 159.73 | backward_inner_microstep: 76.11 | backward_allreduce_microstep: 83.49 | step_microstep: 25.74\n",
      "[2023-06-20 09:29:24,186] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.64 | backward: 159.72 | backward_inner: 76.14 | backward_allreduce: 83.48 | step: 25.75\n",
      "【train】 epoch：1/1 step：217/288 loss：1.230469\n",
      "[2023-06-20 09:29:24,446] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:24,447] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.95 | backward_microstep: 160.11 | backward_inner_microstep: 75.85 | backward_allreduce_microstep: 84.15 | step_microstep: 26.03\n",
      "[2023-06-20 09:29:24,447] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.95 | backward: 160.10 | backward_inner: 75.87 | backward_allreduce: 84.16 | step: 26.03\n",
      "【train】 epoch：1/1 step：218/288 loss：1.001953\n",
      "[2023-06-20 09:29:24,709] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.72\n",
      "[2023-06-20 09:29:24,710] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.75 | backward_microstep: 160.53 | backward_inner_microstep: 76.46 | backward_allreduce_microstep: 83.96 | step_microstep: 26.04\n",
      "[2023-06-20 09:29:24,710] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.75 | backward: 160.53 | backward_inner: 76.47 | backward_allreduce: 83.96 | step: 26.05\n",
      "【train】 epoch：1/1 step：219/288 loss：1.395508\n",
      "[2023-06-20 09:29:24,970] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.77\n",
      "[2023-06-20 09:29:24,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:24,971] [INFO] [timer.py:215:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=492.9141341556985, CurrSamplesPerSec=496.13385712529885, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:24,971] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.42 | backward_microstep: 160.71 | backward_inner_microstep: 75.85 | backward_allreduce_microstep: 84.75 | step_microstep: 26.36\n",
      "[2023-06-20 09:29:24,971] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.42 | backward: 160.71 | backward_inner: 75.87 | backward_allreduce: 84.75 | step: 26.37\n",
      "【train】 epoch：1/1 step：220/288 loss：0.959961\n",
      "[2023-06-20 09:29:25,232] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.68\n",
      "[2023-06-20 09:29:25,233] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.13 | backward_microstep: 159.78 | backward_inner_microstep: 75.86 | backward_allreduce_microstep: 83.82 | step_microstep: 26.01\n",
      "[2023-06-20 09:29:25,233] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.13 | backward: 159.78 | backward_inner: 75.87 | backward_allreduce: 83.82 | step: 26.02\n",
      "【train】 epoch：1/1 step：221/288 loss：1.024414\n",
      "[2023-06-20 09:29:25,493] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:29:25,493] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.46 | backward_microstep: 160.44 | backward_inner_microstep: 75.00 | backward_allreduce_microstep: 85.34 | step_microstep: 25.62\n",
      "[2023-06-20 09:29:25,493] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.46 | backward: 160.44 | backward_inner: 75.01 | backward_allreduce: 85.35 | step: 25.63\n",
      "【train】 epoch：1/1 step：222/288 loss：1.046875\n",
      "[2023-06-20 09:29:25,753] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.72\n",
      "[2023-06-20 09:29:25,754] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.16 | backward_microstep: 159.60 | backward_inner_microstep: 75.73 | backward_allreduce_microstep: 83.78 | step_microstep: 25.68\n",
      "[2023-06-20 09:29:25,754] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.17 | backward: 159.60 | backward_inner: 75.74 | backward_allreduce: 83.78 | step: 25.68\n",
      "【train】 epoch：1/1 step：223/288 loss：1.457031\n",
      "[2023-06-20 09:29:26,014] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:26,015] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.44 | backward_microstep: 160.12 | backward_inner_microstep: 75.38 | backward_allreduce_microstep: 84.62 | step_microstep: 25.51\n",
      "[2023-06-20 09:29:26,015] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.44 | backward: 160.10 | backward_inner: 75.39 | backward_allreduce: 84.63 | step: 25.51\n",
      "【train】 epoch：1/1 step：224/288 loss：1.242188\n",
      "[2023-06-20 09:29:26,276] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:26,277] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 72.13 | backward_microstep: 160.36 | backward_inner_microstep: 76.11 | backward_allreduce_microstep: 84.15 | step_microstep: 25.67\n",
      "[2023-06-20 09:29:26,277] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 72.13 | backward: 160.36 | backward_inner: 76.12 | backward_allreduce: 84.14 | step: 25.68\n",
      "【train】 epoch：1/1 step：225/288 loss：1.195312\n",
      "[2023-06-20 09:29:26,537] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:29:26,538] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.83 | backward_microstep: 160.12 | backward_inner_microstep: 75.79 | backward_allreduce_microstep: 84.23 | step_microstep: 25.62\n",
      "[2023-06-20 09:29:26,538] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.83 | backward: 160.11 | backward_inner: 75.80 | backward_allreduce: 84.23 | step: 25.62\n",
      "【train】 epoch：1/1 step：226/288 loss：0.882812\n",
      "[2023-06-20 09:29:26,798] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:26,798] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.78 | backward_microstep: 160.14 | backward_inner_microstep: 75.00 | backward_allreduce_microstep: 85.05 | step_microstep: 25.79\n",
      "[2023-06-20 09:29:26,798] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.78 | backward: 160.14 | backward_inner: 75.00 | backward_allreduce: 85.05 | step: 25.79\n",
      "【train】 epoch：1/1 step：227/288 loss：1.076172\n",
      "[2023-06-20 09:29:27,058] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:27,059] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.94 | backward_microstep: 159.52 | backward_inner_microstep: 75.54 | backward_allreduce_microstep: 83.88 | step_microstep: 25.78\n",
      "[2023-06-20 09:29:27,059] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.94 | backward: 159.51 | backward_inner: 75.55 | backward_allreduce: 83.88 | step: 25.78\n",
      "【train】 epoch：1/1 step：228/288 loss：1.070312\n",
      "[2023-06-20 09:29:27,319] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:27,319] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.14 | backward_microstep: 159.49 | backward_inner_microstep: 75.03 | backward_allreduce_microstep: 84.37 | step_microstep: 25.66\n",
      "[2023-06-20 09:29:27,319] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.14 | backward: 159.49 | backward_inner: 75.04 | backward_allreduce: 84.37 | step: 25.66\n",
      "【train】 epoch：1/1 step：229/288 loss：1.128906\n",
      "[2023-06-20 09:29:27,578] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:27,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:27,579] [INFO] [timer.py:215:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=493.10604020030297, CurrSamplesPerSec=499.29125630309153, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:27,579] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.97 | backward_microstep: 159.77 | backward_inner_microstep: 75.60 | backward_allreduce_microstep: 84.07 | step_microstep: 26.16\n",
      "[2023-06-20 09:29:27,579] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.97 | backward: 159.77 | backward_inner: 75.61 | backward_allreduce: 84.08 | step: 26.16\n",
      "【train】 epoch：1/1 step：230/288 loss：1.478516\n",
      "[2023-06-20 09:29:27,839] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.58\n",
      "[2023-06-20 09:29:27,839] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.60 | backward_microstep: 160.83 | backward_inner_microstep: 75.44 | backward_allreduce_microstep: 85.29 | step_microstep: 25.55\n",
      "[2023-06-20 09:29:27,839] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.61 | backward: 160.82 | backward_inner: 75.45 | backward_allreduce: 85.30 | step: 25.55\n",
      "【train】 epoch：1/1 step：231/288 loss：1.070312\n",
      "[2023-06-20 09:29:28,100] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.57\n",
      "[2023-06-20 09:29:28,100] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.98 | backward_microstep: 161.04 | backward_inner_microstep: 75.22 | backward_allreduce_microstep: 85.72 | step_microstep: 25.64\n",
      "[2023-06-20 09:29:28,100] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.98 | backward: 161.03 | backward_inner: 75.23 | backward_allreduce: 85.72 | step: 25.64\n",
      "【train】 epoch：1/1 step：232/288 loss：1.130859\n",
      "[2023-06-20 09:29:28,361] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.76\n",
      "[2023-06-20 09:29:28,362] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.40 | backward_microstep: 159.90 | backward_inner_microstep: 75.53 | backward_allreduce_microstep: 84.28 | step_microstep: 25.93\n",
      "[2023-06-20 09:29:28,362] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.41 | backward: 159.90 | backward_inner: 75.53 | backward_allreduce: 84.28 | step: 25.93\n",
      "【train】 epoch：1/1 step：233/288 loss：1.003906\n",
      "[2023-06-20 09:29:28,622] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:28,622] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.89 | backward_microstep: 160.89 | backward_inner_microstep: 75.74 | backward_allreduce_microstep: 85.05 | step_microstep: 25.63\n",
      "[2023-06-20 09:29:28,622] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.89 | backward: 160.89 | backward_inner: 75.75 | backward_allreduce: 85.06 | step: 25.63\n",
      "【train】 epoch：1/1 step：234/288 loss：1.001953\n",
      "[2023-06-20 09:29:28,883] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:28,883] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.52 | backward_microstep: 159.87 | backward_inner_microstep: 75.48 | backward_allreduce_microstep: 84.28 | step_microstep: 25.73\n",
      "[2023-06-20 09:29:28,884] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.52 | backward: 159.87 | backward_inner: 75.48 | backward_allreduce: 84.27 | step: 25.73\n",
      "【train】 epoch：1/1 step：235/288 loss：1.386719\n",
      "[2023-06-20 09:29:29,142] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:29,143] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.45 | backward_microstep: 159.54 | backward_inner_microstep: 74.92 | backward_allreduce_microstep: 84.52 | step_microstep: 25.49\n",
      "[2023-06-20 09:29:29,143] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.45 | backward: 159.53 | backward_inner: 74.92 | backward_allreduce: 84.52 | step: 25.49\n",
      "【train】 epoch：1/1 step：236/288 loss：1.143555\n",
      "[2023-06-20 09:29:29,403] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:29,404] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.25 | backward_microstep: 160.66 | backward_inner_microstep: 74.81 | backward_allreduce_microstep: 85.72 | step_microstep: 25.53\n",
      "[2023-06-20 09:29:29,404] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.25 | backward: 160.66 | backward_inner: 74.84 | backward_allreduce: 85.71 | step: 25.54\n",
      "【train】 epoch：1/1 step：237/288 loss：1.314453\n",
      "[2023-06-20 09:29:29,661] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:29,661] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.46 | backward_microstep: 157.59 | backward_inner_microstep: 74.38 | backward_allreduce_microstep: 83.11 | step_microstep: 25.45\n",
      "[2023-06-20 09:29:29,662] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.47 | backward: 157.58 | backward_inner: 74.39 | backward_allreduce: 83.11 | step: 25.46\n",
      "【train】 epoch：1/1 step：238/288 loss：1.012695\n",
      "[2023-06-20 09:29:29,921] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:29,922] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.06 | backward_microstep: 159.79 | backward_inner_microstep: 74.89 | backward_allreduce_microstep: 84.79 | step_microstep: 25.56\n",
      "[2023-06-20 09:29:29,922] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.07 | backward: 159.77 | backward_inner: 74.89 | backward_allreduce: 84.79 | step: 25.56\n",
      "【train】 epoch：1/1 step：239/288 loss：1.280273\n",
      "[2023-06-20 09:29:30,180] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.58\n",
      "[2023-06-20 09:29:30,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:30,181] [INFO] [timer.py:215:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=493.33185903326836, CurrSamplesPerSec=500.56260355139926, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:30,181] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.15 | backward_microstep: 159.08 | backward_inner_microstep: 74.68 | backward_allreduce_microstep: 84.30 | step_microstep: 26.07\n",
      "[2023-06-20 09:29:30,181] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.15 | backward: 159.07 | backward_inner: 74.68 | backward_allreduce: 84.30 | step: 26.07\n",
      "【train】 epoch：1/1 step：240/288 loss：1.244141\n",
      "[2023-06-20 09:29:30,440] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.58\n",
      "[2023-06-20 09:29:30,441] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.26 | backward_microstep: 160.37 | backward_inner_microstep: 74.44 | backward_allreduce_microstep: 85.83 | step_microstep: 25.23\n",
      "[2023-06-20 09:29:30,441] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.26 | backward: 160.37 | backward_inner: 74.45 | backward_allreduce: 85.82 | step: 25.23\n",
      "【train】 epoch：1/1 step：241/288 loss：1.280273\n",
      "[2023-06-20 09:29:30,698] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:30,699] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 68.96 | backward_microstep: 159.65 | backward_inner_microstep: 74.78 | backward_allreduce_microstep: 84.79 | step_microstep: 25.25\n",
      "[2023-06-20 09:29:30,699] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 68.96 | backward: 159.65 | backward_inner: 74.78 | backward_allreduce: 84.79 | step: 25.25\n",
      "【train】 epoch：1/1 step：242/288 loss：1.083984\n",
      "[2023-06-20 09:29:30,958] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:30,958] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.11 | backward_microstep: 159.91 | backward_inner_microstep: 74.48 | backward_allreduce_microstep: 85.34 | step_microstep: 25.56\n",
      "[2023-06-20 09:29:30,959] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.12 | backward: 159.90 | backward_inner: 74.48 | backward_allreduce: 85.34 | step: 25.56\n",
      "【train】 epoch：1/1 step：243/288 loss：1.220703\n",
      "[2023-06-20 09:29:31,216] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:31,217] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.26 | backward_microstep: 158.86 | backward_inner_microstep: 74.70 | backward_allreduce_microstep: 84.07 | step_microstep: 25.38\n",
      "[2023-06-20 09:29:31,217] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.26 | backward: 158.86 | backward_inner: 74.71 | backward_allreduce: 84.07 | step: 25.38\n",
      "【train】 epoch：1/1 step：244/288 loss：1.287109\n",
      "[2023-06-20 09:29:31,477] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:31,478] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.20 | backward_microstep: 159.86 | backward_inner_microstep: 74.57 | backward_allreduce_microstep: 85.18 | step_microstep: 25.42\n",
      "[2023-06-20 09:29:31,478] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.20 | backward: 159.84 | backward_inner: 74.57 | backward_allreduce: 85.18 | step: 25.42\n",
      "【train】 epoch：1/1 step：245/288 loss：1.289062\n",
      "[2023-06-20 09:29:31,737] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:31,738] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.41 | backward_microstep: 159.47 | backward_inner_microstep: 74.96 | backward_allreduce_microstep: 84.41 | step_microstep: 25.32\n",
      "[2023-06-20 09:29:31,738] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.41 | backward: 159.46 | backward_inner: 74.97 | backward_allreduce: 84.41 | step: 25.33\n",
      "【train】 epoch：1/1 step：246/288 loss：1.199219\n",
      "[2023-06-20 09:29:31,997] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:29:31,997] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.82 | backward_microstep: 159.02 | backward_inner_microstep: 74.72 | backward_allreduce_microstep: 84.21 | step_microstep: 25.66\n",
      "[2023-06-20 09:29:31,997] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.82 | backward: 159.02 | backward_inner: 74.73 | backward_allreduce: 84.20 | step: 25.66\n",
      "【train】 epoch：1/1 step：247/288 loss：0.993652\n",
      "[2023-06-20 09:29:32,334] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:29:32,334] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.90 | backward_microstep: 237.79 | backward_inner_microstep: 74.65 | backward_allreduce_microstep: 163.04 | step_microstep: 25.52\n",
      "[2023-06-20 09:29:32,335] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.90 | backward: 237.79 | backward_inner: 74.66 | backward_allreduce: 163.05 | step: 25.53\n",
      "【train】 epoch：1/1 step：248/288 loss：1.327148\n",
      "[2023-06-20 09:29:32,594] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:32,595] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.60 | backward_microstep: 160.11 | backward_inner_microstep: 75.52 | backward_allreduce_microstep: 84.49 | step_microstep: 25.70\n",
      "[2023-06-20 09:29:32,595] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.60 | backward: 160.10 | backward_inner: 75.52 | backward_allreduce: 84.49 | step: 25.70\n",
      "【train】 epoch：1/1 step：249/288 loss：1.156250\n",
      "[2023-06-20 09:29:32,854] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.60\n",
      "[2023-06-20 09:29:32,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:32,855] [INFO] [timer.py:215:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=492.98027391495157, CurrSamplesPerSec=498.56052419946474, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:32,855] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.19 | backward_microstep: 160.21 | backward_inner_microstep: 75.54 | backward_allreduce_microstep: 84.56 | step_microstep: 25.76\n",
      "[2023-06-20 09:29:32,856] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.20 | backward: 160.20 | backward_inner: 75.54 | backward_allreduce: 84.56 | step: 25.77\n",
      "【train】 epoch：1/1 step：250/288 loss：1.300781\n",
      "[2023-06-20 09:29:33,115] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:33,116] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.21 | backward_microstep: 159.75 | backward_inner_microstep: 75.12 | backward_allreduce_microstep: 84.52 | step_microstep: 25.85\n",
      "[2023-06-20 09:29:33,116] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.21 | backward: 159.75 | backward_inner: 75.13 | backward_allreduce: 84.52 | step: 25.85\n",
      "【train】 epoch：1/1 step：251/288 loss：1.513672\n",
      "[2023-06-20 09:29:33,375] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:33,376] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.82 | backward_microstep: 159.17 | backward_inner_microstep: 75.62 | backward_allreduce_microstep: 83.44 | step_microstep: 25.91\n",
      "[2023-06-20 09:29:33,376] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.82 | backward: 159.17 | backward_inner: 75.63 | backward_allreduce: 83.44 | step: 25.91\n",
      "【train】 epoch：1/1 step：252/288 loss：1.338867\n",
      "[2023-06-20 09:29:33,637] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:33,637] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.79 | backward_microstep: 159.65 | backward_inner_microstep: 75.38 | backward_allreduce_microstep: 84.17 | step_microstep: 25.82\n",
      "[2023-06-20 09:29:33,637] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.79 | backward: 159.65 | backward_inner: 75.39 | backward_allreduce: 84.18 | step: 25.83\n",
      "【train】 epoch：1/1 step：253/288 loss：0.858398\n",
      "[2023-06-20 09:29:33,898] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:33,899] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.95 | backward_microstep: 159.81 | backward_inner_microstep: 75.80 | backward_allreduce_microstep: 83.90 | step_microstep: 26.07\n",
      "[2023-06-20 09:29:33,899] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.95 | backward: 159.80 | backward_inner: 75.80 | backward_allreduce: 83.91 | step: 26.07\n",
      "【train】 epoch：1/1 step：254/288 loss：1.198242\n",
      "[2023-06-20 09:29:34,159] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:29:34,160] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.10 | backward_microstep: 160.76 | backward_inner_microstep: 75.68 | backward_allreduce_microstep: 84.99 | step_microstep: 25.65\n",
      "[2023-06-20 09:29:34,160] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.10 | backward: 160.76 | backward_inner: 75.69 | backward_allreduce: 84.99 | step: 25.66\n",
      "【train】 epoch：1/1 step：255/288 loss：1.424805\n",
      "[2023-06-20 09:29:34,420] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.63\n",
      "[2023-06-20 09:29:34,421] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.33 | backward_microstep: 159.97 | backward_inner_microstep: 75.45 | backward_allreduce_microstep: 84.43 | step_microstep: 25.74\n",
      "[2023-06-20 09:29:34,421] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.33 | backward: 159.97 | backward_inner: 75.45 | backward_allreduce: 84.43 | step: 25.75\n",
      "【train】 epoch：1/1 step：256/288 loss：1.228516\n",
      "[2023-06-20 09:29:34,680] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:29:34,681] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.69 | backward_microstep: 158.59 | backward_inner_microstep: 75.17 | backward_allreduce_microstep: 83.33 | step_microstep: 25.65\n",
      "[2023-06-20 09:29:34,681] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.70 | backward: 158.59 | backward_inner: 75.18 | backward_allreduce: 83.33 | step: 25.65\n",
      "【train】 epoch：1/1 step：257/288 loss：1.022461\n",
      "[2023-06-20 09:29:34,941] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:29:34,941] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.93 | backward_microstep: 159.47 | backward_inner_microstep: 75.41 | backward_allreduce_microstep: 83.95 | step_microstep: 25.90\n",
      "[2023-06-20 09:29:34,941] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.94 | backward: 159.46 | backward_inner: 75.41 | backward_allreduce: 83.96 | step: 25.91\n",
      "【train】 epoch：1/1 step：258/288 loss：1.113281\n",
      "[2023-06-20 09:29:35,202] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:35,202] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.85 | backward_microstep: 159.48 | backward_inner_microstep: 75.69 | backward_allreduce_microstep: 83.68 | step_microstep: 25.61\n",
      "[2023-06-20 09:29:35,202] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.85 | backward: 159.47 | backward_inner: 75.70 | backward_allreduce: 83.67 | step: 25.61\n",
      "【train】 epoch：1/1 step：259/288 loss：1.140625\n",
      "[2023-06-20 09:29:35,461] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.71\n",
      "[2023-06-20 09:29:35,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:35,462] [INFO] [timer.py:215:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=493.15797408536787, CurrSamplesPerSec=498.93628715157837, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:35,462] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.31 | backward_microstep: 159.51 | backward_inner_microstep: 75.09 | backward_allreduce_microstep: 84.30 | step_microstep: 26.40\n",
      "[2023-06-20 09:29:35,462] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.30 | backward: 159.51 | backward_inner: 75.11 | backward_allreduce: 84.29 | step: 26.40\n",
      "【train】 epoch：1/1 step：260/288 loss：1.142578\n",
      "[2023-06-20 09:29:35,720] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:35,721] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.22 | backward_microstep: 158.66 | backward_inner_microstep: 74.91 | backward_allreduce_microstep: 83.65 | step_microstep: 25.60\n",
      "[2023-06-20 09:29:35,721] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.23 | backward: 158.66 | backward_inner: 74.92 | backward_allreduce: 83.65 | step: 25.60\n",
      "【train】 epoch：1/1 step：261/288 loss：1.347656\n",
      "[2023-06-20 09:29:35,980] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.70\n",
      "[2023-06-20 09:29:35,981] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.51 | backward_microstep: 159.52 | backward_inner_microstep: 75.66 | backward_allreduce_microstep: 83.75 | step_microstep: 25.63\n",
      "[2023-06-20 09:29:35,981] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.51 | backward: 159.51 | backward_inner: 75.67 | backward_allreduce: 83.74 | step: 25.63\n",
      "【train】 epoch：1/1 step：262/288 loss：1.537109\n",
      "[2023-06-20 09:29:36,242] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:36,242] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.51 | backward_microstep: 161.19 | backward_inner_microstep: 75.05 | backward_allreduce_microstep: 86.05 | step_microstep: 25.69\n",
      "[2023-06-20 09:29:36,242] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.51 | backward: 161.19 | backward_inner: 75.06 | backward_allreduce: 86.05 | step: 25.70\n",
      "【train】 epoch：1/1 step：263/288 loss：1.069336\n",
      "[2023-06-20 09:29:36,502] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.72\n",
      "[2023-06-20 09:29:36,503] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.82 | backward_microstep: 160.70 | backward_inner_microstep: 75.41 | backward_allreduce_microstep: 85.19 | step_microstep: 25.88\n",
      "[2023-06-20 09:29:36,503] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.82 | backward: 160.69 | backward_inner: 75.41 | backward_allreduce: 85.20 | step: 25.89\n",
      "【train】 epoch：1/1 step：264/288 loss：1.133789\n",
      "[2023-06-20 09:29:36,763] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:36,763] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.36 | backward_microstep: 159.88 | backward_inner_microstep: 75.98 | backward_allreduce_microstep: 83.80 | step_microstep: 25.60\n",
      "[2023-06-20 09:29:36,764] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.36 | backward: 159.87 | backward_inner: 75.99 | backward_allreduce: 83.81 | step: 25.60\n",
      "【train】 epoch：1/1 step：265/288 loss：1.383789\n",
      "[2023-06-20 09:29:37,024] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:37,024] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.64 | backward_microstep: 160.25 | backward_inner_microstep: 75.85 | backward_allreduce_microstep: 84.30 | step_microstep: 25.79\n",
      "[2023-06-20 09:29:37,024] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.64 | backward: 160.24 | backward_inner: 75.86 | backward_allreduce: 84.31 | step: 25.79\n",
      "【train】 epoch：1/1 step：266/288 loss：1.376953\n",
      "[2023-06-20 09:29:37,284] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.58\n",
      "[2023-06-20 09:29:37,284] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.46 | backward_microstep: 160.73 | backward_inner_microstep: 74.93 | backward_allreduce_microstep: 85.68 | step_microstep: 25.52\n",
      "[2023-06-20 09:29:37,284] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.46 | backward: 160.71 | backward_inner: 74.94 | backward_allreduce: 85.68 | step: 25.53\n",
      "【train】 epoch：1/1 step：267/288 loss：1.245117\n",
      "[2023-06-20 09:29:37,545] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.79\n",
      "[2023-06-20 09:29:37,545] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.21 | backward_microstep: 159.21 | backward_inner_microstep: 75.40 | backward_allreduce_microstep: 83.71 | step_microstep: 26.54\n",
      "[2023-06-20 09:29:37,546] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.21 | backward: 159.21 | backward_inner: 75.41 | backward_allreduce: 83.71 | step: 26.54\n",
      "【train】 epoch：1/1 step：268/288 loss：1.337891\n",
      "[2023-06-20 09:29:37,804] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:37,805] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.01 | backward_microstep: 158.95 | backward_inner_microstep: 75.22 | backward_allreduce_microstep: 83.63 | step_microstep: 25.44\n",
      "[2023-06-20 09:29:37,805] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.01 | backward: 158.94 | backward_inner: 75.23 | backward_allreduce: 83.63 | step: 25.44\n",
      "【train】 epoch：1/1 step：269/288 loss：1.153320\n",
      "[2023-06-20 09:29:38,064] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:29:38,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:38,065] [INFO] [timer.py:215:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=493.34940231310173, CurrSamplesPerSec=499.0415651921725, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:38,065] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.18 | backward_microstep: 159.13 | backward_inner_microstep: 74.81 | backward_allreduce_microstep: 84.22 | step_microstep: 25.75\n",
      "[2023-06-20 09:29:38,065] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.18 | backward: 159.13 | backward_inner: 74.82 | backward_allreduce: 84.22 | step: 25.75\n",
      "【train】 epoch：1/1 step：270/288 loss：1.154297\n",
      "[2023-06-20 09:29:38,323] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.69\n",
      "[2023-06-20 09:29:38,324] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.58 | backward_microstep: 158.58 | backward_inner_microstep: 75.16 | backward_allreduce_microstep: 83.32 | step_microstep: 25.80\n",
      "[2023-06-20 09:29:38,324] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.59 | backward: 158.58 | backward_inner: 75.17 | backward_allreduce: 83.32 | step: 25.80\n",
      "【train】 epoch：1/1 step：271/288 loss：1.050781\n",
      "[2023-06-20 09:29:38,583] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:38,584] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.77 | backward_microstep: 157.81 | backward_inner_microstep: 74.81 | backward_allreduce_microstep: 82.90 | step_microstep: 25.74\n",
      "[2023-06-20 09:29:38,584] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.77 | backward: 157.80 | backward_inner: 74.82 | backward_allreduce: 82.90 | step: 25.74\n",
      "【train】 epoch：1/1 step：272/288 loss：1.176758\n",
      "[2023-06-20 09:29:38,842] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.70\n",
      "[2023-06-20 09:29:38,842] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.31 | backward_microstep: 158.41 | backward_inner_microstep: 75.64 | backward_allreduce_microstep: 82.66 | step_microstep: 25.57\n",
      "[2023-06-20 09:29:38,842] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.32 | backward: 158.40 | backward_inner: 75.65 | backward_allreduce: 82.67 | step: 25.57\n",
      "【train】 epoch：1/1 step：273/288 loss：1.243164\n",
      "[2023-06-20 09:29:39,102] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:29:39,102] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.26 | backward_microstep: 159.59 | backward_inner_microstep: 75.71 | backward_allreduce_microstep: 83.77 | step_microstep: 25.93\n",
      "[2023-06-20 09:29:39,102] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.26 | backward: 159.58 | backward_inner: 75.72 | backward_allreduce: 83.77 | step: 25.93\n",
      "【train】 epoch：1/1 step：274/288 loss：1.050781\n",
      "[2023-06-20 09:29:39,362] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:39,362] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.62 | backward_microstep: 160.06 | backward_inner_microstep: 76.12 | backward_allreduce_microstep: 83.75 | step_microstep: 25.32\n",
      "[2023-06-20 09:29:39,363] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.62 | backward: 160.05 | backward_inner: 76.13 | backward_allreduce: 83.76 | step: 25.32\n",
      "【train】 epoch：1/1 step：275/288 loss：0.920410\n",
      "[2023-06-20 09:29:39,622] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.62\n",
      "[2023-06-20 09:29:39,622] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.32 | backward_microstep: 158.69 | backward_inner_microstep: 75.10 | backward_allreduce_microstep: 83.49 | step_microstep: 25.56\n",
      "[2023-06-20 09:29:39,622] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.32 | backward: 158.68 | backward_inner: 75.11 | backward_allreduce: 83.49 | step: 25.57\n",
      "【train】 epoch：1/1 step：276/288 loss：1.224609\n",
      "[2023-06-20 09:29:39,881] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:39,881] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.99 | backward_microstep: 159.88 | backward_inner_microstep: 75.27 | backward_allreduce_microstep: 84.51 | step_microstep: 25.12\n",
      "[2023-06-20 09:29:39,881] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.99 | backward: 159.88 | backward_inner: 75.28 | backward_allreduce: 84.51 | step: 25.12\n",
      "【train】 epoch：1/1 step：277/288 loss：1.149414\n",
      "[2023-06-20 09:29:40,140] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:40,141] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.80 | backward_microstep: 159.12 | backward_inner_microstep: 75.09 | backward_allreduce_microstep: 83.93 | step_microstep: 25.71\n",
      "[2023-06-20 09:29:40,141] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.81 | backward: 159.12 | backward_inner: 75.10 | backward_allreduce: 83.93 | step: 25.71\n",
      "【train】 epoch：1/1 step：278/288 loss：1.164062\n",
      "[2023-06-20 09:29:40,401] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.59\n",
      "[2023-06-20 09:29:40,402] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.34 | backward_microstep: 159.84 | backward_inner_microstep: 75.81 | backward_allreduce_microstep: 83.93 | step_microstep: 25.65\n",
      "[2023-06-20 09:29:40,402] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.34 | backward: 159.83 | backward_inner: 75.82 | backward_allreduce: 83.93 | step: 25.65\n",
      "【train】 epoch：1/1 step：279/288 loss：1.298828\n",
      "[2023-06-20 09:29:40,661] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.65\n",
      "[2023-06-20 09:29:40,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=18, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:40,662] [INFO] [timer.py:215:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=493.56709946190614, CurrSamplesPerSec=499.13203775721473, MemAllocated=0.77GB, MaxMemAllocated=2.63GB\n",
      "[2023-06-20 09:29:40,662] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.55 | backward_microstep: 159.66 | backward_inner_microstep: 75.58 | backward_allreduce_microstep: 83.99 | step_microstep: 25.77\n",
      "[2023-06-20 09:29:40,662] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.55 | backward: 159.65 | backward_inner: 75.58 | backward_allreduce: 83.99 | step: 25.77\n",
      "【train】 epoch：1/1 step：280/288 loss：1.128906\n",
      "[2023-06-20 09:29:40,921] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:40,922] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.54 | backward_microstep: 159.07 | backward_inner_microstep: 75.00 | backward_allreduce_microstep: 83.97 | step_microstep: 25.93\n",
      "[2023-06-20 09:29:40,922] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.55 | backward: 159.07 | backward_inner: 75.01 | backward_allreduce: 83.98 | step: 25.93\n",
      "【train】 epoch：1/1 step：281/288 loss：1.296875\n",
      "[2023-06-20 09:29:41,182] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:41,182] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.55 | backward_microstep: 159.18 | backward_inner_microstep: 75.67 | backward_allreduce_microstep: 83.39 | step_microstep: 25.58\n",
      "[2023-06-20 09:29:41,182] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.54 | backward: 159.16 | backward_inner: 75.67 | backward_allreduce: 83.38 | step: 25.58\n",
      "【train】 epoch：1/1 step：282/288 loss：1.052734\n",
      "[2023-06-20 09:29:41,441] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.64\n",
      "[2023-06-20 09:29:41,442] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.07 | backward_microstep: 158.84 | backward_inner_microstep: 75.04 | backward_allreduce_microstep: 83.70 | step_microstep: 25.79\n",
      "[2023-06-20 09:29:41,442] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.07 | backward: 158.84 | backward_inner: 75.05 | backward_allreduce: 83.70 | step: 25.79\n",
      "【train】 epoch：1/1 step：283/288 loss：1.264648\n",
      "[2023-06-20 09:29:41,781] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:41,782] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 71.17 | backward_microstep: 238.49 | backward_inner_microstep: 75.52 | backward_allreduce_microstep: 162.87 | step_microstep: 25.71\n",
      "[2023-06-20 09:29:41,782] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 71.17 | backward: 238.49 | backward_inner: 75.53 | backward_allreduce: 162.87 | step: 25.71\n",
      "【train】 epoch：1/1 step：284/288 loss：0.983887\n",
      "[2023-06-20 09:29:42,042] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.61\n",
      "[2023-06-20 09:29:42,042] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.88 | backward_microstep: 159.84 | backward_inner_microstep: 75.87 | backward_allreduce_microstep: 83.87 | step_microstep: 25.93\n",
      "[2023-06-20 09:29:42,042] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.88 | backward: 159.83 | backward_inner: 75.88 | backward_allreduce: 83.88 | step: 25.94\n",
      "【train】 epoch：1/1 step：285/288 loss：1.180664\n",
      "[2023-06-20 09:29:42,302] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.76\n",
      "[2023-06-20 09:29:42,303] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.75 | backward_microstep: 159.93 | backward_inner_microstep: 75.37 | backward_allreduce_microstep: 84.46 | step_microstep: 25.97\n",
      "[2023-06-20 09:29:42,303] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.75 | backward: 159.93 | backward_inner: 75.37 | backward_allreduce: 84.45 | step: 25.98\n",
      "【train】 epoch：1/1 step：286/288 loss：1.039062\n",
      "[2023-06-20 09:29:42,562] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.67\n",
      "[2023-06-20 09:29:42,563] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 70.69 | backward_microstep: 159.33 | backward_inner_microstep: 75.32 | backward_allreduce_microstep: 83.92 | step_microstep: 25.64\n",
      "[2023-06-20 09:29:42,563] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 70.69 | backward: 159.33 | backward_inner: 75.32 | backward_allreduce: 83.92 | step: 25.65\n",
      "【train】 epoch：1/1 step：287/288 loss：1.268555\n",
      "[2023-06-20 09:29:42,820] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 4.66\n",
      "[2023-06-20 09:29:42,821] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 69.93 | backward_microstep: 158.66 | backward_inner_microstep: 74.66 | backward_allreduce_microstep: 83.90 | step_microstep: 25.56\n",
      "[2023-06-20 09:29:42,821] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | forward: 69.93 | backward: 158.66 | backward_inner: 74.67 | backward_allreduce: 83.91 | step: 25.56\n",
      "【train】 epoch：1/1 step：288/288 loss：1.096680\n",
      "耗时：1.2790814956029257分钟\n",
      "[2023-06-20 09:29:42,858] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step288 is about to be saved!\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2023-06-20 09:29:42,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:42,862] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_model_states.pt\n",
      "[2023-06-20 09:29:42,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:42,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:42,863] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
      "[2023-06-20 09:29:42,863] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:42,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:42,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:42,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:42,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:42,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2023-06-20 09:29:42,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n",
      "[2023-06-20 09:29:42,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_optim_states.pt...\n",
      "[2023-06-20 09:29:42,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_optim_states.pt...\n",
      "[2023-06-20 09:29:43,325] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_optim_states.pt.\n",
      "[2023-06-20 09:29:43,325] [INFO] [engine.py:3245:_save_zero_checkpoint] zero checkpoint saved output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_optim_states.pt\n",
      "[2023-06-20 09:29:43,327] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_optim_states.pt.\n",
      "[2023-06-20 09:29:43,327] [INFO] [engine.py:3245:_save_zero_checkpoint] zero checkpoint saved output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_optim_states.pt\n",
      "[2023-06-20 09:29:43,328] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n",
      "[2023-06-20 09:29:43,328] [INFO] [engine.py:3245:_save_zero_checkpoint] zero checkpoint saved output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_optim_states.pt\n",
      "[2023-06-20 09:29:43,328] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2023-06-20 09:29:43,329] [INFO] [engine.py:3245:_save_zero_checkpoint] zero checkpoint saved output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2023-06-20 09:29:43,334] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step288 is ready now!\n",
      "[2023-06-20 09:29:43,334] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step288 is ready now!\n",
      "[2023-06-20 09:29:43,335] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step288 is ready now!\n",
      "[2023-06-20 09:29:43,335] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step288 is ready now!\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-06-20 09:29:44,405] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.4, git-hash=unknown, git-branch=unknown\n",
      "[2023-06-20 09:29:44,422] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0006306171417236328 seconds\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0006339550018310547 seconds\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0006382465362548828 seconds\n",
      "Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0007047653198242188 seconds\n",
      "[2023-06-20 09:29:44,508] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2023-06-20 09:29:44,514] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2023-06-20 09:29:44,515] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2023-06-20 09:29:44,515] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2023-06-20 09:29:44,515] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.0002410411834716797 seconds\n",
      "\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00023126602172851562 seconds\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00019788742065429688 seconds\n",
      "[2023-06-20 09:29:44,597] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning\n",
      "[2023-06-20 09:29:44,598] [INFO] [utils.py:786:see_memory_usage] MA 0.98 GB         Max_MA 2.63 GB         CA 3.22 GB         Max_CA 3 GB \n",
      "[2023-06-20 09:29:44,598] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:44,599] [INFO] [stage3.py:113:__init__] Reduce bucket size 200000000\n",
      "[2023-06-20 09:29:44,599] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50,000,000\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002713203430175781 seconds\n",
      "[2023-06-20 09:29:44,677] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-06-20 09:29:44,678] [INFO] [utils.py:786:see_memory_usage] MA 0.98 GB         Max_MA 0.98 GB         CA 3.22 GB         Max_CA 3 GB \n",
      "[2023-06-20 09:29:44,678] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "Parameter Offload: Total persistent parameters: 128262 in 126 params\n",
      "[2023-06-20 09:29:44,796] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-06-20 09:29:44,797] [INFO] [utils.py:786:see_memory_usage] MA 0.82 GB         Max_MA 0.98 GB         CA 3.23 GB         Max_CA 3 GB \n",
      "[2023-06-20 09:29:44,797] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:44,877] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions\n",
      "[2023-06-20 09:29:44,878] [INFO] [utils.py:786:see_memory_usage] MA 0.82 GB         Max_MA 0.82 GB         CA 3.23 GB         Max_CA 3 GB \n",
      "[2023-06-20 09:29:44,878] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:45,137] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2023-06-20 09:29:45,138] [INFO] [utils.py:786:see_memory_usage] MA 0.82 GB         Max_MA 0.82 GB         CA 0.88 GB         Max_CA 3 GB \n",
      "[2023-06-20 09:29:45,138] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:45,218] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions\n",
      "[2023-06-20 09:29:45,219] [INFO] [utils.py:786:see_memory_usage] MA 0.82 GB         Max_MA 0.82 GB         CA 0.88 GB         Max_CA 1 GB \n",
      "[2023-06-20 09:29:45,219] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:45,299] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions\n",
      "[2023-06-20 09:29:45,300] [INFO] [utils.py:786:see_memory_usage] MA 0.91 GB         Max_MA 0.96 GB         CA 1.02 GB         Max_CA 1 GB \n",
      "[2023-06-20 09:29:45,300] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:45,380] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n",
      "[2023-06-20 09:29:45,380] [INFO] [utils.py:786:see_memory_usage] MA 0.91 GB         Max_MA 0.91 GB         CA 1.02 GB         Max_CA 1 GB \n",
      "[2023-06-20 09:29:45,381] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:45,384] [INFO] [logging.py:96:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 2.31\n",
      "[2023-06-20 09:29:45,463] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n",
      "[2023-06-20 09:29:45,464] [INFO] [utils.py:786:see_memory_usage] MA 1.11 GB         Max_MA 1.2 GB         CA 1.31 GB         Max_CA 1 GB \n",
      "[2023-06-20 09:29:45,464] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 13.93 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:45,465] [INFO] [stage3.py:388:_setup_for_real_optimizer] optimizer state initialized\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "\n",
      "Loading extension module utils...Loading extension module utils...\n",
      "\n",
      "Time to load utils op: 0.0002932548522949219 secondsTime to load utils op: 0.0003190040588378906 seconds\n",
      "\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004112720489501953 seconds\n",
      "[2023-06-20 09:29:45,538] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:45,538] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:45,540] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:45,542] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:45,542] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:45,543] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:45,543] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:45,544] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:45,545] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:45,547] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:45,547] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:45,549] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:45,551] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_optim_states.pt...\n",
      "[2023-06-20 09:29:45,551] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n",
      "[2023-06-20 09:29:45,553] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_optim_states.pt...\n",
      "[2023-06-20 09:29:45,619] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-06-20 09:29:45,620] [INFO] [utils.py:786:see_memory_usage] MA 1.53 GB         Max_MA 1.59 GB         CA 1.68 GB         Max_CA 2 GB \n",
      "[2023-06-20 09:29:45,620] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.0 GB, percent = 7.5%\n",
      "[2023-06-20 09:29:45,620] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
      "[2023-06-20 09:29:45,620] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-06-20 09:29:45,620] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2023-06-20 09:29:45,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:960:print] DeepSpeedEngine configuration:\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": true, \n",
      "    \"contiguous_memory_optimization\": true, \n",
      "    \"cpu_checkpointing\": true, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   amp_enabled .................. False\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   amp_params ................... False\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   bfloat16_enabled ............. False\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc4621df4c0>\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   communication_data_type ...... None\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False\n",
      "[2023-06-20 09:29:45,621] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   dataloader_drop_last ......... False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   disable_allgather ............ False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   dump_state ................... False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   elasticity_enabled ........... False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   fp16_auto_cast ............... False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   fp16_enabled ................. True\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   global_rank .................. 0\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   grad_accum_dtype ............. None\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   load_universal_checkpoint .... False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   loss_scale ................... 0\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   memory_breakdown ............. False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   mics_shard_size .............. -1\n",
      "[2023-06-20 09:29:45,622] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   optimizer_name ............... adamw\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 3e-05}\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   pld_enabled .................. False\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   pld_params ................... False\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   prescale_gradients ........... False\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   scheduler_name ............... None\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   scheduler_params ............. None\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   sparse_attention ............. None\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   steps_per_print .............. 10\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   train_batch_size ............. 128\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  32\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   use_node_local_storage ....... False\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   wall_clock_breakdown ......... True\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   world_size ................... 4\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   zero_enabled ................. True\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:964:print]   zero_optimization_stage ...... 3\n",
      "[2023-06-20 09:29:45,623] [INFO] [config.py:950:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05\n",
      "        }\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": true, \n",
      "        \"cpu_checkpointing\": true, \n",
      "        \"contiguous_memory_optimization\": true\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": true, \n",
      "    \"log_dist\": false\n",
      "}\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000270843505859375 seconds\n",
      "[2023-06-20 09:29:45,626] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:45,628] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_3_mp_rank_00_optim_states.pt.\n",
      "[2023-06-20 09:29:45,628] [INFO] [engine.py:2825:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 3\n",
      "[2023-06-20 09:29:45,632] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:45,633] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_model_states.pt...\n",
      "[2023-06-20 09:29:45,639] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_model_states.pt.\n",
      "[2023-06-20 09:29:45,644] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n",
      "[2023-06-20 09:29:45,645] [INFO] [engine.py:2825:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 1\n",
      "[2023-06-20 09:29:45,645] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2023-06-20 09:29:45,654] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_2_mp_rank_00_optim_states.pt.\n",
      "[2023-06-20 09:29:45,654] [INFO] [engine.py:2825:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 2\n",
      "[2023-06-20 09:29:45,740] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output/deepspeed/global_step288/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2023-06-20 09:29:45,740] [INFO] [engine.py:2825:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 0\n",
      "[2023-06-20 09:29:45,791] [INFO] [engine.py:2775:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 2\n",
      "[2023-06-20 09:29:45,791] [INFO] [engine.py:2775:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 1\n",
      "[2023-06-20 09:29:45,791] [INFO] [engine.py:2775:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 3\n",
      "[2023-06-20 09:29:45,791] [INFO] [engine.py:2775:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 0\n",
      "(3200,) (3200,)\n",
      "(3200,) (3200,)\n",
      "(3200,) (3200,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          其他       0.64      0.69      0.67      1092\n",
      "          喜好       0.48      0.68      0.57       448\n",
      "          悲伤       0.61      0.52      0.56       456\n",
      "          厌恶       0.40      0.42      0.41       480\n",
      "          愤怒       0.58      0.35      0.44       248\n",
      "          高兴       0.74      0.54      0.62       476\n",
      "\n",
      "    accuracy                           0.58      3200\n",
      "   macro avg       0.58      0.53      0.54      3200\n",
      "weighted avg       0.59      0.58      0.57      3200\n",
      "\n",
      "(3200,) (3200,)\n",
      "[2023-06-20 09:29:49,335] [INFO] [launch.py:347:main] Process 16056 exits successfully.\n",
      "[2023-06-20 09:29:49,336] [INFO] [launch.py:347:main] Process 16054 exits successfully.\n",
      "[2023-06-20 09:29:49,336] [INFO] [launch.py:347:main] Process 16055 exits successfully.\n",
      "[2023-06-20 09:29:49,336] [INFO] [launch.py:347:main] Process 16057 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!~/anaconda3/envs/python39_p13/bin/deepspeed --master_port 11222 multi-gpu-deepspeed-cls.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bfd5f-5945-4d1a-938d-ef423e85b94a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## accelerate分布式训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7c29e-43f1-4318-be60-b4b164572e66",
   "metadata": {},
   "source": [
    "accelerate launch multi-gpu-accelerate-cls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6695589e-1650-4cea-bd59-76fd9a4f72e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-20 09:37:42,981] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `4`\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2023-06-20 09:37:44,894] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-06-20 09:37:44,895] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-06-20 09:37:44,906] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-06-20 09:37:44,916] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "【train】 epoch：1/1 step：1/72 loss：1.785353\n",
      "【train】 epoch：1/1 step：2/72 loss：1.655338\n",
      "【train】 epoch：1/1 step：3/72 loss：1.751402\n",
      "【train】 epoch：1/1 step：4/72 loss：1.670869\n",
      "【train】 epoch：1/1 step：5/72 loss：1.719969\n",
      "【train】 epoch：1/1 step：6/72 loss：1.683457\n",
      "【train】 epoch：1/1 step：7/72 loss：1.622109\n",
      "【train】 epoch：1/1 step：8/72 loss：1.672972\n",
      "【train】 epoch：1/1 step：9/72 loss：1.568676\n",
      "【train】 epoch：1/1 step：10/72 loss：1.533262\n",
      "【train】 epoch：1/1 step：11/72 loss：1.514990\n",
      "【train】 epoch：1/1 step：12/72 loss：1.549452\n",
      "【train】 epoch：1/1 step：13/72 loss：1.534319\n",
      "【train】 epoch：1/1 step：14/72 loss：1.579864\n",
      "【train】 epoch：1/1 step：15/72 loss：1.505718\n",
      "【train】 epoch：1/1 step：16/72 loss：1.431594\n",
      "【train】 epoch：1/1 step：17/72 loss：1.416635\n",
      "【train】 epoch：1/1 step：18/72 loss：1.430851\n",
      "【train】 epoch：1/1 step：19/72 loss：1.463776\n",
      "【train】 epoch：1/1 step：20/72 loss：1.391501\n",
      "【train】 epoch：1/1 step：21/72 loss：1.445940\n",
      "【train】 epoch：1/1 step：22/72 loss：1.432463\n",
      "【train】 epoch：1/1 step：23/72 loss：1.338597\n",
      "【train】 epoch：1/1 step：24/72 loss：1.277691\n",
      "【train】 epoch：1/1 step：25/72 loss：1.296715\n",
      "【train】 epoch：1/1 step：26/72 loss：1.345743\n",
      "【train】 epoch：1/1 step：27/72 loss：1.288982\n",
      "【train】 epoch：1/1 step：28/72 loss：1.374185\n",
      "【train】 epoch：1/1 step：29/72 loss：1.113034\n",
      "【train】 epoch：1/1 step：30/72 loss：1.289321\n",
      "【train】 epoch：1/1 step：31/72 loss：1.224943\n",
      "【train】 epoch：1/1 step：32/72 loss：1.259383\n",
      "【train】 epoch：1/1 step：33/72 loss：1.253645\n",
      "【train】 epoch：1/1 step：34/72 loss：1.281317\n",
      "【train】 epoch：1/1 step：35/72 loss：1.290305\n",
      "【train】 epoch：1/1 step：36/72 loss：1.214430\n",
      "【train】 epoch：1/1 step：37/72 loss：1.281808\n",
      "【train】 epoch：1/1 step：38/72 loss：1.201862\n",
      "【train】 epoch：1/1 step：39/72 loss：1.123906\n",
      "【train】 epoch：1/1 step：40/72 loss：1.014217\n",
      "【train】 epoch：1/1 step：41/72 loss：1.228409\n",
      "【train】 epoch：1/1 step：42/72 loss：1.217548\n",
      "【train】 epoch：1/1 step：43/72 loss：1.179475\n",
      "【train】 epoch：1/1 step：44/72 loss：1.188559\n",
      "【train】 epoch：1/1 step：45/72 loss：1.153583\n",
      "【train】 epoch：1/1 step：46/72 loss：1.233602\n",
      "【train】 epoch：1/1 step：47/72 loss：1.313673\n",
      "【train】 epoch：1/1 step：48/72 loss：1.236282\n",
      "【train】 epoch：1/1 step：49/72 loss：1.302555\n",
      "【train】 epoch：1/1 step：50/72 loss：1.280405\n",
      "【train】 epoch：1/1 step：51/72 loss：1.141787\n",
      "【train】 epoch：1/1 step：52/72 loss：1.385400\n",
      "【train】 epoch：1/1 step：53/72 loss：1.250002\n",
      "【train】 epoch：1/1 step：54/72 loss：1.241858\n",
      "【train】 epoch：1/1 step：55/72 loss：1.098119\n",
      "【train】 epoch：1/1 step：56/72 loss：1.142250\n",
      "【train】 epoch：1/1 step：57/72 loss：1.240373\n",
      "【train】 epoch：1/1 step：58/72 loss：1.054865\n",
      "【train】 epoch：1/1 step：59/72 loss：1.189831\n",
      "【train】 epoch：1/1 step：60/72 loss：1.123348\n",
      "【train】 epoch：1/1 step：61/72 loss：1.211454\n",
      "【train】 epoch：1/1 step：62/72 loss：1.157606\n",
      "【train】 epoch：1/1 step：63/72 loss：1.134397\n",
      "【train】 epoch：1/1 step：64/72 loss：1.045733\n",
      "【train】 epoch：1/1 step：65/72 loss：1.023316\n",
      "【train】 epoch：1/1 step：66/72 loss：1.233094\n",
      "【train】 epoch：1/1 step：67/72 loss：1.199572\n",
      "【train】 epoch：1/1 step：68/72 loss：1.040617\n",
      "【train】 epoch：1/1 step：69/72 loss：1.066492\n",
      "【train】 epoch：1/1 step：70/72 loss：1.048623\n",
      "【train】 epoch：1/1 step：71/72 loss：1.092311\n",
      "【train】 epoch：1/1 step：72/72 loss：1.216881\n",
      "耗时：0.2994843085606893分钟\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "(3200,) (3200,)\n",
      "(3200,) (3200,)\n",
      "(3200,) (3200,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          其他       0.59      0.79      0.68      1092\n",
      "          喜好       0.48      0.68      0.57       448\n",
      "          悲伤       0.60      0.36      0.45       456\n",
      "          厌恶       0.37      0.39      0.38       480\n",
      "          愤怒       0.70      0.11      0.19       248\n",
      "          高兴       0.77      0.49      0.60       476\n",
      "\n",
      "    accuracy                           0.56      3200\n",
      "   macro avg       0.59      0.47      0.48      3200\n",
      "weighted avg       0.58      0.56      0.54      3200\n",
      "\n",
      "(3200,) (3200,)\n"
     ]
    }
   ],
   "source": [
    "!~/anaconda3/envs/python39_p13/bin/accelerate launch multi-gpu-accelerate-cls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302724a-e457-435d-b9a2-55b8b7d97c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2baf2164-6871-4428-a4bf-d3fa538d76b3",
   "metadata": {},
   "source": [
    "## transformers的Trainer分布式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8914439-ffe4-4e3e-85f1-d3709cf3b313",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-20 09:51:07,596] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/72 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.4242, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.69}         \n",
      " 69%|█████████████████████████████▊             | 50/72 [00:23<00:08,  2.72it/s]\n",
      "  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|████████████▊                                | 2/7 [00:00<00:00, 11.07it/s]\u001b[A\n",
      " 57%|█████████████████████████▋                   | 4/7 [00:00<00:00,  7.01it/s]\u001b[A\n",
      " 71%|████████████████████████████████▏            | 5/7 [00:00<00:00,  6.52it/s]\u001b[A\n",
      " 86%|██████████████████████████████████████▌      | 6/7 [00:00<00:00,  6.22it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2517703771591187, 'eval_accuracy：': 0.54125, 'eval_runtime': 1.2174, 'eval_samples_per_second': 657.124, 'eval_steps_per_second': 5.75, 'epoch': 0.69}\n",
      " 69%|█████████████████████████████▊             | 50/72 [00:24<00:08,  2.72it/s]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:01<00:00,  6.47it/s]\u001b[A\n",
      "                                                                                \u001b[A/home/ec2-user/anaconda3/envs/python39_p13/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'train_runtime': 37.2363, 'train_samples_per_second': 247.071, 'train_steps_per_second': 1.934, 'train_loss': 1.3621528148651123, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████████| 72/72 [00:37<00:00,  1.93it/s]\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:01<00:00,  6.73it/s]\n",
      "{'eval_loss': 1.2517703771591187, 'eval_accuracy：': 0.54125, 'eval_runtime': 1.2212, 'eval_samples_per_second': 655.088, 'eval_steps_per_second': 5.732, 'epoch': 1.0}\n",
      "(800,) (800,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          其他       0.61      0.73      0.66       273\n",
      "          喜好       0.43      0.54      0.47       112\n",
      "          悲伤       0.53      0.37      0.44       114\n",
      "          厌恶       0.37      0.29      0.33       120\n",
      "          愤怒       0.62      0.29      0.40        62\n",
      "          高兴       0.61      0.66      0.63       119\n",
      "\n",
      "    accuracy                           0.54       800\n",
      "   macro avg       0.53      0.48      0.49       800\n",
      "weighted avg       0.54      0.54      0.53       800\n",
      "\n",
      "CPU times: user 317 ms, sys: 144 ms, total: 461 ms\n",
      "Wall time: 46.3 s\n"
     ]
    }
   ],
   "source": [
    "%ittime\n",
    "!~/anaconda3/envs/python39_p13/bin/python multi-gpu-transformers-cls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcd50f-07f0-4bd3-92e2-d38f31293cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796021a-b0fe-47db-9e34-24f629360216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ad4ce3-8212-4c28-863e-f4c9c0ac89b7",
   "metadata": {},
   "source": [
    "# accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0872258-3cd1-49e3-bdc9-3da276e1ca9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'accelerate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m write_basic_config\n\u001b[1;32m      3\u001b[0m write_basic_config() \u001b[38;5;66;03m# Write a config file\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'accelerate'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from accelerate.utils import write_basic_config\n",
    "write_basic_config() # Write a config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e40f8c-eb29-4961-8502-cba6c42a21a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
